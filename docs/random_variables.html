<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Applied Machine Learning for Aerospace Systems - 4&nbsp; Random Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./random_processes.html" rel="next">
<link href="./functions.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./math.html">Foundational Mathematics</a></li><li class="breadcrumb-item"><a href="./random_variables.html"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Applied Machine Learning for Aerospace Systems</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Mathematics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Functions &amp; Function Spaces</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_variables.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Random Processes and Sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./monte_carlo_methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Monte Carlo Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./function_approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Function Approximation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./neural_networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Neural Networks</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning Algorithms</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_pre_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Data Pre-Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dimensionality_reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Dimensionality Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./aerospace_applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aerospace Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./application1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Aerospace Application 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./application2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Aerospace Application 2</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="7">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#definitions-and-probability-axioms" id="toc-definitions-and-probability-axioms" class="nav-link active" data-scroll-target="#definitions-and-probability-axioms"><span class="header-section-number">4.1</span> Definitions and Probability Axioms</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability"><span class="header-section-number">4.2</span> Conditional Probability</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">4.3</span> Bayes’ Theorem</a></li>
  <li><a href="#discrete-random-variables" id="toc-discrete-random-variables" class="nav-link" data-scroll-target="#discrete-random-variables"><span class="header-section-number">4.4</span> Discrete Random Variables</a>
  <ul class="collapse">
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables"><span class="header-section-number">4.4.1</span> Continuous Random Variables</a></li>
  </ul></li>
  <li><a href="#cumulative-distribution-functions-cdfs" id="toc-cumulative-distribution-functions-cdfs" class="nav-link" data-scroll-target="#cumulative-distribution-functions-cdfs"><span class="header-section-number">4.5</span> Cumulative Distribution Functions (CDFs)</a></li>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value"><span class="header-section-number">4.6</span> Expected Value</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">4.7</span> Variance</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers"><span class="header-section-number">4.8</span> Law of Large Numbers</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="header-section-number">4.9</span> Central Limit Theorem</a></li>
  <li><a href="#common-distributions" id="toc-common-distributions" class="nav-link" data-scroll-target="#common-distributions"><span class="header-section-number">4.10</span> Common Distributions</a></li>
  <li><a href="#joint-marginal-and-conditional-distributions" id="toc-joint-marginal-and-conditional-distributions" class="nav-link" data-scroll-target="#joint-marginal-and-conditional-distributions"><span class="header-section-number">4.11</span> Joint, Marginal, and Conditional Distributions</a>
  <ul class="collapse">
  <li><a href="#joint-distributions" id="toc-joint-distributions" class="nav-link" data-scroll-target="#joint-distributions"><span class="header-section-number">4.11.1</span> Joint Distributions</a></li>
  <li><a href="#marginal-distribution" id="toc-marginal-distribution" class="nav-link" data-scroll-target="#marginal-distribution"><span class="header-section-number">4.11.2</span> Marginal Distribution</a></li>
  <li><a href="#conditional-distribution" id="toc-conditional-distribution" class="nav-link" data-scroll-target="#conditional-distribution"><span class="header-section-number">4.11.3</span> Conditional Distribution</a></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance"><span class="header-section-number">4.11.4</span> Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="header-section-number">4.11.5</span> Correlation</a></li>
  </ul></li>
  <li><a href="#various-sampling-techniques-in-statistics-and-machine-learning" id="toc-various-sampling-techniques-in-statistics-and-machine-learning" class="nav-link" data-scroll-target="#various-sampling-techniques-in-statistics-and-machine-learning"><span class="header-section-number">4.12</span> Various Sampling Techniques in Statistics and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#simple-random-sampling" id="toc-simple-random-sampling" class="nav-link" data-scroll-target="#simple-random-sampling"><span class="header-section-number">4.12.1</span> Simple Random Sampling</a></li>
  <li><a href="#stratified-sampling" id="toc-stratified-sampling" class="nav-link" data-scroll-target="#stratified-sampling"><span class="header-section-number">4.12.2</span> Stratified Sampling</a></li>
  <li><a href="#cluster-sampling" id="toc-cluster-sampling" class="nav-link" data-scroll-target="#cluster-sampling"><span class="header-section-number">4.12.3</span> Cluster Sampling</a></li>
  <li><a href="#systematic-sampling" id="toc-systematic-sampling" class="nav-link" data-scroll-target="#systematic-sampling"><span class="header-section-number">4.12.4</span> Systematic Sampling</a></li>
  </ul></li>
  <li><a href="#sampling-from-distributions" id="toc-sampling-from-distributions" class="nav-link" data-scroll-target="#sampling-from-distributions"><span class="header-section-number">4.13</span> Sampling from Distributions</a>
  <ul class="collapse">
  <li><a href="#inverse-transform-sampling" id="toc-inverse-transform-sampling" class="nav-link" data-scroll-target="#inverse-transform-sampling"><span class="header-section-number">4.13.1</span> Inverse Transform Sampling</a></li>
  <li><a href="#rejection-sampling" id="toc-rejection-sampling" class="nav-link" data-scroll-target="#rejection-sampling"><span class="header-section-number">4.13.2</span> Rejection Sampling</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling"><span class="header-section-number">4.13.3</span> Importance Sampling</a></li>
  <li><a href="#markov-chain-monte-carlo-mcmc" id="toc-markov-chain-monte-carlo-mcmc" class="nav-link" data-scroll-target="#markov-chain-monte-carlo-mcmc"><span class="header-section-number">4.13.4</span> Markov Chain Monte Carlo (MCMC)</a></li>
  </ul></li>
  <li><a href="#distance-between-distributions." id="toc-distance-between-distributions." class="nav-link" data-scroll-target="#distance-between-distributions."><span class="header-section-number">4.14</span> Distance between Distributions.</a>
  <ul class="collapse">
  <li><a href="#kullback-leibler-divergence-kl-divergence" id="toc-kullback-leibler-divergence-kl-divergence" class="nav-link" data-scroll-target="#kullback-leibler-divergence-kl-divergence"><span class="header-section-number">4.14.1</span> Kullback-Leibler Divergence (KL Divergence)</a></li>
  <li><a href="#jensen-shannon-divergence-jsd" id="toc-jensen-shannon-divergence-jsd" class="nav-link" data-scroll-target="#jensen-shannon-divergence-jsd"><span class="header-section-number">4.14.2</span> Jensen-Shannon Divergence (JSD)</a></li>
  <li><a href="#earth-movers-distance-emd-or-wasserstein-distance" id="toc-earth-movers-distance-emd-or-wasserstein-distance" class="nav-link" data-scroll-target="#earth-movers-distance-emd-or-wasserstein-distance"><span class="header-section-number">4.14.3</span> Earth Mover’s Distance (EMD) or Wasserstein Distance</a></li>
  <li><a href="#hellinger-distance" id="toc-hellinger-distance" class="nav-link" data-scroll-target="#hellinger-distance"><span class="header-section-number">4.14.4</span> Hellinger Distance</a></li>
  <li><a href="#total-variation-distance-tvd" id="toc-total-variation-distance-tvd" class="nav-link" data-scroll-target="#total-variation-distance-tvd"><span class="header-section-number">4.14.5</span> Total Variation Distance (TVD)</a></li>
  </ul></li>
  <li><a href="#functions-of-random-variables" id="toc-functions-of-random-variables" class="nav-link" data-scroll-target="#functions-of-random-variables"><span class="header-section-number">4.15</span> Functions of Random Variables</a>
  <ul class="collapse">
  <li><a href="#calculating-the-distribution-of-y-gx" id="toc-calculating-the-distribution-of-y-gx" class="nav-link" data-scroll-target="#calculating-the-distribution-of-y-gx"><span class="header-section-number">4.15.1</span> Calculating the Distribution of <span class="math inline">\(Y = g(X)\)</span></a></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance"><span class="header-section-number">4.15.2</span> Expectation and Variance</a></li>
  </ul></li>
  <li><a href="#probabilistic-programming-languages" id="toc-probabilistic-programming-languages" class="nav-link" data-scroll-target="#probabilistic-programming-languages"><span class="header-section-number">4.16</span> Probabilistic Programming Languages</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this chapter, we delve into the fundamental concepts of random variables and stochastic processes, essential components of probability theory and statistical analysis in the realm of machine learning. This section aims to provide an introduction to the principles governing probability, characteristics of random variables, and the nature of stochastic processes. Central to understanding data’s inherent randomness and uncertainty, these topics are indispensable for developing a solid theoretical foundation in machine learning algorithms. This overview encompasses a spectrum of topics, from elementary probability distributions to sophisticated techniques such as Bayesian inference and Monte Carlo methods. Emphasizing both theoretical acumen and practical application, the course integrates Python-based examples to demonstrate the real-world applicability of these concepts. This overview is designed to equip students with the essential knowledge and skills to become experts in machine learning.</p>
<section id="definitions-and-probability-axioms" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="definitions-and-probability-axioms"><span class="header-section-number">4.1</span> Definitions and Probability Axioms</h2>
<p>Probability theory begins with the fundamental notion of a <strong>probability space</strong>, which consists of three elements: a sample space <span class="math inline">\(\Omega\)</span>, a set of events <span class="math inline">\(F\)</span>, and a probability measure <span class="math inline">\(P\)</span>.</p>
<ul>
<li><p><strong>Sample Space (<span class="math inline">\(\Omega\)</span>)</strong>: The set of all possible outcomes of a random experiment. For example, in a coin toss, <span class="math inline">\(\Omega = \{\text{heads}, \text{tails}\}\)</span>.</p></li>
<li><p><strong>Events (<span class="math inline">\(F\)</span>)</strong>: A collection of outcomes (a subset of <span class="math inline">\(\Omega\)</span>). An event is said to occur if the outcome of the experiment is in this set.</p></li>
<li><p><strong>Probability Measure (<span class="math inline">\(P\)</span>)</strong>: A function that assigns a probability to each event in <span class="math inline">\(F\)</span>, satisfying the following axioms:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: For every event <span class="math inline">\(A \in F\)</span>, <span class="math inline">\(P(A) \geq 0\)</span>.</li>
<li><strong>Normalization</strong>: <span class="math inline">\(P(\Omega) = 1\)</span>.</li>
<li><strong>Additivity</strong>: For any sequence of mutually exclusive events <span class="math inline">\(A_1, A_2, \ldots\)</span>, <span class="math inline">\(P(\bigcup_{i} A_i) = \sum_{i} P(A_i)\)</span>.</li>
</ol></li>
</ul>
</section>
<section id="conditional-probability" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="conditional-probability"><span class="header-section-number">4.2</span> Conditional Probability</h2>
<p>Conditional probability refers to the probability of an event given that another event has occurred, denoted as <span class="math inline">\(P(A|B)\)</span>for events <span class="math inline">\(A\)</span>and <span class="math inline">\(B\)</span>. It is defined as: <span class="math display">\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]</span> provided that <span class="math inline">\(P(B) &gt; 0\)</span>.</p>
</section>
<section id="bayes-theorem" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">4.3</span> Bayes’ Theorem</h2>
<p>Bayes’ theorem provides a way to update our probability estimates based on new information. It is expressed as: <span class="math display">\[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]</span> where:</p>
<ul>
<li><span class="math inline">\(P(A|B)\)</span> is the posterior probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>.</li>
<li><span class="math inline">\(P(B|A)\)</span> is the likelihood of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(A)\)</span> is the prior probability of <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(B)\)</span> is the marginal probability of <span class="math inline">\(B\)</span>.</li>
</ul>
<p>Bayes’ theorem is foundational in various machine learning algorithms, especially in Bayesian inference, where it is used to update the probability estimate for a hypothesis as more evidence or information becomes available.</p>
</section>
<section id="discrete-random-variables" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="discrete-random-variables"><span class="header-section-number">4.4</span> Discrete Random Variables</h2>
<ul>
<li><strong>Definition</strong>: A discrete random variable is a type of random variable that can take on a countable number of distinct outcomes. Common examples include the number of heads in a coin toss or the number of successes in a series of Bernoulli trials.</li>
<li><strong>Probability Mass Function (PMF)</strong>: The PMF of a discrete random variable provides the probabilities of all possible outcomes. For a discrete random variable <span class="math inline">\(X\)</span>, the PMF <span class="math inline">\(P(X=x)\)</span> gives the probability that <span class="math inline">\(X\)</span> equals a particular value <span class="math inline">\(x\)</span>.</li>
</ul>
<section id="continuous-random-variables" class="level3" data-number="4.4.1">
<h3 data-number="4.4.1" class="anchored" data-anchor-id="continuous-random-variables"><span class="header-section-number">4.4.1</span> Continuous Random Variables</h3>
<ul>
<li><strong>Definition</strong>: A continuous random variable is a random variable that can take on an uncountably infinite number of possible values. For example, the exact time it takes for a chemical reaction to occur or the precise temperature at a given location.</li>
<li><strong>Probability Density Function (PDF)</strong>: For continuous random variables, the PDF is used to specify the probability of the random variable falling within a particular range of values. The probability of the variable falling within an interval is given by the integral of the PDF over that interval.</li>
</ul>
</section>
</section>
<section id="cumulative-distribution-functions-cdfs" class="level2" data-number="4.5">
<h2 data-number="4.5" class="anchored" data-anchor-id="cumulative-distribution-functions-cdfs"><span class="header-section-number">4.5</span> Cumulative Distribution Functions (CDFs)</h2>
<ul>
<li><strong>Definition</strong>: The CDF is a function that gives the probability that a random variable is less than or equal to a certain value. It applies to both discrete and continuous random variables.</li>
<li><strong>Expression</strong>: For a random variable <span class="math inline">\(X\)</span>, the CDF <span class="math inline">\(F(x)\)</span> is defined as <span class="math inline">\(F(x) = P(X \leq x)\)</span>. In the case of a discrete random variable, it is the sum of the probabilities up to <span class="math inline">\(x\)</span>; for a continuous variable, it is the integral of the PDF up to <span class="math inline">\(x\)</span>.</li>
</ul>
<p>We’ll use Python to generate illustrations for PMFs, PDFs, and CDFs. For PMFs, we’ll consider a simple binomial distribution, and for PDFs and CDFs, we’ll use a normal distribution.</p>
<div>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom, norm</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co"># PMF of a Binomial Distribution</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> <span class="dv">10</span>, <span class="fl">0.5</span>  <span class="co"># Parameters for the binomial distribution</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>x_binom <span class="op">=</span> np.arange(<span class="dv">0</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>pmf_binom <span class="op">=</span> binom.pmf(x_binom, n, p)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">4</span>))</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>plt.bar(x_binom, pmf_binom)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PMF of a Binomial Distribution'</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Successes'</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># PDF and CDF of a Normal Distribution</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>  <span class="co"># Mean and standard deviation</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>x_norm <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1000</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>pdf_norm <span class="op">=</span> norm.pdf(x_norm, mu, sigma)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>cdf_norm <span class="op">=</span> norm.cdf(x_norm, mu, sigma)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>plt.plot(x_norm, pdf_norm)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PDF of a Normal Distribution'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>plt.plot(x_norm, cdf_norm)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'CDF of a Normal Distribution'</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cumulative Probability'</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="1">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-2-output-1.png" class="img-fluid figure-img" width="1141"></p>
<figcaption class="figure-caption">The Python code creates a figure with three subplots: the first showing the PMF of a binomial distribution, the second showing the PDF of a normal distribution, and the third displaying the CDF of the same normal distribution.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="expected-value" class="level2" data-number="4.6">
<h2 data-number="4.6" class="anchored" data-anchor-id="expected-value"><span class="header-section-number">4.6</span> Expected Value</h2>
<ul>
<li><p><strong>Definition</strong>: The expected value (or mean) of a random variable is a measure of the central tendency of the variable’s probability distribution. Mathematically, for a discrete random variable <span class="math inline">\(X\)</span> with possible values <span class="math inline">\(x_1, x_2, \ldots\)</span> and a probability mass function <span class="math inline">\(P(X)\)</span>, the expected value <span class="math inline">\(\mathbb{E}\left[X\right]\)</span> is defined as: <span class="math display">\[ \mathbb{E}\left[X\right] = \sum_{i} x_i P(X = x_i).\]</span> For a continuous random variable with a probability density function <span class="math inline">\(f(x)\)</span>, it is defined as: <span class="math display">\[ \mathbb{E}\left[X\right] = \int_{-\infty}^{\infty} x f(x) dx \]</span></p></li>
<li><p><strong>Properties</strong>: The expected value is linear, meaning for any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we have <span class="math inline">\(\mathbb{E}\left[aX + bY\right] = a\mathbb{E}\left[X\right] + b\mathbb{E}\left[Y\right]\)</span>.</p></li>
<li><p><strong>Significance in Machine Learning</strong>: The expected value is used in various machine learning contexts, such as defining the loss functions (e.g., mean squared error) and as a measure of central tendency in data analysis.</p></li>
</ul>
</section>
<section id="variance" class="level2" data-number="4.7">
<h2 data-number="4.7" class="anchored" data-anchor-id="variance"><span class="header-section-number">4.7</span> Variance</h2>
<ul>
<li><p><strong>Definition</strong>: The variance of a random variable measures the spread of its values. It is defined as the expected value of the squared deviation from the mean. For a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu = \mathbb{E}\left[X\right]\)</span>, the variance <span class="math inline">\(\text{Var}(X)\)</span> is given by: <span class="math display">\[ \text{Var}(X) = \mathbb{E}\left[(X - \mu\right]^2] \]</span> which can also be written as: <span class="math display">\[\text{Var}(X) = \mathbb{E}\left[X^2\right] - (\mathbb{E}\left[X\right])^2\]</span></p></li>
<li><p><strong>Properties</strong>: Variance measures the dispersion of the data. A high variance indicates that the data points are spread out from the mean, while a low variance indicates that they are clustered closely around the mean.</p></li>
<li><p><strong>Significance in Machine Learning</strong>: In machine learning, variance is a key concept in understanding the model’s behavior, particularly in the bias-variance tradeoff, which is crucial for understanding model performance and overfitting.</p></li>
</ul>
<p>We’ll use Python to illustrate the computation of expected value and variance for a simple discrete distribution (e.g., rolling a fair six-sided die) and a continuous distribution (e.g., normal distribution).</p>
<div>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> randint, norm</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected Value and Variance of a Discrete Random Variable (Die Roll)</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>values <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="dv">7</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>probabilities <span class="op">=</span> np.full(<span class="dv">6</span>, <span class="dv">1</span><span class="op">/</span><span class="dv">6</span>)  <span class="co"># Fair die</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>expected_value_die <span class="op">=</span> np.<span class="bu">sum</span>(values <span class="op">*</span> probabilities)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>variance_die <span class="op">=</span> np.<span class="bu">sum</span>((values <span class="op">-</span> expected_value_die)<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> probabilities)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected Value (Die Roll):"</span>, expected_value_die)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance (Die Roll):"</span>, variance_die)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Expected Value and Variance of a Continuous Random Variable (Normal Distribution)</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>  <span class="co"># Mean and standard deviation</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">1000</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>pdf <span class="op">=</span> norm.pdf(x, mu, sigma)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>expected_value_normal <span class="op">=</span> np.<span class="bu">sum</span>(x <span class="op">*</span> pdf) <span class="op">*</span> (x[<span class="dv">1</span>] <span class="op">-</span> x[<span class="dv">0</span>])  <span class="co"># Approximate integral</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>variance_normal <span class="op">=</span> np.<span class="bu">sum</span>(((x <span class="op">-</span> mu)<span class="op">**</span><span class="dv">2</span>) <span class="op">*</span> pdf) <span class="op">*</span> (x[<span class="dv">1</span>] <span class="op">-</span> x[<span class="dv">0</span>])</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Expected Value (Normal Distribution):"</span>, expected_value_normal)</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Variance (Normal Distribution):"</span>, variance_normal)</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>plt.bar(values, probabilities)</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PMF of a Fair Die'</span>)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Die Value'</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plt.plot(x, pdf)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'PDF of a Normal Distribution'</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="2">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<pre><code>Expected Value (Die Roll): 3.5
Variance (Die Roll): 2.9166666666666665
Expected Value (Normal Distribution): 7.112539897498498e-17
Variance (Normal Distribution): 0.9999849286984711</code></pre>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-3-output-2.png" class="img-fluid figure-img" width="969"></p>
<figcaption class="figure-caption">This code calculates and prints the expected value and variance for both a fair die roll (discrete) and a normal distribution (continuous). It also provides visualizations of their probability distributions. These examples demonstrate how expected value and variance are computed and their importance in describing the characteristics of a distribution.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="law-of-large-numbers" class="level2" data-number="4.8">
<h2 data-number="4.8" class="anchored" data-anchor-id="law-of-large-numbers"><span class="header-section-number">4.8</span> Law of Large Numbers</h2>
<p>The Law of Large Numbers (LLN) is a fundamental theorem in probability theory that describes the result of performing the same experiment a large number of times. It states that as the number of trials or observations increases, the average of the results obtained from these trials converges to the expected value. This theorem provides a solid foundation for the concept of statistical stability and the predictability of outcomes in probability and statistics.</p>
<p>There are two types of Law of Large Numbers:</p>
<ol type="1">
<li><p><strong>Weak Law of Large Numbers (WLLN)</strong>: This version, also known as Khinchin’s Law, asserts that the sample mean converges in probability towards the expected value as the sample size increases. Mathematically, for a sequence of independent and identically distributed (i.i.d.) random variables <span class="math inline">\(X_1, X_2, ..., X_n\)</span> with expected value <span class="math inline">\(\mathbb{E}\left[X\right] = \mu\)</span>, the WLLN states that for any positive number <span class="math inline">\(\epsilon\)</span>, <span class="math display">\[ P\left( \left| \frac{1}{n}\sum_{i=1}^n X_i - \mu \right| &lt; \epsilon \right) \to 1 \text{ as } n \to \infty. \]</span> This means the probability that the sample mean differs from the true mean by more than <span class="math inline">\(\epsilon\)</span> tends to zero as <span class="math inline">\(n\)</span> becomes large.</p></li>
<li><p><strong>Strong Law of Large Numbers (SLLN)</strong>: The SLLN states that the sample mean almost surely converges to the expected value as the sample size goes to infinity. In other words, the sample mean and the expected value will be equal with probability 1 in the limit of an infinite number of trials. This is a stronger statement than the WLLN and requires slightly stronger conditions.</p></li>
</ol>
<p>Some application of LLNs include:</p>
<ul>
<li><strong>Empirical Predictability</strong>: LLN explains why averages of larger samples are more stable and reliable than those of smaller samples, a principle that underpins much of empirical science.</li>
<li><strong>Statistics and Data Analysis</strong>: In statistics, LLN is crucial for the justification of using sample means as estimates for population means.</li>
<li><strong>Financial Modeling</strong>: In finance, LLN helps in predicting long-term investment outcomes based on historical averages.</li>
<li><strong>Quality Control</strong>: In industrial processes, LLN is used to understand that averaging the results of a process over a long period will give a good estimation of the overall process performance.</li>
</ul>
<p>The Law of Large Numbers is a cornerstone of probability theory and statistics, providing a rationale for the apparent regularity of large systems and the basis for making inferences about population parameters based on sample statistics. It essentially underlines the reliability of averages in large datasets, a key concept in many practical applications across various disciplines.</p>
<p>A simple demonstration of LLN is achieved by the following Python code.</p>
<div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the number of trials</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>trials <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>, <span class="dv">10000</span>]</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a random variable (e.g., a normal distribution)</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>std_dev <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Store the average outcomes</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>averages <span class="op">=</span> []</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate the trials</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> trials:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> np.random.normal(mean, std_dev, n)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    average <span class="op">=</span> np.mean(samples)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    averages.append(average)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the results</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.plot(trials, averages, marker<span class="op">=</span><span class="st">'o'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, color<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.axhline(y<span class="op">=</span>mean, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Demonstration of the Law of Large Numbers'</span>)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>plt.xscale(<span class="st">'log'</span>)  <span class="co"># Log scale for better visualization</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Number of Trials (log scale)'</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Sample Mean'</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="3">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-4-output-1.png" class="img-fluid figure-img" width="812"></p>
<figcaption class="figure-caption">In the plot, the x-axis represents the number of trials (on a logarithmic scale for clarity), ranging from small to large sample sizes. The y-axis shows the sample mean for each set of trials. The red dashed line indicates the true mean of the distribution (0 in this case). We observe that as the number of samples increases, the emprical mean converges to the true mean.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>The Python script demonstrates the Law of Large Numbers (LLN) by simulating random samples from a normal distribution with a mean of 0 and a standard deviation of 1. As the number of trials increases, the script calculates the sample mean for each set of trials and plots these means. As illustrated in the plot, as the number of trials increases, the sample mean converges towards the true mean of the distribution, consistent with the Law of Large Numbers. This demonstrates that with a larger number of observations, the average of the outcomes tends to get closer to the expected value, showcasing the LLN’s fundamental principle in probability and statistics.</p>
</section>
<section id="central-limit-theorem" class="level2" data-number="4.9">
<h2 data-number="4.9" class="anchored" data-anchor-id="central-limit-theorem"><span class="header-section-number">4.9</span> Central Limit Theorem</h2>
<p>The Central Limit Theorem (CLT) is a fundamental principle in probability theory that states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables will be approximately normally distributed, regardless of the underlying distribution of the individual variables. This theorem is pivotal in statistics because it justifies the use of the normal distribution in many real-world situations.</p>
<p>Key Points of the CLT:</p>
<ol type="1">
<li><strong>Normal Approximation</strong>: As the sample size grows, the distribution of the sample mean will approach a normal distribution, even if the original variables are not normally distributed.</li>
<li><strong>Conditions</strong>: The CLT holds under certain conditions, primarily that the variables are independent and identically distributed with a finite mean and variance.</li>
<li><strong>Sample Mean and Variance</strong>: The mean of the sample means will be equal to the mean of the original distribution, and the variance of the sample means will be equal to the variance of the original distribution divided by the sample size.</li>
</ol>
<p>The Central Limit Theorem and the Law of Large Numbers (LLN) are related but address different aspects of convergence:</p>
<ul>
<li><p><strong>Law of Large Numbers</strong>: The LLN focuses on the convergence of the sample mean to the expected value as the sample size increases. It asserts that the average of a large number of i.i.d. random variables will be close to the expected value, providing a strong foundation for statistical estimation.</p></li>
<li><p><strong>Central Limit Theorem</strong>: The CLT takes this a step further by describing the shape of the distribution of the sample mean. It not only asserts that the sample mean will converge to the expected value but also that the way in which it converges will follow a normal distribution if the sample size is large enough. This is crucial for hypothesis testing and confidence interval estimation.</p></li>
</ul>
<p>In summary, while the LLN tells us that the sample mean will be a good estimate of the population mean for large sample sizes, the CLT tells us about the distribution of this estimate, enabling the use of normal distribution-based inference methods even when the underlying data does not follow a normal distribution.</p>
<p>The following Python script demonstrates the Central Limit Theorem (CLT) by generating distributions of sample means from an original normal distribution (with a mean of 5 and standard deviation of 2) for different sample sizes. The script creates multiple samples for each specified sample size and computes the mean of each sample.</p>
<div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Settings for the demonstration</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>sample_sizes <span class="op">=</span> [<span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>number_of_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>original_distribution_mean <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>original_distribution_std <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Function to generate sample means</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_sample_means(sample_size, number_of_samples, mean, std):</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [np.mean(np.random.normal(mean, std, sample_size)) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(number_of_samples)]</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the distributions of sample means</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample_size <span class="kw">in</span> sample_sizes:</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    sample_means <span class="op">=</span> generate_sample_means(sample_size, number_of_samples, original_distribution_mean, original_distribution_std)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    plt.hist(sample_means, bins<span class="op">=</span><span class="dv">30</span>, alpha<span class="op">=</span><span class="fl">0.5</span>, label<span class="op">=</span><span class="ss">f'Sample Size = </span><span class="sc">{</span>sample_size<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Central Limit Theorem Demonstration'</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Sample Mean'</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="4">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img" width="965"></p>
<figcaption class="figure-caption">In the figure, each histogram represents the distribution of sample means for a particular sample size. As the sample size increases, the distribution of the sample means becomes more tightly clustered around the true mean of the original distribution (5 in this case). The shape of these distributions of sample means increasingly resembles a normal distribution, especially as the sample size gets larger.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>The plot effectively illustrates the essence of the CLT: regardless of the original distribution (which is normal in this case but could be any distribution with a defined mean and variance), the distribution of the sample means approaches a normal distribution as the sample size increases. This phenomenon underpins many statistical methods, especially those involving hypothesis testing and confidence interval estimation.</p>
</section>
<section id="common-distributions" class="level2" data-number="4.10">
<h2 data-number="4.10" class="anchored" data-anchor-id="common-distributions"><span class="header-section-number">4.10</span> Common Distributions</h2>
<section id="binomial-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h5>
<ul>
<li><strong>Description</strong>: The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.</li>
<li><strong>Parameters</strong>: Number of trials <span class="math inline">\(n\)</span> and probability of success <span class="math inline">\(p\)</span> in each trial.</li>
<li><strong>PMF</strong>: <span class="math inline">\(P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\)</span> for <span class="math inline">\(k = 0, 1, 2, \ldots, n\)</span>.</li>
</ul>
</section>
<section id="poisson-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h5>
<ul>
<li><strong>Description</strong>: The Poisson distribution models the number of times an event occurs in a fixed interval of time or space.</li>
<li><strong>Parameter</strong>: Average number of occurrences <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>PMF</strong>: <span class="math inline">\(P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\)</span> for <span class="math inline">\(k = 0, 1, 2, \ldots\)</span>.</li>
</ul>
</section>
<section id="gaussian-normal-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="gaussian-normal-distribution">Gaussian (Normal) Distribution</h5>
<ul>
<li><strong>Description</strong>: The Gaussian distribution is a continuous distribution that is symmetric around its mean, showing the familiar bell-shaped curve.</li>
<li><strong>Parameters</strong>: Mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}\)</span>.</li>
</ul>
</section>
<section id="uniform-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h5>
<ul>
<li><strong>Description</strong>: The uniform distribution describes an experiment where each outcome is equally likely.</li>
<li><strong>Parameters</strong>: Lower bound <span class="math inline">\(a\)</span> and upper bound <span class="math inline">\(b\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \frac{1}{b - a}\)</span> for <span class="math inline">\(x\)</span> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</li>
</ul>
</section>
<section id="exponential-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="exponential-distribution">Exponential Distribution</h5>
<ul>
<li><strong>Description</strong>: The exponential distribution models the time between events in a Poisson point process.</li>
<li><strong>Parameter</strong>: Rate <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x \geq 0\)</span>.</li>
</ul>
<p>Let’s illustrate these distributions using Python:</p>
<div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> binom, poisson, norm, uniform, expon</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>n, p <span class="op">=</span> <span class="dv">10</span>, <span class="fl">0.5</span>  <span class="co"># Binomial parameters</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="dv">3</span>     <span class="co"># Poisson parameter</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>mu, sigma <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span>  <span class="co"># Gaussian parameters</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="dv">0</span>, <span class="dv">10</span>     <span class="co"># Uniform parameters</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>lambda_exp <span class="op">=</span> <span class="dv">1</span>   <span class="co"># Exponential parameter</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Data for plotting</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>x_binom <span class="op">=</span> np.arange(<span class="dv">0</span>, n<span class="op">+</span><span class="dv">1</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>x_poisson <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">20</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>x_gaussian <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1000</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>x_uniform <span class="op">=</span> np.linspace(a, b, <span class="dv">1000</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>x_exponential <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">3</span>))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.bar(x_binom, binom.pmf(x_binom, n, p))</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Binomial Distribution'</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">2</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plt.bar(x_poisson, poisson.pmf(x_poisson, lambda_))</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Poisson Distribution'</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">3</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>plt.plot(x_gaussian, norm.pdf(x_gaussian, mu, sigma))</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Gaussian Distribution'</span>)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">4</span>)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>plt.plot(x_uniform, uniform.pdf(x_uniform, a, b<span class="op">-</span>a))</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Uniform Distribution'</span>)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>plt.plot(x_exponential, expon.pdf(x_exponential, scale<span class="op">=</span><span class="dv">1</span><span class="op">/</span>lambda_exp))</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Exponential Distribution'</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="5">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img" width="1435"></p>
<figcaption class="figure-caption">This code will generate a series of plots illustrating the probability mass or density functions for each of the five distributions. These visualizations help in understanding the characteristics and differences of these common distributions, which are widely used in various machine learning and statistical modeling scenarios.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="joint-marginal-and-conditional-distributions" class="level2" data-number="4.11">
<h2 data-number="4.11" class="anchored" data-anchor-id="joint-marginal-and-conditional-distributions"><span class="header-section-number">4.11</span> Joint, Marginal, and Conditional Distributions</h2>
<p>Understanding the relationships between multiple random variables is crucial in statistics and machine learning. Here we’ll discuss joint, marginal, and conditional distributions.</p>
<section id="joint-distributions" class="level3" data-number="4.11.1">
<h3 data-number="4.11.1" class="anchored" data-anchor-id="joint-distributions"><span class="header-section-number">4.11.1</span> Joint Distributions</h3>
<p>Joint distributions are fundamental in understanding the relationship between two or more random variables. They can be categorized based on whether the variables involved are discrete, continuous, or a combination of both.</p>
<p>For discrete random variables, the joint distribution is defined by a joint probability mass function (PMF). This function gives the probability that each of the random variables falls within a specific range of values. For discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint PMF <span class="math inline">\(P(X = x, Y = y)\)</span> provides the probability that <span class="math inline">\(X = x\)</span> and <span class="math inline">\(Y = y\)</span> simultaneously. Probabilities are calculated by summing the joint PMF over the desired range of values. They have the following properties:</p>
<ul>
<li>The sum of the joint PMF over all possible values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is 1.</li>
<li>The joint PMF is always non-negative: <span class="math inline">\(P(X = x, Y = y) \geq 0\)</span>.</li>
</ul>
<p>In the case of continuous random variables, the joint distribution is described by a joint probability density function (PDF). The joint PDF <span class="math inline">\(f_{X,Y}(x, y)\)</span> represents the density of probabilities at any point in the range of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Probabilities are calculated by integrating the joint PDF over the region of interest. They have the following properties:</p>
<ul>
<li>The integral of the joint PDF over the entire space is 1.</li>
<li>The joint PDF is non-negative: <span class="math inline">\(f_{X,Y}(x, y) \geq 0\)</span>.</li>
</ul>
<p>Joint distributions are used in machine learning for modeling the relationships between features, in probabilistic classifiers, and in Bayesian inference, where understanding the dependence between variables is crucial. Joint distributions also form a core concept in statistics and machine learning, as they provide a comprehensive picture of how multiple random variables interact with each other.</p>
<section id="homogenous-joint-distributions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homogenous-joint-distributions">Homogenous Joint Distributions</h4>
<p>Homogeneous joint distributions refer to joint probability distributions where the involve random variables that follow identical types of distributions. They also refer to probability distributions where the statistical properties and relationships between two or more random variables remain consistent across the entire range of their values. The following are some examples of homegenous joint distributions:</p>
<p><strong>Joint Probability Mass Function (Discrete Random Variables)</strong><br> Consider the outcomes from tossing two dice. Let <span class="math inline">\(X\)</span> be the outcome of the first die, and <span class="math inline">\(Y\)</span> be the outcome of the second die. Both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables taking values from 1 to 6. The joint PMF <span class="math inline">\(P(X = x, Y = y)\)</span> is <span class="math inline">\(\frac{1}{36}\)</span> for <span class="math inline">\(x, y \in \{1, 2, 3, 4, 5, 6\}\)</span>, assuming fair dice.</p>
<p><strong>Joint Probability Density Function (Continuous Random Variables)</strong><br> Consider a height and weight distribution. Let <span class="math inline">\(X\)</span> be a person’s height (in cm) and <span class="math inline">\(Y\)</span> their weight (in kg). Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a joint PDF given by <span class="math inline">\(f(x, y) = k \cdot e^{-2x - 3y}\)</span> for <span class="math inline">\(x, y &gt; 0\)</span> (a hypothetical example). Here, <span class="math inline">\(k\)</span> is a normalizing constant to ensure that the total probability integrates to 1.</p>
<p><strong>Bivariate Normal Distribution</strong><br> A common example in statistics is the bivariate normal distribution. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly normally distributed with means <span class="math inline">\(\mu_X, \mu_Y\)</span>, standard deviations <span class="math inline">\(\sigma_X, \sigma_Y\)</span>, and correlation coefficient <span class="math inline">\(\rho\)</span>. The joint PDF is: <span class="math display">\[ f(x, y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma_Y^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}\right]\right).\]</span></p>
<p><strong>Multivariate Gaussian Distribution</strong><br> The Multivariate Gaussian (or Normal) Distribution is a generalization of the one-dimensional Gaussian distribution to multiple dimensions. It describes a random vector in <span class="math inline">\(n\)</span>-dimensional space with a specific correlation structure between its elements.</p>
<p>A random vector <span class="math inline">\(\boldsymbol{x}= [x_1, x_2, ..., x_n]^T\)</span> is said to follow a multivariate Gaussian distribution if its probability density function (PDF) is given by: <span class="math display">\[ f(\mathbf{x}; \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \frac{1}{\sqrt{(2\pi)^n |\boldsymbol{\Sigma}|}} \exp\left(-\frac{1}{2}(\mathbf{x} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu})\right),\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\boldsymbol{x}\)</span> is a realization of the random vector <span class="math inline">\(\boldsymbol{x}\)</span>.</li>
<li><span class="math inline">\(\boldsymbol{\mu}\)</span> is the mean vector, and $$is the covariance matrix.</li>
<li><span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> is the determinant of the covariance matrix.</li>
<li><span class="math inline">\(\boldsymbol{\Sigma}^{-1}\)</span> is the inverse of the covariance matrix.</li>
</ul>
<p>It has the following properties:</p>
<ul>
<li><strong>Mean Vector</strong>: <span class="math inline">\(\boldsymbol{\mu}\)</span> specifies the mean of each component of the vector <span class="math inline">\(\boldsymbol{x}\)</span>.</li>
<li><strong>Covariance Matrix</strong>: <span class="math inline">\(\boldsymbol{\Sigma}\)</span> describes the variance of each component and the covariances between them. The diagonal elements of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> are the variances of the individual components, and the off-diagonal elements are the covariances.</li>
<li><strong>Correlation Structure</strong>: The covariance matrix determines how the components of <span class="math inline">\(\boldsymbol{x}\)</span> are linearly related to each other.</li>
<li><strong>Shape and Geometry</strong>: The shape of the distribution in <span class="math inline">\(n\)</span>-dimensional space can range from spherical (when <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is proportional to the identity matrix) to elliptical (when <span class="math inline">\(\boldsymbol{\Sigma}\)</span> has distinct eigenvalues).</li>
</ul>
<p><strong>Uniform Distribution Over a Region</strong><br> Consider distribution of random points in a rectangle. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> represent the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates of a point uniformly distributed over a rectangle <span class="math inline">\([a, b] \times [c, d]\)</span>. The joint PDF is <span class="math inline">\(f(x, y) = \frac{1}{(b-a)(d-c)}\)</span> for <span class="math inline">\(a \leq x \leq b\)</span> and <span class="math inline">\(c \leq y \leq d\)</span>, and 0 otherwise.</p>
</section>
<section id="heterogenous-joint-distributions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="heterogenous-joint-distributions">Heterogenous Joint Distributions</h4>
<p>Joint distibutions need not be homogenous. They can be heterogenous as well. Heterogeneous joint distributions involve random variables that may follow different types of distributions, and they can capture complex relationships in probabilistic models. Here are some mathematical examples:</p>
<p><strong>Joint Distribution of a Discrete and a Continuous Variable</strong><br> Suppose <span class="math inline">\(X\)</span> is a discrete random variable representing the number of defects in a product (following a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>), and <span class="math inline">\(Y\)</span> is a continuous random variable representing the time (in hours) until the first defect is detected (following an Exponential distribution with rate <span class="math inline">\(\lambda\)</span>). The joint probability function is not a standard PMF or PDF but a mixed distribution that combines the characteristics of both. The joint distribution might be defined as <span class="math inline">\(f(x, y) = P(X = x) \cdot f_Y(y|x)\)</span>, where <span class="math inline">\(f_Y(y|x)\)</span> is the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>.</p>
<p><strong>Joint Distribution with Conditional Dependence</strong><br> Let <span class="math inline">\(X\)</span> be a Bernoulli random variable indicating whether an event occurs (1) or not (0), and <span class="math inline">\(Y\)</span> be a normally distributed variable representing the outcome measurement, whose mean depends on <span class="math inline">\(X\)</span>. For instance, if <span class="math inline">\(X = 1\)</span>, <span class="math inline">\(Y\)</span> follows <span class="math inline">\(N(\mu_1, \sigma^2)\)</span>, and if <span class="math inline">\(X = 0\)</span>, <span class="math inline">\(Y\)</span> follows <span class="math inline">\(N(\mu_0, \sigma^2)\)</span>. The joint distribution is a combination of a Bernoulli and a conditional normal distribution.</p>
<p><strong>Bivariate Distribution with Different Marginals</strong><br> Consider a scenario with two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where <span class="math inline">\(X\)</span> follows a Uniform distribution <span class="math inline">\(U(a, b)\)</span> and <span class="math inline">\(Y\)</span>, conditionally on <span class="math inline">\(X\)</span>, follows a Normal distribution with mean <span class="math inline">\(X\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The joint distribution in this case would be <span class="math inline">\(f(x, y) = \frac{1}{b-a} \cdot \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-x)^2}{2\sigma^2}\right)\)</span> for <span class="math inline">\(a \leq x \leq b\)</span>.</p>
<p><strong>Piecewise Joint Distribution</strong><br> Imagine a joint distribution where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> follow different distributions in different regions of their support. For example, for <span class="math inline">\(x &lt; 0\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> might jointly follow one distribution (e.g., bivariate normal), and for <span class="math inline">\(x \geq 0\)</span>, they follow a different distribution (e.g., bivariate exponential).</p>
<p>These heterogeneous joint distributions are particularly useful in modeling complex real-world phenomena where different variables exhibit different types of probabilistic behavior or where their behavior changes under different conditions. Such models are widely used in areas like econometrics, biostatistics, and environmental science.</p>
<p>Some practical examples illustrating such heterogeneous joint distributions are:</p>
<ol type="1">
<li><p>Consider a situation where <span class="math inline">\(X\)</span> is a discrete random variable representing the number of customers arriving at a store (which can be modeled by a Poisson distribution), and <span class="math inline">\(Y\)</span> is a continuous random variable representing the total amount spent by these customers (which could be modeled by a normal distribution). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be a mix of a discrete and a continuous distribution.</p></li>
<li><p>Suppose <span class="math inline">\(X\)</span> is a binary variable indicating whether a machine is in operation (1) or not (0), and <span class="math inline">\(Y\)</span> is a continuous variable representing the temperature of the machine when it is operational. The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> combines a Bernoulli distribution (for <span class="math inline">\(X\)</span>) and a conditional continuous distribution (for <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = 1\)</span>). This is an example of a joint distribution with binary and continuous variables.</p></li>
<li><p>Imagine a survey where <span class="math inline">\(X\)</span> is a nominal variable representing a respondent’s preferred type of music (e.g., rock, jazz, classical), and <span class="math inline">\(Y\)</span> is an ordinal variable indicating satisfaction level (e.g., unsatisfied, neutral, satisfied). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be a cross-tabulation of these categories, showing the frequencies or probabilities of each combination. This is an example of a joint distribution of nominal and ordinal variables.</p></li>
<li><p>Consider a scenario where <span class="math inline">\(X\)</span> is the number of products sold (following a Poisson distribution), and <span class="math inline">\(Y\)</span> is the number of customer complaints (which could follow a different discrete distribution, such as a binomial distribution). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would describe the relationship between sales and complaints, but each variable follows its own distinct distribution. This is an example of a joint distribution with multiple discrete variables with different distributions.</p></li>
</ol>
<p>These examples of heterogeneous joint distributions highlight the complexity and diversity of real-world data, where variables can be of different types and follow different distributions, yet their interdependence can be crucial for analysis and decision-making.</p>
</section>
</section>
<section id="marginal-distribution" class="level3" data-number="4.11.2">
<h3 data-number="4.11.2" class="anchored" data-anchor-id="marginal-distribution"><span class="header-section-number">4.11.2</span> Marginal Distribution</h3>
<p>Marginal distributions are used to describe the probability distribution of a subset of a collection of variables, irrespective of the values of the other variables. They are particularly important in multivariate analysis, where we have multiple interrelated random variables. The marginal distribution of a variable is the probability distribution of that variable alone, obtained by summing (in the discrete case) or integrating (in the continuous case) the joint distribution over the other variable.</p>
<p>For discrete variables, <span class="math display">\[ P(X = x) = \sum_{y} P(X = x, Y = y). \]</span></p>
<p>For continuous random variables, the marginal distribution of a variable is obtained by integrating the joint probability density function (PDF) over the range of the other variables. Suppose <span class="math inline">\(f(x, y)\)</span> is the joint PDF of two continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The marginal PDFs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are given by:</p>
<ul>
<li><p><strong>Marginal PDF of <span class="math inline">\(X\)</span></strong>: <span class="math display">\[ f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy.\]</span></p></li>
<li><p><strong>Marginal PDF of <span class="math inline">\(Y\)</span></strong>: <span class="math display">\[ f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx.\]</span></p></li>
</ul>
<p>These integrals sum up the joint probability over all possible values of the other variable, effectively ‘collapsing’ the joint distribution into a single-variable distribution.</p>
<p>Marginal distributions are significant in machine learning in the following way:</p>
<ol type="1">
<li><p><strong>Feature Analysis</strong>: Understanding the marginal distribution of each feature can provide insights into the data’s structure and inform preprocessing and feature engineering steps.</p></li>
<li><p><strong>Model Assumptions</strong>: Certain models assume independence between features, which can be examined by comparing joint and marginal distributions.</p></li>
<li><p><strong>Probabilistic Models</strong>: In probabilistic modeling, marginal distributions are crucial for making predictions and understanding the behavior of a system when only partial information is available.</p></li>
</ol>
<div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> multivariate_normal</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.ticker <span class="im">import</span> NullFormatter</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Parameters for the bivariate Gaussian</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>mean <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>cov <span class="op">=</span> [[<span class="dv">1</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">1</span>]]  <span class="co"># Covariance matrix</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of (x,y) values</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Bivariate Gaussian distribution</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>rv <span class="op">=</span> multivariate_normal(mean, cov)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> rv.pdf(np.dstack((X, Y)))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Marginal distributions</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>marginal_x <span class="op">=</span> multivariate_normal(mean[<span class="dv">0</span>], cov[<span class="dv">0</span>][<span class="dv">0</span>])</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>marginal_y <span class="op">=</span> multivariate_normal(mean[<span class="dv">1</span>], cov[<span class="dv">1</span>][<span class="dv">1</span>])</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Create figure</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Define axes</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>left, width <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.65</span></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>bottom, height <span class="op">=</span> <span class="fl">0.1</span>, <span class="fl">0.65</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>spacing <span class="op">=</span> <span class="fl">0.005</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>rect_scatter <span class="op">=</span> [left, bottom, width, height]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>rect_histx <span class="op">=</span> [left, bottom <span class="op">+</span> height <span class="op">+</span> spacing, width, <span class="fl">0.2</span>]</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>rect_histy <span class="op">=</span> [left <span class="op">+</span> width <span class="op">+</span> spacing, bottom, <span class="fl">0.2</span>, height]</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Add axes to the figure</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>ax_scatter <span class="op">=</span> plt.axes(rect_scatter)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>ax_histx <span class="op">=</span> plt.axes(rect_histx)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>ax_histy <span class="op">=</span> plt.axes(rect_histy)</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a><span class="co"># No labels for the additional axes</span></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>ax_histx.xaxis.set_major_formatter(NullFormatter())</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>ax_histy.yaxis.set_major_formatter(NullFormatter())</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Contour plot</span></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>ax_scatter.contour(X, Y, Z)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot marginal distributions</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>ax_histx.plot(x, marginal_x.pdf(x))</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>ax_histy.plot(marginal_y.pdf(y), y)</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a><span class="co"># Set limits and labels</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>ax_scatter.set_xlim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>ax_scatter.set_ylim(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>ax_histx.set_xlim(ax_scatter.get_xlim())</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>ax_histy.set_ylim(ax_scatter.get_ylim())</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>ax_scatter.set_xlabel(<span class="st">"X"</span>)</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>ax_scatter.set_ylabel(<span class="st">"Y"</span>)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>ax_histx.set_ylabel(<span class="st">'Density'</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>ax_histy.set_xlabel(<span class="st">'Density'</span>)</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="6">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-7-output-1.png" class="img-fluid figure-img" width="729"></p>
<figcaption class="figure-caption">The plot shows a joint distribution of two continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as a contour plot in the center and the marginal distributions along the X and Y axes. The use of additional axes for the marginal distributions alongside the contour plot offers a comprehensive view of how the marginals relate to the joint distribution.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="conditional-distribution" class="level3" data-number="4.11.3">
<h3 data-number="4.11.3" class="anchored" data-anchor-id="conditional-distribution"><span class="header-section-number">4.11.3</span> Conditional Distribution</h3>
<p>The conditional distribution describes the probability of one random variable given the occurrence of another random variable. It essentially gives the distribution of a variable contingent on the value of another.</p>
<p>For discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional probability mass function (PMF) of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is defined as: <span class="math display">\[ P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)}.\]</span> provided <span class="math inline">\(P(Y = y) &gt; 0\)</span>.</p>
<p>For continuous random variables, the conditional probability density function (PDF) is defined similarly. If <span class="math inline">\(f_{X,Y}(x, y)\)</span> is the joint PDF of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(f_Y(y)\)</span> is the marginal PDF of <span class="math inline">\(Y\)</span>, then the conditional PDF of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is: <span class="math display">\[ f_{X|Y}(x | y) = \frac{f_{X,Y}(x, y)}{f_Y(y)},\]</span> again, provided <span class="math inline">\(f_Y(y) &gt; 0\)</span>.</p>
<p>We will create a bivariate distribution using sampling from a Beta distribution and a Gaussian distribution, and then compute the conditional distribution.</p>
<div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> beta, norm</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generating a bivariate distribution: Beta and Gaussian</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> beta.rvs(<span class="dv">2</span>, <span class="dv">5</span>, size<span class="op">=</span><span class="dv">10000</span>)  <span class="co"># Beta distribution</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> x <span class="op">+</span> norm.rvs(scale<span class="op">=</span><span class="fl">0.1</span>, size<span class="op">=</span><span class="dv">10000</span>)  <span class="co"># Gaussian distribution, dependent on x</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute conditional distribution: P(Y|X=x0)</span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="fl">0.5</span>  <span class="co"># Condition on this value of X</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> (x <span class="op">&gt;</span> x0 <span class="op">-</span> <span class="fl">0.05</span>) <span class="op">&amp;</span> (x <span class="op">&lt;</span> x0 <span class="op">+</span> <span class="fl">0.05</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>conditional_samples <span class="op">=</span> y[indices]</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">6</span>))</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>plt.scatter(x, y, <span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>plt.axvline(x<span class="op">=</span>x0, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X (Beta distributed)'</span>)</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Y (Gaussian distributed)'</span>)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bivariate Distribution (Beta and Gaussian)'</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>plt.hist(conditional_samples, bins<span class="op">=</span><span class="dv">20</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, color<span class="op">=</span><span class="st">'g'</span>)</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Y values'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'Conditional Distribution P(Y|X=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">)'</span>)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="7">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-8-output-1.png" class="img-fluid figure-img" width="1141"></p>
<figcaption class="figure-caption">Figure shows a joint Beta-Gaussian for (x,y). It also shows the conditional distribution of y for x = 0.5. This conditional distribution is visualized in the form of a histogram. The scatter plot of the bivariate distribution helps visualize the dependency between x and y.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="covariance" class="level3" data-number="4.11.4">
<h3 data-number="4.11.4" class="anchored" data-anchor-id="covariance"><span class="header-section-number">4.11.4</span> Covariance</h3>
<p>Covariance measures the joint variability of two random variables. It indicates the direction of the linear relationship between variables.</p>
<p>For discrete variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with mean values <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span>, the covariance is: <span class="math display">\[\text{Cov}(X, Y) = \sum (x_i - \mu_X)(y_i - \mu_Y)P(x_i, y_i).\]</span></p>
<p>For continuous variables, it is: <span class="math display">\[\text{Cov}(X, Y) = \int \int (x - \mu_X)(y - \mu_Y)f_{X,Y}(x, y) dx dy.\]</span></p>
<p>Positive covariance indicates that higher values of one variable are associated with higher values of the other, and vice versa. Negative covariance indicates the opposite.</p>
</section>
<section id="correlation" class="level3" data-number="4.11.5">
<h3 data-number="4.11.5" class="anchored" data-anchor-id="correlation"><span class="header-section-number">4.11.5</span> Correlation</h3>
<p>Correlation, specifically the Pearson correlation coefficient, measures the strength and direction of a linear relationship between two variables. Unlike covariance, it is dimensionless and normalized. It is defined as <span class="math display">\[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y},\]</span> where <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.The correlation coefficient ranges from -1 to 1. A value of 1 implies a perfect positive linear relationship, -1 implies a perfect negative linear relationship, and 0 implies no linear relationship.</p>
<p>We’ll use a simple dataset to compute and visualize the covariance and correlation between two variables:</p>
<div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a simple dataset</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.rand(<span class="dv">1000</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> X <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">1000</span>)  <span class="co"># Y is linearly related to X</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Covariance</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>covariance <span class="op">=</span> np.cov(X, Y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Correlation</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>correlation <span class="op">=</span> np.corrcoef(X, Y)[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the computed values</span></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Covariance between X and Y:"</span>, covariance)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Correlation between X and Y:"</span>, correlation)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the variables</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, Y, <span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Scatter Plot of X vs Y"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"X"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Y"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="8">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-stdout quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<pre><code>Covariance between X and Y: 0.1682441569776687
Correlation between X and Y: 0.9863442594104381</code></pre>
</div>
</div>
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img" width="589"></p>
<figcaption class="figure-caption">The script generates two variables, X and Y, where Y is linearly dependent on X with some added noise. It calculates the covariance and correlation between these variables and then plots them to show their relationship visually. The scatter plot provides a clear picture of how Y changes with X, and the covariance and correlation values quantify this relationship.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="various-sampling-techniques-in-statistics-and-machine-learning" class="level2" data-number="4.12">
<h2 data-number="4.12" class="anchored" data-anchor-id="various-sampling-techniques-in-statistics-and-machine-learning"><span class="header-section-number">4.12</span> Various Sampling Techniques in Statistics and Machine Learning</h2>
<p>Sampling techniques are methods used to select a subset of data from a larger dataset. In statistics and machine learning, different sampling methods are employed based on the nature of the data and the specific goals of the analysis. Here are some common sampling techniques:</p>
<section id="simple-random-sampling" class="level3" data-number="4.12.1">
<h3 data-number="4.12.1" class="anchored" data-anchor-id="simple-random-sampling"><span class="header-section-number">4.12.1</span> Simple Random Sampling</h3>
<p>Simple Random Sampling (SRS) is a fundamental statistical method where each member of a population has an equal chance of being selected for a sample. This method is highly valued for its ability to provide unbiased representations of a larger group. In SRS, the selection of one individual is completely independent of the selection of any other, ensuring that every subset of the population has an equal probability of being chosen. This randomness can be achieved through various means, such as using lottery methods or random number generators.</p>
<p>One of the primary advantages of SRS is its simplicity and the reduction of selection bias, making it an effective tool for obtaining representative samples. However, its practicality can be challenged in large populations due to logistical constraints, and its reliance on randomness does not always guarantee a perfectly representative sample, especially with smaller sample sizes. In the realm of machine learning and data analysis, SRS is often employed for tasks like creating balanced training and test datasets or for statistical estimations of population parameters. For example, in Python, the <code>numpy.random.choice</code> function can be utilized to perform SRS, allowing for the efficient selection of a random subset from a larger dataset.</p>
</section>
<section id="stratified-sampling" class="level3" data-number="4.12.2">
<h3 data-number="4.12.2" class="anchored" data-anchor-id="stratified-sampling"><span class="header-section-number">4.12.2</span> Stratified Sampling</h3>
<p>Stratified Sampling is a statistical technique designed to improve the representativeness and efficiency of a sample by dividing the population into distinct subgroups, or strata, based on shared characteristics before sampling. This method ensures that each subgroup is adequately represented in the final sample, addressing the potential shortcomings of simple random sampling, especially in diverse populations. The process involves first identifying relevant strata within the population – these could be based on factors like age, income, education level, or any other relevant criteria. Once the strata are established, samples are drawn independently from each stratum, typically through simple random sampling or systematic sampling methods. The size of the sample from each stratum can be proportional to the stratum’s size in the population or can be equal-sized to give equal representation to each stratum regardless of its size in the population.</p>
<p>Stratified sampling is particularly useful in surveys and research studies where certain subgroups within a population may be underrepresented or have significant variability. By ensuring that these subgroups are adequately represented, stratified sampling enhances the accuracy and reliability of results, making it a powerful tool for obtaining a comprehensive understanding of the entire population.</p>
</section>
<section id="cluster-sampling" class="level3" data-number="4.12.3">
<h3 data-number="4.12.3" class="anchored" data-anchor-id="cluster-sampling"><span class="header-section-number">4.12.3</span> Cluster Sampling</h3>
<p>Cluster sampling is a practical and efficient sampling method, especially useful in situations where the population is large and geographically dispersed. This technique involves dividing the entire population into groups or clusters, often based on geographical regions or other natural groupings. These clusters should ideally represent small-scale versions of the population. In cluster sampling, instead of selecting individual members from the entire population, a random sample of these clusters is chosen for the study, and all individuals within these selected clusters are included in the sample.</p>
<p>This approach can be particularly advantageous in large-scale surveys or field studies where reaching every individual is logistically challenging and cost-prohibitive. In Python, cluster sampling can be simulated by dividing a population dataset into clusters and then randomly selecting a subset of these clusters. The data from the chosen clusters are then combined to form the sample. This method reduces the cost and time of data collection significantly, making it a preferred choice for many large-scale research projects and surveys.</p>
</section>
<section id="systematic-sampling" class="level3" data-number="4.12.4">
<h3 data-number="4.12.4" class="anchored" data-anchor-id="systematic-sampling"><span class="header-section-number">4.12.4</span> Systematic Sampling</h3>
<p>Systematic sampling is a streamlined and efficient approach to sampling, particularly effective when dealing with large, ordered populations. This method starts by arranging the population in a sequence, after which a fixed interval or step size, known as the sampling interval, is determined. This interval is typically calculated by dividing the total population size by the desired sample size. The key to systematic sampling is the selection of a random starting point within the first interval, which ensures an element of randomness in the process. From this point, every <span class="math inline">\(k^\text{th}\)</span> member of the population is selected to be part of the sample, where <span class="math inline">\(k\)</span> is the sampling interval.</p>
<p>One of the significant advantages of systematic sampling is its simplicity and the uniform coverage it provides across the population. It’s particularly useful when a comprehensive list of the population is available, and the sampling process needs to be quick and straightforward. However, it’s important to be aware of the potential for bias, especially if the population’s ordering has a hidden periodicity that aligns with the sampling interval. Despite this, systematic sampling is a popular choice in various fields due to its ease of implementation, which can be efficiently executed in Python and other programming environments. This sampling method strikes a balance between the randomness of simple random sampling and the convenience of having a structured approach to selecting a sample.</p>
<p>Here’s a Python script illustrating these sampling techniques:</p>
<div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Population Data</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>population <span class="op">=</span> np.random.randn(<span class="dv">1000</span>)  <span class="co"># Normally distributed data</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Simple Random Sampling</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>simple_random_sample <span class="op">=</span> np.random.choice(population, <span class="dv">100</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Stratified Sampling</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming a binary characteristic for simplicity</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>strata_1 <span class="op">=</span> population[population <span class="op">&lt;</span> <span class="dv">0</span>]  <span class="co"># Negative values</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>strata_2 <span class="op">=</span> population[population <span class="op">&gt;=</span> <span class="dv">0</span>] <span class="co"># Non-negative values</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>stratified_sample <span class="op">=</span> np.concatenate([np.random.choice(strata_1, <span class="dv">50</span>), np.random.choice(strata_2, <span class="dv">50</span>)])</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Cluster Sampling</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Dividing into 10 clusters</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> np.array_split(population, <span class="dv">10</span>)</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>selected_clusters <span class="op">=</span> np.random.choice(np.arange(<span class="dv">10</span>), <span class="dv">3</span>, replace<span class="op">=</span><span class="va">False</span>)  <span class="co"># Select 3 clusters</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>cluster_sample <span class="op">=</span> np.concatenate([clusters[i] <span class="cf">for</span> i <span class="kw">in</span> selected_clusters])</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Systematic Sampling</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> np.random.randint(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>systematic_sample <span class="op">=</span> population[start::<span class="dv">10</span>]</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">8</span>))</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].hist(simple_random_sample, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'skyblue'</span>)</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">0</span>].set_title(<span class="st">'Simple Random Sampling'</span>)</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].hist(stratified_sample, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'lightgreen'</span>)</span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>, <span class="dv">1</span>].set_title(<span class="st">'Stratified Sampling'</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].hist(cluster_sample, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'salmon'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">0</span>].set_title(<span class="st">'Cluster Sampling'</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].hist(systematic_sample, bins<span class="op">=</span><span class="dv">20</span>, color<span class="op">=</span><span class="st">'gold'</span>)</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>, <span class="dv">1</span>].set_title(<span class="st">'Systematic Sampling'</span>)</span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="9">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img" width="950"></p>
<figcaption class="figure-caption">Histograms visualizing various distributions of the samples obtained through each technique described above.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="sampling-from-distributions" class="level2" data-number="4.13">
<h2 data-number="4.13" class="anchored" data-anchor-id="sampling-from-distributions"><span class="header-section-number">4.13</span> Sampling from Distributions</h2>
<p>Sampling from distributions is a crucial technique in statistics and machine learning, particularly in simulations, probabilistic modeling, and Bayesian inference. Various techniques can be used to sample from different kinds of distributions, each with its own advantages and applicable scenarios.</p>
<section id="inverse-transform-sampling" class="level3" data-number="4.13.1">
<h3 data-number="4.13.1" class="anchored" data-anchor-id="inverse-transform-sampling"><span class="header-section-number">4.13.1</span> Inverse Transform Sampling</h3>
<p>Inverse Transform Sampling, also known as the Inverse Probability Integral Transform, is a method used to generate random samples from any probability distribution, given its cumulative distribution function (CDF). This technique is particularly useful when direct sampling from the distribution is challenging.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Cumulative Distribution Function (CDF)</strong>: Start with the CDF of the desired probability distribution. The CDF, <span class="math inline">\(F(x)\)</span>, of a random variable <span class="math inline">\(X\)</span> is defined as the probability that <span class="math inline">\(X\)</span> will take a value less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Uniform Random Variable</strong>: Generate a random sample <span class="math inline">\(U\)</span> from a standard uniform distribution, i.e., <span class="math inline">\(U \sim \text{Uniform}(0,1)\)</span>.</p></li>
<li><p><strong>Inverse of CDF</strong>: Use the inverse of the CDF, <span class="math inline">\(F^{-1}(u)\)</span>, to transform the uniformly distributed sample <span class="math inline">\(U\)</span> into a sample that follows the desired distribution. The inverse CDF method hinges on the principle that if <span class="math inline">\(U\)</span> is a uniform random variable on the interval <span class="math inline">\([0,1]\)</span>, then the variable <span class="math inline">\(X = F^{-1}(U)\)</span> has the desired distribution.</p></li>
</ol>
<p>This method effectively transforms uniformly distributed data into data that follows any given distribution, using the inverse of the CDF of that distribution. It can be applied to any distribution, provided its CDF is known and is invertible. The method is computationally efficient and widely used, especially when other sampling methods are not feasible. Inverse transform sampling is extensively used in simulations, Monte Carlo methods, and various areas where generating random samples from specific distributions is required.</p>
<p>Suppose we want to generate random samples from an exponential distribution using inverse transform sampling. Here’s how we might implement it in Python:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of samples to generate</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate uniform samples</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>uniform_samples <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">1</span>, n_samples)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Inverse CDF (percent point function) of the exponential distribution</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>exponential_samples <span class="op">=</span> expon.ppf(uniform_samples)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the generated samples</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.hist(exponential_samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Sampled Data'</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.plot(x, expon.pdf(x), label<span class="op">=</span><span class="st">'Target Distribution'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Random Samples from an Exponential Distribution'</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-11-output-1.png" width="589" height="449" class="figure-img"></p>
<figcaption class="figure-caption">Samples from exponential distribution using inverse CDF technique.</figcaption>
</figure>
</div>
</div>
</div>
<p>This example demonstrates the simplicity and power of inverse transform sampling for generating random samples from a specified distribution.</p>
</section>
<section id="rejection-sampling" class="level3" data-number="4.13.2">
<h3 data-number="4.13.2" class="anchored" data-anchor-id="rejection-sampling"><span class="header-section-number">4.13.2</span> Rejection Sampling</h3>
<p>Rejection sampling, also known as the acceptance-rejection method, is a technique used to generate samples from a target probability distribution <span class="math inline">\(f(x)\)</span> when direct sampling is difficult. The method involves using a simpler proposal distribution <span class="math inline">\(g(x)\)</span> from which we can easily sample.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Choose a Proposal Distribution <span class="math inline">\(g(x)\)</span></strong>: Select a distribution <span class="math inline">\(g(x)\)</span> that is easy to sample from and for which there exists a constant <span class="math inline">\(M\)</span> such that <span class="math inline">\(M \cdot g(x) \geq f(x)\)</span> for all <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Generate Samples</strong>: Sample a point <span class="math inline">\(x\)</span> from <span class="math inline">\(g(x)\)</span> and a uniform random number <span class="math inline">\(u\)</span> from the interval <span class="math inline">\([0, M \cdot g(x)]\)</span>.</p></li>
<li><p><strong>Accept or Reject</strong>: Accept the sample <span class="math inline">\(x\)</span> if <span class="math inline">\(u \leq f(x)\)</span>; otherwise, reject it and repeat the process.</p></li>
<li><p><strong>Repeat</strong>: Repeat steps 2 and 3 until the desired number of samples is obtained.</p></li>
</ol>
<p>The efficiency of rejection sampling depends on how closely <span class="math inline">\(g(x)\)</span> approximates <span class="math inline">\(f(x)\)</span> and on the value of <span class="math inline">\(M\)</span>. If <span class="math inline">\(M\)</span> is too large or if <span class="math inline">\(g(x)\)</span> is a poor approximation of <span class="math inline">\(f(x)\)</span>, the rejection rate will be high, making the method inefficient.</p>
<p>Let’s demonstrate rejection sampling in Python, where we sample from an exponential distribution using a uniform proposal distribution.</p>
<div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Target distribution: Exponential distribution</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_distribution(x):</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> expon.pdf(x)</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Proposal distribution: Uniform distribution</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> proposal_distribution(x):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.ones_like(x)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Rejection Sampling</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rejection_sampling(target_dist, proposal_dist, M, size<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> []</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(samples) <span class="op">&lt;</span> size:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        x_proposal <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, <span class="dv">4</span>, size<span class="op">=</span>size)</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>        u <span class="op">=</span> np.random.uniform(<span class="dv">0</span>, M <span class="op">*</span> proposal_dist(x_proposal), size<span class="op">=</span>size)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        accepted <span class="op">=</span> x_proposal[u <span class="op">&lt;=</span> target_dist(x_proposal)]</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>        samples.extend(accepted.tolist())</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(samples[:size])</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Constants</span></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="fl">1.5</span>  <span class="co"># Choose M such that M * g(x) &gt;= f(x)</span></span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate samples</span></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> rejection_sampling(target_distribution, proposal_distribution, M, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the sampled data</span></span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Sampled Data'</span>)</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">4</span>, <span class="dv">100</span>)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>plt.plot(x, expon.pdf(x), label<span class="op">=</span><span class="st">'Target Distribution'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Rejection Sampling from an Exponential Distribution'</span>)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="11">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img" width="571"></p>
<figcaption class="figure-caption">Samples from the exponential distribution using rejection sampling and uniform proposal distribution.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>Rejection sampling is particularly useful in scenarios where direct sampling from the target distribution is not feasible or too complex. The key is to choose an appropriate proposal distribution and constant <span class="math inline">\(M\)</span> to ensure a reasonable acceptance rate.</p>
</section>
<section id="importance-sampling" class="level3" data-number="4.13.3">
<h3 data-number="4.13.3" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">4.13.3</span> Importance Sampling</h3>
<p>Importance Sampling is a statistical technique used in probability and statistics, particularly in the fields of Monte Carlo simulation and data science. It is a method for estimating properties of a particular distribution, while only having samples from a different distribution. This technique is especially useful in scenarios where direct sampling from the desired distribution is challenging or inefficient.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Select a Proposal Distribution</strong>: Choose a distribution <span class="math inline">\(g(x)\)</span> from which it is easier to sample (known as the proposal distribution). This distribution should ideally be similar to the target distribution <span class="math inline">\(f(x)\)</span> but also ensure that where <span class="math inline">\(f(x)\)</span> is significantly non-zero, <span class="math inline">\(g(x)\)</span> is also non-zero.</p></li>
<li><p><strong>Generate Samples</strong>: Draw samples from the proposal distribution <span class="math inline">\(g(x)\)</span>.</p></li>
<li><p><strong>Weighting Samples</strong>: Calculate weights for each sampled point. The weight for a sample <span class="math inline">\(x\)</span> is given by the ratio of the target probability density to the proposal probability density at <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(w(x) = \frac{f(x)}{g(x)}\)</span>.</p></li>
<li><p><strong>Estimate Target Expectation</strong>: The expected value of a function under the target distribution is estimated by the weighted average of that function over the samples drawn from the proposal distribution.</p></li>
</ol>
<p>Importance Sampling can be more efficient than simple random sampling, especially when the area of interest under the target distribution occupies a small part of the space. It allows for the estimation of properties of one distribution using samples from another, providing flexibility in situations where sampling from the target distribution is hard.</p>
<p>However, it has some disadvantages. The efficiency of importance sampling heavily depends on the choice of the proposal distribution. A poor choice can lead to high variance in estimates and inefficient sampling. In some cases, a few samples may end up with very high weights, dominating the estimate and leading to high variance.</p>
<p>The following Python code demonstrates a simple example of importance sampling where we estimate the mean of a target exponential distribution using samples from a uniform distribution:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> expon, uniform</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Target distribution: Exponential</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>target_dist <span class="op">=</span> expon()</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Proposal distribution: Uniform</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>proposal_dist <span class="op">=</span> uniform()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of samples</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Draw samples from the proposal distribution</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> proposal_dist.rvs(size<span class="op">=</span>n_samples)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute weights</span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> target_dist.pdf(samples) <span class="op">/</span> proposal_dist.pdf(samples)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Estimate the mean of the target distribution</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>estimated_mean <span class="op">=</span> np.<span class="bu">sum</span>(weights <span class="op">*</span> samples) <span class="op">/</span> np.<span class="bu">sum</span>(weights)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Estimated Mean of the Target Distribution:"</span>, estimated_mean)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated Mean of the Target Distribution: 0.41768407894841997</code></pre>
</div>
</div>
<p>In this code, we use importance sampling to estimate the mean of an exponential distribution, using samples drawn from a uniform distribution. The weights are calculated based on the ratio of the probability densities of the target and proposal distributions at each sampled point. This method provides an estimation of the desired expectation under the target distribution.</p>
</section>
<section id="markov-chain-monte-carlo-mcmc" class="level3" data-number="4.13.4">
<h3 data-number="4.13.4" class="anchored" data-anchor-id="markov-chain-monte-carlo-mcmc"><span class="header-section-number">4.13.4</span> Markov Chain Monte Carlo (MCMC)</h3>
<p>Markov Chain Monte Carlo (MCMC) is a set of algorithms used for sampling from probability distributions where direct sampling is difficult or impossible. MCMC enables the estimation of the distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution.</p>
<p>The Key concepts in MCMC are:</p>
<ol type="1">
<li><p><strong>Markov Chain</strong>: A Markov chain is a stochastic model describing a sequence of possible events, where the probability of each event depends only on the state attained in the previous event. MCMC utilizes this property to generate samples.</p></li>
<li><p><strong>Monte Carlo Integration</strong>: MCMC methods use Monte Carlo integration, where random samples are generated and used to compute estimates of desired quantities, such as means, variances, or probabilities.</p></li>
<li><p><strong>Convergence to Target Distribution</strong>: The Markov chain is constructed so that it converges to the target distribution as its stationary distribution. After a ‘burn-in’ period, samples drawn from the Markov chain are used as samples from the target distribution.</p></li>
</ol>
<p>The following are two popular MCMC algorithms:</p>
<ul>
<li><p><strong>Metropolis-Hastings Algorithm</strong>: This algorithm generates a Markov chain using a proposal distribution and an acceptance criterion based on the ratio of the target densities.</p></li>
<li><p><strong>Gibbs Sampling</strong>: A special case of the Metropolis-Hastings algorithm that is particularly useful when sampling from high-dimensional distributions. It samples successively from the conditional distribution of each variable.</p></li>
</ul>
<p>MCMC methods are widely used in various fields for estimating complex probability distributions, especially in Bayesian statistics for computing posterior distributions, in statistical physics, and in financial modeling.</p>
<p>Their strentgth is in versatility and ability to handle high dimensional spaces. MCMC methods can sample from virtually any probability distribution. They are particularly powerful in high-dimensional spaces where other sampling methods fail.</p>
<p>However, determining whether the Markov chain has converged to the target distribution can be challenging. Also, these methods can be slow, especially for complex or high-dimensional distributions.</p>
<p>In Bayesian statistics, MCMC is used to estimate the posterior distribution of parameters. For instance, if the likelihood function is complex or the prior distribution is not conjugate to the likelihood, traditional analytical approaches may not work, and MCMC methods like Metropolis-Hastings or Gibbs Sampling can be employed to approximate the posterior distribution. These samples then allow for statistical inference about the parameters, such as estimating means, variances, or constructing credible intervals.</p>
<p>MCMC methods, due to their flexibility and power, have become a cornerstone technique in modern statistical inference, particularly in scenarios where other methods are impractical or infeasible.</p>
<p>Implementing a simple Markov Chain Monte Carlo (MCMC) algorithm in Python can be educational for understanding how MCMC works, especially in Bayesian inference contexts. We’ll demonstrate the Metropolis-Hastings algorithm, a widely used MCMC method, to sample from a probability distribution.</p>
<p>For simplicity, let’s assume we want to sample from a unimodal Gaussian distribution, but the same approach can be extended to more complex distributions.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Target distribution (Gaussian in this example)</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> target_distribution(x):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> norm.pdf(x, loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Metropolis-Hastings Algorithm</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> metropolis_hastings(target_pdf, initial_value, iterations, proposal_width):</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> initial_value</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> [x]</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iterations):</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Proposal distribution: Gaussian centered at the current sample</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        x_proposal <span class="op">=</span> np.random.normal(x, proposal_width)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calculate acceptance probability</span></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        acceptance_probability <span class="op">=</span> <span class="bu">min</span>(target_pdf(x_proposal) <span class="op">/</span> target_pdf(x), <span class="dv">1</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Accept or reject the proposal</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.random.random() <span class="op">&lt;</span> acceptance_probability:</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_proposal</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>        samples.append(x)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array(samples)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the algorithm</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>initial_value <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>iterations <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>proposal_width <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> metropolis_hastings(target_distribution, initial_value, iterations, proposal_width)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the results</span></span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>plt.hist(samples, bins<span class="op">=</span><span class="dv">30</span>, density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Sampled Distribution'</span>)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">1000</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>plt.plot(x, norm.pdf(x), label<span class="op">=</span><span class="st">'Target Gaussian Distribution'</span>, color<span class="op">=</span><span class="st">'red'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Sampling from a Gaussian Distribution using MCMC'</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Value'</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-14-output-1.png" width="597" height="449" class="figure-img"></p>
<figcaption class="figure-caption">Samples from a unimodal Gaussian using the Metropolis-Hastings algorithm.</figcaption>
</figure>
</div>
</div>
</div>
<p>In this script:</p>
<ul>
<li>We define a target distribution as a standard Gaussian.</li>
<li>The <code>metropolis_hastings</code> function implements the MCMC algorithm. It generates a proposal for the next sample based on the current sample (Gaussian proposal distribution) and then decides whether to accept or reject this proposal based on the acceptance probability.</li>
<li>We run the algorithm for a number of iterations and collect the samples.</li>
<li>The resulting samples are plotted against the true Gaussian distribution for comparison.</li>
</ul>
<p>This example demonstrates how MCMC can be used to sample from a given distribution. The Metropolis-Hastings algorithm, in particular, is effective for complex distributions where direct sampling is challenging.</p>
</section>
</section>
<section id="distance-between-distributions." class="level2" data-number="4.14">
<h2 data-number="4.14" class="anchored" data-anchor-id="distance-between-distributions."><span class="header-section-number">4.14</span> Distance between Distributions.</h2>
<p>In machine learning, measuring the distance between probability distributions is crucial for various tasks, including statistical inference, clustering, and classification. These distances provide a way to quantify how similar or different two distributions are. Here are some commonly used distance metrics:</p>
<section id="kullback-leibler-divergence-kl-divergence" class="level3" data-number="4.14.1">
<h3 data-number="4.14.1" class="anchored" data-anchor-id="kullback-leibler-divergence-kl-divergence"><span class="header-section-number">4.14.1</span> Kullback-Leibler Divergence (KL Divergence)</h3>
<p>KL Divergence, named after Solomon Kullback and Richard Leibler, is a measure of how one probability distribution diverges from a second, reference probability distribution. It’s a concept from information theory, often used in statistics and machine learning.</p>
<p>For discrete distributions, the KL divergence of a distribution <span class="math inline">\(P\)</span> from a distribution <span class="math inline">\(Q\)</span> over the same probability space is defined as: <span class="math display">\[ D_{KL}(P \parallel Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right).\]</span></p>
<p>For continuous distributions, it is given by: <span class="math display">\[ D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log\left(\frac{p(x)}{q(x)}\right) dx.\]</span></p>
<p>It quantifies the amount of information lost when <span class="math inline">\(Q\)</span> is used to approximate <span class="math inline">\(P\)</span>. A KL divergence of 0 indicates that the two distributions are identical (in the continuous case, almost everywhere).</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Non-Negativity</strong>: <span class="math inline">\(D_{KL}(P \parallel Q) \geq 0\)</span>, with equality if and only if <span class="math inline">\(P = Q\)</span> (almost everywhere).</li>
<li><strong>Not Symmetric</strong>: <span class="math inline">\(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\)</span>, meaning it’s not a distance in the traditional sense.</li>
<li><strong>Not a Distance Metric</strong>: Since it’s not symmetric and doesn’t satisfy the triangle inequality, it’s not a true metric.</li>
</ul>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Model Evaluation and Selection</strong>: In Bayesian statistics, KL divergence measures how much a model’s predicted distribution deviates from the true distribution, helping in model comparison and selection.</p></li>
<li><p><strong>Variational Inference</strong>: It’s used in variational inference as a part of the Evidence Lower Bound (ELBO), to approximate complex posterior distributions in Bayesian models.</p></li>
<li><p><strong>Feature Selection and Dimensionality Reduction</strong>: Methods like t-Distributed Stochastic Neighbor Embedding (t-SNE) use KL divergence to preserve small pairwise distances or similarities between high-dimensional data points when mapping them to a lower-dimensional space.</p></li>
<li><p><strong>Training Generative Models</strong>: In generative models, like Variational Autoencoders (VAEs), KL divergence is used to regularize the encoder by penalizing deviations of its output distribution from a prior distribution, typically a Gaussian.</p></li>
<li><p><strong>Information Theory</strong>: In information retrieval and natural language processing, it helps in quantifying the information gain between different stages of the model or different models.</p></li>
</ol>
<p>KL Divergence is particularly useful in scenarios where understanding the difference or the information gain between distributions is crucial. Its ability to quantify the ‘distance’ between probability distributions makes it a valuable tool for probabilistic modeling, particularly in Bayesian frameworks and in scenarios where approximation of complex distributions is necessary.</p>
</section>
<section id="jensen-shannon-divergence-jsd" class="level3" data-number="4.14.2">
<h3 data-number="4.14.2" class="anchored" data-anchor-id="jensen-shannon-divergence-jsd"><span class="header-section-number">4.14.2</span> Jensen-Shannon Divergence (JSD)</h3>
<p>Jensen-Shannon Divergence is a method to measure the similarity between two probability distributions. It is a symmetrized and smoothed version of the Kullback-Leibler Divergence (KL Divergence). JSD overcomes some of the limitations of KL Divergence, particularly its asymmetry and the fact that it can be infinite if the two distributions being compared do not overlap.</p>
<p>The Jensen-Shannon Divergence between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as the average of the KL Divergences of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> from the mean distribution <span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span>: <span class="math display">\[ \text{JSD}(P \parallel Q) = \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M).\]</span></p>
<p>JSD quantifies the similarity between two probability distributions, with a value of 0 indicating identical distributions. It is always a finite value, bounded between 0 and 1, making it a more stable measure compared to KL Divergence.</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Symmetric</strong>: Unlike KL Divergence, JSD is symmetric, i.e., <span class="math display">\[\text{JSD}(P \parallel Q) = \text{JSD}(Q \parallel P).\]</span></li>
<li><strong>Bounded</strong>: The values of JSD range from 0 to 1, where 0 indicates identical distributions and 1 indicates maximal divergence.</li>
</ul>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Model Evaluation</strong>: In machine learning models, especially in natural language processing and information retrieval, JSD can be used to compare word distribution, topic distribution, or any other probability distributions that arise.</p></li>
<li><p><strong>Generative Models</strong>: In training generative models, such as Generative Adversarial Networks (GANs), JSD can be used as an objective function to measure the difference between the generated data distribution and the real data distribution.</p></li>
<li><p><strong>Clustering and Similarity Measurement</strong>: JSD is used in clustering algorithms to measure the similarity between different data points or clusters when the data is represented as probability distributions.</p></li>
<li><p><strong>Feature Selection</strong>: It can be applied in feature selection to measure the amount of information gained by including a particular feature, especially when features can be represented probabilistically.</p></li>
</ol>
<p>JSD’s symmetric and bounded nature makes it a versatile tool for comparing probability distributions in various machine learning tasks. Its ability to provide a smooth and finite measure of divergence is particularly useful in scenarios where KL Divergence might be too sensitive or undefined.</p>
</section>
<section id="earth-movers-distance-emd-or-wasserstein-distance" class="level3" data-number="4.14.3">
<h3 data-number="4.14.3" class="anchored" data-anchor-id="earth-movers-distance-emd-or-wasserstein-distance"><span class="header-section-number">4.14.3</span> Earth Mover’s Distance (EMD) or Wasserstein Distance</h3>
<p>Earth Mover’s Distance (EMD), also known as the Wasserstein Distance, is a measure of the distance between two probability distributions over a given space. The name “Earth Mover’s” stems from a practical analogy: it represents the minimum amount of “work” required to transform one distribution into the other, where “work” is quantified as the product of the amount of “mass” moved and the distance it’s moved.</p>
<p>In a discrete setting, if we have two distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> with the same total mass, EMD is the minimum cost of turning one distribution into the other, given a “ground distance” between individual points.</p>
<p>In continuous spaces, EMD is defined via the solution to the transportation problem from the field of optimization, where the goal is to find the most efficient way to move a distribution of mass to match another distribution.</p>
<p>The Wasserstein distance of order 1 (often used in practice) between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as: <span class="math display">\[ W(P, Q) = \inf_{\gamma \in \Pi(P, Q)} \int_{X \times Y} d(x, y) \, d\gamma(x, y),\]</span> where <span class="math inline">\(\Pi(P, Q)\)</span> is the set of all joint distributions <span class="math inline">\(\gamma(x, y)\)</span> whose marginals are <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, and <span class="math inline">\(d(x, y)\)</span> is a ground distance between points <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>Unlike other distances like KL divergence, EMD provides a more intuitive geometric interpretation. EMD is effective even when the compared distributions do not overlap, a scenario where other distances can fail or give misleading results.</p>
<p>EMD’s ability to provide a meaningful and geometrically interpretable measure of distance between distributions has made it a valuable tool in various machine learning applications, particularly in areas where understanding the “transport” of mass or information between distributions is important.</p>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Optimal Transport</strong>: EMD provides a natural way to compare distributions, making it useful in optimal transport problems, where the goal is to find the most efficient way to redistribute resources.</p></li>
<li><p><strong>Generative Models</strong>: In training Generative Adversarial Networks (GANs), EMD can be used as a loss function (Wasserstein loss) to measure the distance between the distribution of generated data and the distribution of real data. It has been shown to improve the stability and performance of GAN training.</p></li>
<li><p><strong>Domain Adaptation</strong>: EMD is used in domain adaptation to measure the discrepancy between source and target domains, guiding the learning process to minimize this discrepancy.</p></li>
<li><p><strong>Image Retrieval and Computer Vision</strong>: EMD is applied in image retrieval systems to compare images represented as distributions of features. It’s also used in other computer vision tasks for comparing histograms and texture matching.</p></li>
</ol>
</section>
<section id="hellinger-distance" class="level3" data-number="4.14.4">
<h3 data-number="4.14.4" class="anchored" data-anchor-id="hellinger-distance"><span class="header-section-number">4.14.4</span> Hellinger Distance</h3>
<p>The Hellinger Distance is a metric used to quantify the similarity between two probability distributions. It’s derived from the Bhattacharyya coefficient and is used in various applications in statistics and machine learning.</p>
<p>The Hellinger Distance between two discrete or continuous probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as follows:</p>
<ul>
<li><p><strong>For Discrete Distributions</strong>: If <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> are discrete distributions with the same support, <span class="math display">\[ H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i} (\sqrt{P(i)} - \sqrt{Q(i)})^2}.\]</span></p></li>
<li><p><strong>For Continuous Distributions</strong>: For continuous distributions with density functions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>, <span class="math display">\[ H(p, q) = \frac{1}{\sqrt{2}} \sqrt{\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx}.\]</span></p></li>
</ul>
<p>The Hellinger Distance is a measure of the “overlap” between two probability distributions. A value of 0 indicates identical distributions, while a value of 1 indicates no overlap. It is symmetric and bounded between 0 and 1, making it a true metric.</p>
<p>As a metric, it is non-negative, symmetric, and obeys the triangle inequality, making it a reliable measure for comparing distributions. The bounded range (from 0 to 1) offers an intuitive interpretation of results. Suitable for both discrete and continuous distributions and is particularly effective in scenarios where the distributions have non-overlapping support.</p>
<p>In summary, the Hellinger Distance provides a robust and interpretable way to measure the similarity or difference between two probability distributions, making it a valuable tool in various machine learning tasks, particularly those involving probabilistic modeling and analysis.</p>
<p>It has the following application in machine learning:</p>
<ol type="1">
<li><p><strong>Clustering and Classification</strong>: In clustering algorithms and classification models, the Hellinger Distance can be used as a similarity measure between distributions. This is particularly useful when dealing with probability histograms or distributions as features.</p></li>
<li><p><strong>Model Evaluation</strong>: In probabilistic models, it can be used to compare the estimated probability distribution with the true distribution, providing a measure of model performance.</p></li>
<li><p><strong>Feature Selection</strong>: In scenarios where features are represented as distributions (such as word distributions in text data), the Hellinger Distance can aid in assessing the importance of features.</p></li>
<li><p><strong>Kernel Methods</strong>: The Hellinger Distance can be used to construct kernels for SVMs and other kernel-based methods, especially in applications dealing with probability distributions.</p></li>
</ol>
</section>
<section id="total-variation-distance-tvd" class="level3" data-number="4.14.5">
<h3 data-number="4.14.5" class="anchored" data-anchor-id="total-variation-distance-tvd"><span class="header-section-number">4.14.5</span> Total Variation Distance (TVD)</h3>
<p>Total Variation Distance is a measure of the difference between two probability distributions. It is a metric that quantifies how much two distributions differ from each other.</p>
<p>For discrete distributions, the total variation distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> over a finite or countably infinite set is defined as: <span class="math display">\[ TVD(P, Q) = \frac{1}{2} \sum_{i} |P(i) - Q(i)|.\]</span></p>
<p>For continuous distributions with probability density functions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>, TVD is given by: <span class="math display">\[ TVD(p, q) = \frac{1}{2} \int |p(x) - q(x)| dx.\]</span></p>
<p>TVD ranges from 0 to 1. A value of 0 indicates that the two distributions are identical, while a value of 1 indicates that the distributions are completely disjoint.</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Symmetric and Bounded</strong>: TVD is symmetric (i.e., <span class="math inline">\(TVD(P, Q) = TVD(Q, P)\)</span>) and bounded between 0 and 1.</li>
<li><strong>Interpretable</strong>: It provides an easily interpretable measure of the distance between distributions.</li>
</ul>
<p>TVD is a robust metric for comparing distributions, providing a clear and bounded measure of their difference. It can be applied to both discrete and continuous distributions, making it widely applicable in various machine learning contexts.</p>
<p>Total Variation Distance is particularly valuable in scenarios where a straightforward and robust measure of distributional difference is required. Its bounded and symmetric nature makes it an intuitive and reliable tool for comparing probability distributions in various machine learning applications.</p>
<p>It has the following applications in machine learing:</p>
<ol type="1">
<li><p><strong>Model Comparison and Selection</strong>: In machine learning, especially in probabilistic modeling, TVD can be used to compare different models by measuring how close their output distributions are to the true distribution.</p></li>
<li><p><strong>Generative Models</strong>: In the training of generative models, such as Generative Adversarial Networks (GANs), TVD can be used to measure the difference between the distribution of generated data and the real data distribution.</p></li>
<li><p><strong>Reinforcement Learning</strong>: In reinforcement learning, TVD can help compare the policy distributions over different iterations, aiding in the analysis of policy convergence.</p></li>
<li><p><strong>Statistical Learning Theory</strong>: TVD plays a role in theoretical aspects of machine learning, such as understanding the behavior of learning algorithms and their convergence properties.</p></li>
<li><p><strong>Feature Engineering</strong>: In tasks involving feature engineering, TVD can be useful for comparing feature distributions across different classes or groups, helping to identify features that provide the most discriminative power.</p></li>
</ol>
</section>
</section>
<section id="functions-of-random-variables" class="level2" data-number="4.15">
<h2 data-number="4.15" class="anchored" data-anchor-id="functions-of-random-variables"><span class="header-section-number">4.15</span> Functions of Random Variables</h2>
<p>In probability and statistics, a function of a random variable is a new random variable formed by applying a function to an existing random variable. Functions of random variables are used extensively in statistics for hypothesis testing, in generating derived distributions, and in modeling relationships between variables. In machine learning, transformations of random variables are common in feature engineering and in the development of probabilistic models. Understanding functions of random variables allows one to comprehend how transformations affect the behavior and properties of the data, which is crucial in data analysis, inference, and predictive modeling.</p>
<p>Given a random variable <span class="math inline">\(X\)</span> and a real-valued function <span class="math inline">\(g\)</span>, a new random variable <span class="math inline">\(Y\)</span> can be defined as <span class="math inline">\(Y = g(X)\)</span>. The probability distribution of <span class="math inline">\(Y\)</span> is determined by the function <span class="math inline">\(g\)</span> and the distribution of <span class="math inline">\(X\)</span>. In general, we can have linear or nonlinear transformation of random variables,</p>
<ul>
<li><strong>Linear Transformations</strong>: For example, <span class="math inline">\(Y = aX + b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. Such transformations linearly scale and shift the distribution of <span class="math inline">\(X\)</span>.</li>
<li><strong>Non-linear Transformations</strong>: For example, <span class="math inline">\(Y = X^2\)</span> or <span class="math inline">\(Y = e^X\)</span>. These can significantly alter the shape of the original distribution.</li>
</ul>
<section id="calculating-the-distribution-of-y-gx" class="level3" data-number="4.15.1">
<h3 data-number="4.15.1" class="anchored" data-anchor-id="calculating-the-distribution-of-y-gx"><span class="header-section-number">4.15.1</span> Calculating the Distribution of <span class="math inline">\(Y = g(X)\)</span></h3>
<section id="functions-of-discrete-random-variables" class="level4" data-number="4.15.1.1">
<h4 data-number="4.15.1.1" class="anchored" data-anchor-id="functions-of-discrete-random-variables"><span class="header-section-number">4.15.1.1</span> Functions of Discrete Random Variables</h4>
<p>When dealing with discrete random variables, a function of a random variable results in another discrete random variable. The distribution of this transformed variable can be computed both analytically and numerically.</p>
<p><strong>Example: Analytical Approach</strong></p>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable and <span class="math inline">\(g\)</span> is a function, then <span class="math inline">\(Y = g(X)\)</span> is also a discrete random variable. The PMF of <span class="math inline">\(Y\)</span>, denoted as <span class="math inline">\(p_Y(y)\)</span>, is computed by summing the probabilities of all values <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> that map to the same value <span class="math inline">\(y\)</span> under the function <span class="math inline">\(g\)</span>. Mathematically, <span class="math display">\[ p_Y(y) = \sum_{x: g(x) = y} p_X(x), \]</span> where <span class="math inline">\(p_X(x)\)</span> is the PMF of <span class="math inline">\(X\)</span>.</p>
<p>Consider a fair six-sided die. Let <span class="math inline">\(X\)</span> be the outcome of a roll (1 to 6), and let <span class="math inline">\(Y = X^2\)</span>. We find the PMF of <span class="math inline">\(Y\)</span> as follows:</p>
<ul>
<li>The possible values of <span class="math inline">\(Y\)</span> are <span class="math inline">\(\{1, 4, 9, 16, 25, 36\}\)</span>.</li>
<li>For each <span class="math inline">\(y\)</span> in <span class="math inline">\(\{1, 4, 9, 16, 25, 36\}\)</span>, compute <span class="math inline">\(p_Y(y)\)</span> using the formula. For example, <span class="math inline">\(p_Y(1) = P(X=1) = 1/6\)</span>, <span class="math inline">\(p_Y(4) = P(X=2) = 1/6\)</span>, and so on.</li>
</ul>
<p><strong>Example: Numerical Approach</strong></p>
<p>We can also apply a numerical approach to compute the distribution of a transformed variable. We first generate a large number of values of <span class="math inline">\(X\)</span> and apply <span class="math inline">\(g\)</span> to each value to get corresponding values of <span class="math inline">\(Y\)</span>. We then <em>count</em> the frequency of each value of <span class="math inline">\(Y\)</span> and divide by the total number of simulations to approximate the PMF. For die roll problem, we first simulate rolling a die a large number of times, say 10,000 times. For each roll, we compute compute the square of the outcome. Then we count the frequency of each squared value and divide by 10,000 to estimate the PMF of <span class="math inline">\(Y\)</span>.</p>
<p>This is shown in the following Python code.</p>
<div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation of die roll</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)  <span class="co"># for reproducibility</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>rolls <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, <span class="dv">10000</span>)  <span class="co"># simulate 10,000 die rolls</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>squared_rolls <span class="op">=</span> rolls <span class="op">**</span> <span class="dv">2</span>  <span class="co"># square each roll</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute empirical PMF</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>values, counts <span class="op">=</span> np.unique(squared_rolls, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>pmf <span class="op">=</span> counts <span class="op">/</span> counts.<span class="bu">sum</span>()</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the PMF</span></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>plt.bar(values, pmf)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Y values'</span>)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Probability'</span>)</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Empirical PMF of Y = X^2'</span>)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>plt.xticks(values)</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="14">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-15-output-1.png" class="img-fluid figure-img" width="597"></p>
<figcaption class="figure-caption">Emprical determination of the distributions of a transformed variable.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>This code simulates the square of a die roll and estimates its PMF numerically. It also visualizes the PMF, offering a clear understanding of the distribution of <span class="math inline">\(Y = X^2\)</span> when <span class="math inline">\(X\)</span> is the outcome of a die roll. This approach is particularly useful when the analytical computation is complex or infeasible.</p>
</section>
<section id="functions-of-continuous-random-variables" class="level4" data-number="4.15.1.2">
<h4 data-number="4.15.1.2" class="anchored" data-anchor-id="functions-of-continuous-random-variables"><span class="header-section-number">4.15.1.2</span> Functions of Continuous Random Variables</h4>
<p>When a function is applied to a continuous random variable, it results in another continuous random variable whose distribution can be derived from the original one. This transformation is key in statistical modeling and analysis.</p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable with a probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>. Consider a function <span class="math inline">\(g\)</span> that maps <span class="math inline">\(X\)</span> to <span class="math inline">\(Y = g(X)\)</span>. The PDF of <span class="math inline">\(Y\)</span>, denoted as <span class="math inline">\(f_Y(y)\)</span>, can be found using the change-of-variable formula, which involves the derivative of the inverse function of <span class="math inline">\(g\)</span> and the PDF of <span class="math inline">\(X\)</span>. The formula is: <span class="math display">\[ f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|.\]</span></p>
<p><em>Note:</em> This approach requires <span class="math inline">\(g\)</span> to be a monotonic function (i.e., either strictly increasing or decreasing).</p>
<p><strong>Example: Analytical Approach</strong></p>
<p>Let <span class="math inline">\(X\)</span> be uniformly distributed between 0 and 1 (<span class="math inline">\(U(0,1)\)</span>), and consider the transformation <span class="math inline">\(Y = e^X\)</span>. The inverse function of <span class="math inline">\(g(x) = e^x\)</span> is <span class="math inline">\(g^{-1}(y) = \ln(y)\)</span>. The derivative of <span class="math inline">\(g^{-1}(y)\)</span> is <span class="math inline">\(\frac{1}{y}\)</span>. Since <span class="math inline">\(X\)</span> is uniform, <span class="math inline">\(f_X(x) = 1\)</span> for <span class="math inline">\(x\)</span> in [0,1]. The PDF of <span class="math inline">\(Y\)</span> is then <span class="math inline">\(f_Y(y) = 1/y\)</span> for <span class="math inline">\(y\)</span> in [1, <span class="math inline">\(e\)</span>].</p>
<p><strong>Example: Numerical Approach</strong> The numerical approach consists of the following two steps:</p>
<ol type="1">
<li>We first generate a large number of samples from the PDF of <span class="math inline">\(X\)</span> and apply <span class="math inline">\(g\)</span> to obtain samples of <span class="math inline">\(Y\)</span>. This step involves two main actions: simulation and transformation.</li>
</ol>
<ul>
<li><p><strong>Simulation</strong>: First, we simulate a large number of values from the probability density function (PDF) of the random variable <span class="math inline">\(X\)</span>. This simulation process requires selecting a distribution that <span class="math inline">\(X\)</span> follows and generating random samples from this distribution. This can be done using statistical software or programming languages like Python, which have built-in functions to generate random samples from various distributions.</p></li>
<li><p><strong>Transformation with Function <span class="math inline">\(g(\cdot)\)</span></strong>: Once we have these samples, the next step is to apply a function <span class="math inline">\(g\)</span> to each sampled value. This function <span class="math inline">\(g\)</span> transforms the original variable <span class="math inline">\(X\)</span> into a new variable <span class="math inline">\(Y\)</span>. The nature of <span class="math inline">\(g\)</span> can vary greatly depending on the analysis; it could be a simple linear function, a power function, an exponential function, or any other form of mathematical function. The crucial point is that each value of <span class="math inline">\(X\)</span> is independently put through the function <span class="math inline">\(g\)</span> to generate a corresponding value of <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<ol start="2" type="1">
<li>After transforming the simulated values, the next step is to analyze the distribution of the transformed values. There are many ways of achieving this.</li>
</ol>
<ul>
<li><p><strong>Empirical Distribution</strong>: This involves plotting the values of <span class="math inline">\(Y\)</span> to visualize their distribution. One common method is constructing a histogram, where the range of <span class="math inline">\(Y\)</span> values is divided into bins, and the frequency of values in each bin is plotted. This histogram provides a visual approximation of the PDF of <span class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Density Estimation</strong>: To get a smoother estimate of the PDF of <span class="math inline">\(Y\)</span>, kernel density estimation (KDE) can be used. KDE is a non-parametric way to estimate the probability density function of a random variable and can provide a clearer view of the distribution, especially when the transformation <span class="math inline">\(g\)</span> leads to a complex distribution.</p></li>
</ul>
<p>The following Python code illustrates the steps involved in empirically determing the distribution of the square of a normal distribution. Let <span class="math inline">\(X\)</span> be a standard normal variable. We first simulate a large number of values from <span class="math inline">\(X\)</span> and compute <span class="math inline">\(Y = X^2\)</span>. We then plot a histogram of the <span class="math inline">\(Y\)</span> values to approximate its distribution.</p>
<div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulation of a standard normal variable</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>x_values <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">10000</span>)  <span class="co"># 10,000 samples from N(0,1)</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>y_values <span class="op">=</span> x_values <span class="op">**</span> <span class="dv">2</span>  <span class="co"># square each value</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting the empirical distribution of Y</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plt.hist(y_values, bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Empirical Distribution of Y = X^2'</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Y values'</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="15">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-16-output-1.png" class="img-fluid figure-img" width="589"></p>
<figcaption class="figure-caption">Numerical estimation of the distribution of <span class="math inline">\(Y=X^2\)</span>, where <span class="math inline">\(X\)</span> is normally distributed.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>In summary, numerical approaches are essential when the analytical derivation of <span class="math inline">\(Y\)</span>’s distribution is complex or intractable. It provides a practical means to understand how transformations affect the distribution of a variable, which is a cornerstone in statistical analysis and data science.</p>
</section>
</section>
<section id="expectation-and-variance" class="level3" data-number="4.15.2">
<h3 data-number="4.15.2" class="anchored" data-anchor-id="expectation-and-variance"><span class="header-section-number">4.15.2</span> Expectation and Variance</h3>
<p>Computing the mean and variance of functions of random variables is a fundamental task in probability and statistics. The methods depend on whether the random variable is discrete or continuous, and the nature of the function applied to it.</p>
<section id="mean-of-a-function-of-a-random-variable" class="level4" data-number="4.15.2.1">
<h4 data-number="4.15.2.1" class="anchored" data-anchor-id="mean-of-a-function-of-a-random-variable"><span class="header-section-number">4.15.2.1</span> Mean of a Function of a Random Variable</h4>
<ol type="1">
<li><p><strong>Discrete Random Variables</strong>: Let <span class="math inline">\(X\)</span> be a discrete random variable and <span class="math inline">\(g(X)\)</span> be a function of <span class="math inline">\(X\)</span>. The mean or expected value of <span class="math inline">\(g(X)\)</span> is computed as: <span class="math display">\[ \mathbb{E}\left[g(X)\right] = \sum_{x} g(x) \cdot P(X = x).\]</span> Here, the sum is taken over all possible values of <span class="math inline">\(X\)</span>, and <span class="math inline">\(P(X = x)\)</span> is the probability mass function (PMF) of <span class="math inline">\(X\)</span>.</p></li>
<li><p><strong>Continuous Random Variables</strong>: For a continuous random variable <span class="math inline">\(X\)</span> with a probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>, the mean of <span class="math inline">\(g(X)\)</span> is: <span class="math display">\[ \mathbb{E}\left[g(X)\right] = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx.\]</span> The integral is taken over the entire range of <span class="math inline">\(X\)</span>.</p></li>
</ol>
</section>
<section id="variance-of-a-function-of-a-random-variable" class="level4" data-number="4.15.2.2">
<h4 data-number="4.15.2.2" class="anchored" data-anchor-id="variance-of-a-function-of-a-random-variable"><span class="header-section-number">4.15.2.2</span> Variance of a Function of a Random Variable</h4>
<p>The variance of <span class="math inline">\(g(X)\)</span> measures how much <span class="math inline">\(g(X)\)</span> is expected to deviate from its mean. It is calculated as: <span class="math display">\[ Var[g(X)] = \mathbb{E}\left[g(X)^2\right] - (\mathbb{E}\left[g(X)\right])^2.\]</span></p>
<ol type="1">
<li><p><strong>For Discrete Random Variables</strong>: Compute <span class="math inline">\(\mathbb{E}\left[g(X)^2\right]\)</span> as: <span class="math display">\[ \mathbb{E}\left[g(X)^2\right] = \sum_{x} g(x)^2 \cdot P(X = x), \]</span> and then, use the formula for variance.</p></li>
<li><p><strong>For Continuous Random Variables</strong>: Compute <span class="math inline">\(\mathbb{E}\left[g(X)^2\right]\)</span> by integrating: <span class="math display">\[\mathbb{E}\left[g(X)^2\right] = \int_{-\infty}^{\infty} g(x)^2 \cdot f_X(x) \, dx, \]</span> and then apply the variance formula.</p></li>
</ol>
</section>
<section id="example" class="level4" data-number="4.15.2.3">
<h4 data-number="4.15.2.3" class="anchored" data-anchor-id="example"><span class="header-section-number">4.15.2.3</span> Example</h4>
<p>Consider <span class="math inline">\(X\)</span> is a random variable representing the roll of a fair six-sided die, and <span class="math inline">\(g(X) = X^2\)</span> (the square of the roll).</p>
<ol type="1">
<li><p><strong>Mean of <span class="math inline">\(g(X)\)</span></strong>: <span class="math display">\[\mathbb{E}\left[g(X)\right] = \sum_{x=1}^{6} x^2 \cdot \frac{1}{6} = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6}.\]</span></p></li>
<li><p><strong>Variance of <span class="math inline">\(g(X)\)</span></strong>: First, compute <span class="math inline">\(\mathbb{E}\left[g(X)^2\right] = \sum_{x=1}^{6} x^4 \cdot \frac{1}{6}\)</span>. Then, use <span class="math inline">\(Var[g(X)] = \mathbb{E}\left[g(X)^2\right]- (\mathbb{E}\left[g(X)\right])^2\)</span>.</p></li>
</ol>
</section>
</section>
</section>
<section id="probabilistic-programming-languages" class="level2" data-number="4.16">
<h2 data-number="4.16" class="anchored" data-anchor-id="probabilistic-programming-languages"><span class="header-section-number">4.16</span> Probabilistic Programming Languages</h2>
<p>A probabilistic programming language (PPL) is a high-level programming language designed to describe probabilistic models and perform statistical inference within those models. Essentially, it allows us to define models in terms of probability distributions and then automatically perform complex computations like Bayesian inference.</p>
<p>Probabilistic Programming Languages (PPLs) and are particularly useful in machine learning for several reasons:</p>
<ul>
<li>PPLs allow for defining complex probabilistic models where functions of random variables can represent various stochastic processes or data generation mechanisms. For example, a function might transform a Gaussian random variable to model non-normal data.</li>
<li>Functions of random variables can be used to incorporate domain knowledge or specific hypotheses into a model. For example, a linear combination of random variables might represent a regression model, while more complex functions can represent non-linear relationships.</li>
<li>PPLs provide the flexibility to define arbitrary functions of random variables, making them highly expressive for modeling a wide range of phenomena. This includes transformations, nonlinear relationships, or hierarchical structures.</li>
</ul>
<p>Key features of PPLs include:</p>
<ol type="1">
<li><strong>Model Specification</strong>: Simplifies the specification of complex probabilistic models, often using syntax similar to standard programming languages.</li>
<li><strong>Inbuilt Inference Engines</strong>: PPLs typically come with in-built algorithms for performing inference, such as Markov Chain Monte Carlo (MCMC), variational inference, or other sampling methods.</li>
<li><strong>Flexibility and Extensibility</strong>: They allow for easy modification and extension of models, making them suitable for a wide range of applications from simple statistical tasks to complex machine learning models.</li>
</ol>
<p>Current state of the art packages for PPL are:</p>
<ol type="1">
<li><strong>Stan</strong>: A state-of-the-art platform for statistical modeling and high-performance statistical computation.</li>
<li><strong>PyMC3</strong>: A Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms.</li>
<li><strong>TensorFlow Probability (TFP)</strong>: A Python library built on TensorFlow for probabilistic reasoning and statistical analysis.</li>
<li><strong>Edward</strong>: A Python library for probabilistic modeling, inference, and criticism, integrated with TensorFlow.</li>
<li><strong>JAGS/BUGS</strong>: Software for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation.</li>
</ol>
<p>Here’s a simple example using TensorFlow Probability (TFP) to infer the distribution of <span class="math inline">\(Y = e^X\)</span>, where <span class="math inline">\(X\)</span> is normal.</p>
<div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability <span class="im">as</span> tfp</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>tfd <span class="op">=</span> tfp.distributions</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a standard normal distribution for X</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>dist_X <span class="op">=</span> tfd.Normal(loc<span class="op">=</span><span class="fl">0.</span>, scale<span class="op">=</span><span class="fl">1.</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a sample size</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>sample_size <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample from X</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>samples_X <span class="op">=</span> dist_X.sample(sample_size)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the transformation g(X) = exp(X) to get Y</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>samples_Y <span class="op">=</span> tf.exp(<span class="fl">0.5</span><span class="op">*</span>samples_X)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a><span class="co"># # Convert to numpy for plotting</span></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a><span class="co"># samples_Y_np = samples_Y.numpy()</span></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the histogram of X and Y</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>plt.hist(samples_X.numpy(), bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Histogram of X (normal distribution)'</span>)</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'X'</span>)</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>plt.subplot(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)<span class="op">;</span></span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>plt.hist(samples_Y.numpy(), bins<span class="op">=</span><span class="dv">50</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Histogram of Y = exp(X)'</span>)</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Y'</span>)</span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-execution_count="17">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output cell-output-display quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="random_variables_files/figure-html/cell-18-output-1.png" class="img-fluid figure-img" width="661"></p>
<figcaption class="figure-caption">Use of TensorFlow Probability to estimate distribution of <span class="math inline">\(Y=e^X\)</span>, where <span class="math inline">\(X\)</span> is normal.</figcaption>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>The above script uses TensorFlow and TensorFlow Probability to create a standard normal distribution, samples from it, applies a nonlinear transformation <span class="math inline">\(Y=e^X\)</span>, and then plots the histogram of the transformed samples. This is a practical demonstration of how functions of random variables can be explored within the TensorFlow framework, which is especially useful for complex probabilistic models in machine learning. We can use this approach to determine transformation of distributions by more complex functions, such as neural networks.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./functions.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Functions &amp; Function Spaces</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./random_processes.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Random Processes and Sequences</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>