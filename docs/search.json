[
  {
    "objectID": "intro.html#types-of-learning",
    "href": "intro.html#types-of-learning",
    "title": "1  Introduction to Machine Learning",
    "section": "1.1 Types of Learning",
    "text": "1.1 Types of Learning\nMachine learning can be broadly categorized into several types, each with its own methodology and application areas. The main types of machine learning are:\n\nSupervised Learning: This is the most prevalent type of machine learning. In supervised learning, the algorithm is trained on a labeled dataset, meaning that each example in the training dataset is paired with the correct output. The algorithm learns by comparing its actual output with correct outputs to find errors and modify the model accordingly. It is used for applications like regression and classification tasks.\nUnsupervised Learning: In unsupervised learning, the training data is not labeled, so the algorithm must find patterns and relationships in the data on its own. Common unsupervised learning methods include clustering and dimensionality reduction. These techniques are often used for exploratory data analysis, customer segmentation, and image and pattern recognition.\nSemi-Supervised Learning: This approach lies between supervised and unsupervised learning. It uses a small amount of labeled data along with a larger amount of unlabeled data. This method can improve learning accuracy while reducing the effort required to label data. It’s useful when labeling data becomes too expensive or time-consuming.\nReinforcement Learning: In reinforcement learning, an agent learns to make decisions by performing certain actions and receiving rewards or penalties in return. It’s different from other types of learning in that it focuses on performance in a dynamic environment and is based on feedback rather than data. Reinforcement learning is widely used in areas like robotics, gaming, and navigation.\nDeep Learning: A subset of machine learning, deep learning uses multi-layered neural networks to analyze various factors in large amounts of data. Deep learning is particularly known for its effectiveness in fields like computer vision, speech recognition, and natural language processing.\n\nEach type of machine learning has its strengths and is suitable for different kinds of problems and data sets. The choice of which type to use depends on the specific requirements and constraints of the task at hand.\nApart from the primary types of machine learning (supervised, unsupervised, semi-supervised, and reinforcement learning), there are several other techniques and approaches that can be used for learning in different contexts. These include:\n\nTransfer Learning: This technique involves taking a pre-trained model (usually trained on a large benchmark dataset) and fine-tuning it for a specific task. Transfer learning is particularly useful when the available data for a task is limited, as it leverages learned features from a related task.\nEnsemble Methods: Ensemble methods combine multiple machine learning models to improve performance. Techniques like bagging, boosting, and stacking are used to create a stronger model by aggregating the predictions from multiple models. Common examples include Random Forests and Gradient Boosted Machines.\nActive Learning: In active learning, the algorithm selectively queries the user or a database to label new data points with the greatest potential to improve the model. This approach is useful when labeled data is scarce or expensive to obtain.\nDimensionality Reduction: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) reduce the number of variables under consideration, making the data easier to explore and visualize. Dimensionality reduction is often used in preprocessing to improve the efficiency of other learning algorithms.\nFeature Engineering and Selection: This involves selecting the most relevant features or constructing new features from raw data to improve the performance of machine learning models.\nEvolutionary Algorithms: These are algorithms that mimic the process of natural selection to solve optimization problems. They are used for tasks where the search space is extremely large and complex.\nFederated Learning: A technique where machine learning models are trained across multiple decentralized devices or servers holding local data samples, without exchanging them. This approach is beneficial for privacy preservation and efficient use of bandwidth.\nRule-Based Learning: This method involves creating a set of rules for decision-making, which can be derived from domain knowledge or learned from data. It includes approaches like decision trees and rule-based classifiers.\nAnomaly Detection: Used to identify unusual patterns that do not conform to expected behavior. It is commonly used in fraud detection, network security, and fault detection.\nTime Series Analysis: Specialized techniques for analyzing time-ordered data, often involving unique methods for dealing with trends, seasonality, and autocorrelation in data.\n\nEach of these techniques addresses specific types of problems and data characteristics, offering a range of tools that can be applied to a wide array of learning tasks in different domains."
  },
  {
    "objectID": "intro.html#how-does-a-machine-learn",
    "href": "intro.html#how-does-a-machine-learn",
    "title": "1  Introduction to Machine Learning",
    "section": "1.2 How does a Machine Learn?",
    "text": "1.2 How does a Machine Learn?\n\n1.2.1 A Function Approximation Perspective\nMachine learning is a process where a computer system is taught to make predictions or decisions based on data, a process deeply intertwined with the concept of function approximation. At its core, machine learning involves a machine ‘learning’ from data—either labeled in supervised learning or unlabeled in unsupervised learning—by processing this data to extract patterns or rules. The crux of this process is to approximate a function that accurately maps inputs to outputs. For instance, in classification tasks, this function categorizes input data, while in regression tasks, it maps inputs to a continuous output. The overarching goal is to discover the function that most accurately represents the relationship between these inputs and outputs.\nDifferent machine learning models, such as decision trees, neural networks, or support vector machines, employ various approaches to function approximation. The choice of model is dictated by the complexity of the function being approximated and the specifics of the problem at hand. In essence, machine learning is a quest to find the best possible mapping from inputs to outputs, based on the available data. The effectiveness of a machine learning model is largely determined by how well it approximates the underlying function and how effectively it generalizes this knowledge to new data.\nVarious architectures or models used in machine learning, each serving as a different approach to function approximation, include:\n\nLinear and Logistic Regression: These are the simplest forms of function approximators used for prediction. Linear regression is used for continuous output prediction, and logistic regression is employed for binary classification tasks. They approximate a linear relationship between input features and the output.\nDecision Trees and Random Forests: Decision trees split data based on certain criteria and are particularly good for interpretability. Random forests are an ensemble of decision trees and are more robust and less prone to overfitting. They approximate functions by segmenting the input space into simpler, easier to model regions.\nSupport Vector Machines (SVMs): SVMs are effective in high-dimensional spaces and for classification tasks. They work by finding a hyperplane that best divides a dataset into classes. SVMs are particularly good for approximating complex nonlinear relationships using kernel methods.\nNeural Networks and Deep Learning: These models consist of layers of interconnected nodes or neurons and are capable of learning complex, nonlinear relationships. Deep learning models, with multiple hidden layers, are particularly potent at approximating functions from large amounts of data, especially in fields like image and speech recognition.\nConvolutional Neural Networks (CNNs): Specialized for processing structured array data like images, CNNs are powerful in function approximation for tasks like image classification and object detection. They do this by learning spatial hierarchies of features from the input data.\nRecurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM): These are designed for sequential data like time series or language. RNNs and LSTM models can capture temporal dynamics and are useful in approximating functions where the output is dependent on previous elements in the input sequence.\nAutoencoders: Used for unsupervised learning tasks like dimensionality reduction or feature learning, autoencoders learn efficient data codings in an unsupervised manner, often for the purpose of reconstructing the input data from compressed representations.\nReinforcement Learning Models: These models learn optimal actions through trial and error to maximize some notion of cumulative reward. They are typically used in dynamic environments where the function to be approximated is the best action to take in a given state.\n\nEach of these models has its strengths and is suited for particular types of problems and data characteristics. The choice of model depends on the specific requirements of the task at hand, such as the complexity of the function to be approximated, the nature of the input and output data, and the amount of training data available.\n\n\n1.2.2 Learning as an Optimization Process\nThe learning process is often described as ‘training a model.’ Here, a machine learning algorithm iteratively adjusts its parameters to minimize the difference between its predictions and the actual outcomes in the training data, effectively seeking the function that best fits the data. The machine learning model, in this sense, acts as a function approximator, striving to estimate the true underlying function that describes the input-output relationship. The accuracy of this approximation is influenced by the model’s complexity, the nature of the data, and the algorithm used.\nA key part of this process is minimizing an error or loss function, which quantifies the divergence between the model’s predictions and the actual values. Successfully minimizing this error leads to a more accurate approximation of the underlying function. An essential aspect of a machine learning model is its ability to generalize from the training data to unseen data, ensuring that a good function approximation not only fits the training data well but can also predict new, unseen data accurately.\nIn machine learning, cost functions or loss functions quantify the error between the predicted values by the model and the actual values in the data. Different problems use different loss functions, depending on the nature of the task. Some of the commonly used loss functions are summarized below.\n\nMean Squared Error (MSE):\n\nUsed in: Regression Problems.\nDescription: MSE measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. It’s widely used due to its simplicity and the fact that it penalizes larger errors more heavily.\n\nMean Absolute Error (MAE):\n\nUsed in: Regression Problems.\nDescription: MAE measures the average of the absolute differences between predicted values and actual values. It gives a linear score, which means all individual differences are weighted equally in the average.\n\nCross-Entropy Loss or Log Loss:\n\nUsed in: Classification Problems.\nDescription: Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1. It increases as the predicted probability diverges from the actual label, making it ideal for models where the outputs are probabilities.\n\nHinge Loss:\n\nUsed in: Support Vector Machines for binary classification.\nDescription: Hinge loss is used primarily for “maximum-margin” classification, most notably for support vector machines. It is intended to create a wide margin between data points of different classes.\n\nBinary Cross-Entropy Loss:\n\nUsed in: Binary Classification Problems.\nDescription: A special case of cross-entropy loss for binary classification tasks. It calculates the cross-entropy loss between the predicted and actual labels.\n\nCategorical Cross-Entropy Loss:\n\nUsed in: Multi-class Classification Problems.\nDescription: It’s used when assigning an observation to one of more than two classes. This loss function is ideal for multi-class classification where each example belongs to a single class.\n\nSparse Categorical Cross-Entropy Loss:\n\nUsed in: Multi-class Classification Problems with many classes.\nDescription: It’s the same as categorical cross-entropy but is used when your classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array of mostly zeros).\n\nKullback-Leibler (KL) Divergence:\n\nUsed in: Model reliability and probability distributions.\nDescription: KL Divergence measures how one probability distribution diverges from a second, expected probability distribution. It’s used in scenarios like variational autoencoders in deep learning.\n\nHuber Loss:\n\nUsed in: Robust Regression Problems.\nDescription: Huber loss is less sensitive to outliers in data than the squared error loss. It’s used in robust regression, combining the properties of MSE and MAE.\n\nCosine Similarity Loss:\n\nUsed in: Measuring similarity between two vectors.\nDescription: This loss function measures the cosine of the angle between two vectors and is used in various applications like recommendation systems and text analysis where the magnitude of vectors is not as important as their direction. This is essentially the inner product between two vectors.\n\n\nEach of these loss functions has a specific scenario or type of problem where they are most effective, and the choice of a loss function can significantly influence the performance of the machine learning model."
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Introduction to Machine Learning",
    "section": "1.3 Summary",
    "text": "1.3 Summary\nIn conclusion, machine learning can fundamentally be understood as a process of function representation and optimization. At its heart, it revolves around the concept of accurately representing complex relationships within data through mathematical functions, and then optimizing these functions to minimize errors in predictions or decisions. Whether it’s through supervised learning that maps input to output, unsupervised learning that uncovers hidden patterns, or reinforcement learning that navigates through a space of actions, each method seeks to approximate an underlying function as closely as possible. The optimization aspect, achieved through various algorithms and loss functions, refines these representations, striving to improve accuracy and efficiency. This dual focus on representation and optimization underscores the essence of machine learning, highlighting its role as a powerful tool in translating vast, often unstructured data into meaningful insights and actions."
  },
  {
    "objectID": "linear_algebra.html#vectors",
    "href": "linear_algebra.html#vectors",
    "title": "2  Linear Algebra",
    "section": "2.1 Vectors",
    "text": "2.1 Vectors\nA vector of numbers is a one-dimensional arrangement of numbers (integers, real, complex, etc.).\nA column-vector is a column arrangement of mathematical objects, e.g., \\[\\begin{bmatrix} v_1 \\\\ v_2 \\\\v_3\\\\ \\vdots \\\\ v_n\\end{bmatrix}.\\]\nExamples include vector of integers: \\(\\begin{bmatrix} 1 \\\\ 2 \\\\3\\\\ \\vdots \\\\ 20\\end{bmatrix}\\), vector of real-numbers: \\(\\begin{bmatrix} 1.234 \\\\ 2.345 \\\\3.456\\\\ \\vdots \\\\ 20.212\\end{bmatrix}\\), and vector of complex numbers: \\(\\begin{bmatrix} 1+2j \\\\ 2+3j \\\\4+5j\\\\ \\vdots \\\\ 20+21j\\end{bmatrix}\\).\nSometimes we represent the entire vector using a symbol with boldface font, e.g., \\[\\boldsymbol{x}=  \\begin{bmatrix} 1 \\\\ 2 \\\\3\\\\ \\vdots \\\\5 \\end{bmatrix},\\] and refer to the entire vector as \\(\\boldsymbol{x}\\).\n\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5])  # This is an array with 5 elements.\nprint(X)\n\n[1 2 3 4 5]\n\n\n\n2.1.1 Dimension\nThe number of elements in a vector is the dimension of the vector. For example, \\[\\boldsymbol{x}=  \\begin{bmatrix} 1 \\\\ 2 \\\\3\\\\ \\vdots \\\\10 \\end{bmatrix},\\] is ten dimensional. If all the entries are real numbers, then we compactly represent \\(\\boldsymbol{x}\\in\\mathcal{R}^{10}\\), where \\(\\mathcal{R}^n\\) is the space of n-dimensional vectors with real entries. A vector can have elements that are complex numbers as well. In that case, we will denote \\(\\boldsymbol{x}\\in\\mathcal{C}^{n}\\), where \\(\\mathcal{C}^n\\) is the space of n-dimensional vectors with complex entries.\nHere is a Python code computing the dimension of a vector.\n\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # This is an array with 10 elements.\nprint(\"Dimension of X is: \", X.shape)\n\nDimension of X is:  (10,)\n\n\n\n\n2.1.2 Transpose\nIf a vector \\(\\boldsymbol{x}\\in\\mathcal{R}^n := \\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\) then the transpose of \\(\\boldsymbol{x}\\) is denoted by \\(\\boldsymbol{x}^T\\) and defined by \\[\n\\boldsymbol{x}^T = \\begin{bmatrix} x_1 & \\cdots & x_n \\end{bmatrix}.\\] Note: \\((\\boldsymbol{x}^T)^T = \\boldsymbol{x}\\), i.e. the transpose of a transposed vector is the same vector.\nNote: If \\(\\boldsymbol{x}\\) is a column vector, \\(\\boldsymbol{x}^T\\) is a row vector and vice versa.\nHere is a Python code computing transpose of a vector.\n\nimport numpy as np\n\n# Define a vector as a 1D NumPy array\nvector = np.array([1, 2, 3, 4])\n\n# Compute the transpose of the vector (which is still the same vector)\ntranspose_vector = vector.T\n\n# Print the original vector and its transpose\nprint(\"Vector:\", vector)\n\nprint(\"Transpose:\", transpose_vector)\nprint(transpose_vector)\n\nVector: [1 2 3 4]\nTranspose: [1 2 3 4]\n[1 2 3 4]\n\n\nPython doesn’t print the transpose of a column vector as a row vector. If they are treated as matrices then there will be a difference in how they are printed.\n\n\n2.1.3 Operations\n\n2.1.3.1 Negative of Vector\nGiven a vector \\(\\boldsymbol{x}= \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}\\), the negative of \\(\\boldsymbol{x}\\) is \\[-\\boldsymbol{x}:= \\begin{bmatrix}-x_1\\\\ \\vdots \\\\ -x_n\\end{bmatrix}.\\]\n\n\n2.1.3.2 Multiplication by a Scalar\nMultiplication of a scalar with a vector is defined as \\[\n\\alpha\\boldsymbol{x}=  \\begin{bmatrix}\\alpha x_1 \\\\ \\alpha x_2 \\\\ \\vdots \\end{bmatrix},\n\\] i.e., the scalar \\(\\alpha\\) multiplies all the elements of \\(\\boldsymbol{x}\\). Also, \\[\\alpha\\boldsymbol{x}= \\boldsymbol{x}\\alpha.\\]\n\n\n2.1.3.3 Vector Equality\nVector equality between two vectors is defined elementwise, i.e., the condition \\(\\boldsymbol{x}= \\boldsymbol{y}\\) is equivalent to \\(n\\) conditions \\(x_i = y_i\\), for \\(i=1,\\cdots,n\\). Clearly, \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) would have to have the same dimension.\n\n\n2.1.3.4 Vector Addition\nGiven two vectors \\(\\boldsymbol{x}\\in \\mathcal{R}^n\\) and \\(\\boldsymbol{y}\\in\\mathcal{R}^n\\) with respective components \\(x_i\\) and \\(y_i\\), then \\(\\boldsymbol{z}:= \\boldsymbol{x}+\\boldsymbol{y}\\) has components \\(z_i := x_i + y_i\\). That is \\[\\boldsymbol{z}:= \\boldsymbol{x}+ \\boldsymbol{y}= \\begin{bmatrix}x_1\\\\ \\vdots \\\\ x_n\\end{bmatrix} + \\begin{bmatrix}y_1\\\\ \\vdots \\\\ y_n\\end{bmatrix} = \\begin{bmatrix}x_1 + y_1\\\\ \\vdots \\\\ x_n + y_n\\end{bmatrix} = \\begin{bmatrix}z_1\\\\ \\vdots \\\\ z_n\\end{bmatrix} .\\] Note: \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) must of the same dimension.\nVector addition satisfies the following properties: \\[\\begin{align*}\n\\boldsymbol{x}+ (\\boldsymbol{y}+ \\boldsymbol{z}) &= (\\boldsymbol{x}+ \\boldsymbol{y}) + \\boldsymbol{z},\\\\\n\\alpha(\\boldsymbol{x}+ \\boldsymbol{y})  &= \\alpha\\boldsymbol{x}+ \\alpha\\boldsymbol{y}.\n\\end{align*}\\]\n\n\n2.1.3.5 Vector Subtraction\nGiven two vectors \\(\\boldsymbol{x}\\in \\mathcal{R}^n\\) and \\(\\boldsymbol{y}\\in\\mathcal{R}^n\\) with respective components \\(x_i\\) and \\(y_i\\), then \\(\\boldsymbol{z}:= \\boldsymbol{x}-\\boldsymbol{y}= \\boldsymbol{x}+ (-\\boldsymbol{y})\\). Note: \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\) must of the same dimension.\n\n\n2.1.3.6 Inner Product\nInner product between vectors \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) and \\(\\boldsymbol{y}\\in\\mathcal{R}^n\\) is defined as \\[\\langle\\boldsymbol{x},\\boldsymbol{y}\\rangle := \\boldsymbol{x}^T\\boldsymbol{y}= \\boldsymbol{y}^T\\boldsymbol{x}= \\Sigma_{i=1}^n x_iy_i.\\] Note: Inner product results in a scalar.\nHere is a Python code computing inner product of vectors.\n\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\nY = np.array([6, 7, 8, 9, 10])\nprint(\"Inner product between X and Y: \", X.dot(Y))\nprint(\"Inner product between Y and X: \", Y.dot(X)) # Should get the same result.\n\nInner product between X and Y:  130\nInner product between Y and X:  130\n\n\n\n\n2.1.3.7 Outer Product\nOuter product between two vectors \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) and \\(\\boldsymbol{y}\\in\\mathcal{R}^m\\) is defined as \\[\n\\boldsymbol{x}\\boldsymbol{y}^T = \\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix} \\begin{bmatrix}y_1 & y_2 & \\cdots & y_n\\end{bmatrix} = \\begin{bmatrix} x_1y_1 & x_1y_2 & \\cdots & x_1y_m\\\\\nx_2y_1 & x_2y_2 & \\cdots & x_2 y_m \\\\\n\\vdots & \\vdots & & \\vdots \\\\\nx_n y_1 & x_n y_2 & \\cdots & x_n y_m\n\\end{bmatrix}\n\\] Note: Clearly \\(\\boldsymbol{x}\\boldsymbol{y}^T \\neq \\boldsymbol{y}\\boldsymbol{x}^T\\).  Note: Outer product result in a matrix.\nHere is a Python code computing outer product of vectors.\n\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\nY = np.array([6, 7, 8, 9, 10])\nprint(\"Outer product between X and Y: \\n\", np.multiply.outer(X, Y))\nprint(\"\\nOuter product between Y and X: \\n\", np.multiply.outer(Y, X))\n\nOuter product between X and Y: \n [[ 6  7  8  9 10]\n [12 14 16 18 20]\n [18 21 24 27 30]\n [24 28 32 36 40]\n [30 35 40 45 50]]\n\nOuter product between Y and X: \n [[ 6 12 18 24 30]\n [ 7 14 21 28 35]\n [ 8 16 24 32 40]\n [ 9 18 27 36 45]\n [10 20 30 40 50]]\n\n\n\n\n\n2.1.4 Length of a Vector\nThe length of a vector \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) is denoted by \\[\\|\\boldsymbol{x}\\|_2 := \\sqrt{x_1^2 + \\cdots + x_n^2} = \\sqrt{\\boldsymbol{x}^T\\boldsymbol{x}}.\\] Therefore, the length of a vector is the square-root of the inner product with itself. Often we will drop the subscript \\(2\\) in \\(\\|\\boldsymbol{x}\\|_2\\), when it is clear from context.\nHere is a Python code computing length of a vector.\n\nimport numpy as np\n\nX = np.array([1, 2, 3, 4, 5])\nprint(\"Length of vector X is: \\n\", np.sqrt(X.dot(X)))\n\nLength of vector X is: \n 7.416198487095663\n\n\n\n\n2.1.5 Vector Norms\nA norm is a function \\(\\|\\cdot\\|: \\mathcal{V} \\rightarrow \\mathcal{R}\\) from a vector space \\(\\mathcal{V}\\) over a field (typically \\(\\mathcal{R}^n\\) or \\(\\mathcal{C}^n\\)) to the non-negative real numbers. It satisfies the following properties:\n\nNon-negativity: \\[\\forall \\boldsymbol{x}\\in \\mathcal{V}, \\|\\boldsymbol{x}\\| \\geq 0 \\text{ and } \\|\\boldsymbol{x}\\| = 0 \\iff \\boldsymbol{x}\\text{ is the zero vector}.\\]\nScalar Multiplication: \\[\\forall \\alpha \\text{ (a scalar) and } \\forall \\boldsymbol{x}\\in \\mathcal{V}, \\|\\alpha \\boldsymbol{x}\\| = |\\alpha| \\|\\boldsymbol{x}\\|.\\]\nTriangle Inequality: \\[\\forall \\boldsymbol{x}, \\boldsymbol{y}\\in \\mathcal{V}, \\|\\boldsymbol{x}+ \\boldsymbol{y}\\| \\leq \\|\\boldsymbol{x}\\| + \\|\\boldsymbol{y}\\|.\\]\nDefiniteness: \\[\\|\\boldsymbol{x}\\| = 0 \\iff \\boldsymbol{x}\\text{ is the zero vector}.\\]\n\nIn general, we can define \\[\\|\\boldsymbol{x}\\|_p := \\left(x_1^p + \\cdots + x_n^p\\right)^{\\frac{1}{p}},\\] which is the \\(p^\\text{th}\\) norm of \\(\\boldsymbol{x}\\).\nWe have for \\(p=1,2,\\infty\\):\n\nManhattan Norm (\\(l_1\\) Norm): \\[\\|\\boldsymbol{x}\\|_1 = |x_1| + |x_2| + \\cdots + |x_n|.\\]\nEuclidean Norm (\\(l_2\\) Norm): \\[\\|\\boldsymbol{x}\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\cdots + x_n^2}.\\]\nMaximum Norm (\\(l_\\infty\\) Norm): \\[\\|\\boldsymbol{x}\\|_{\\infty} = \\max(|x_1|, |x_2|, \\cdots, |x_n|).\\]\n\nHere is a Python code computing various vector norms.\n\nimport numpy as np\n\n# Define a vector as a NumPy array\nvector = np.array([1, 2, 3])\n\n# Compute different vector norms\nL1_norm = np.linalg.norm(vector, ord=1)  # L1 norm (Manhattan norm)\nL2_norm = np.linalg.norm(vector, ord=2)  # L2 norm (Euclidean norm)\ninf_norm = np.linalg.norm(vector, ord=np.inf)  # Infinity norm (Maximum norm)\n\n# Print the original vector and its norms\nprint(\"Vector:\")\nprint(vector)\n\nprint(f\"L1 Norm: {L1_norm}\")\nprint(f\"L2 Norm: {L2_norm}\")\nprint(f\"Infinity Norm: {inf_norm}\")\n\nVector:\n[1 2 3]\nL1 Norm: 6.0\nL2 Norm: 3.7416573867739413\nInfinity Norm: 3.0"
  },
  {
    "objectID": "linear_algebra.html#matrices",
    "href": "linear_algebra.html#matrices",
    "title": "2  Linear Algebra",
    "section": "2.2 Matrices",
    "text": "2.2 Matrices\nMatrices are two-dimensional arrangement of mathematical objects, such as real-numbers, complex numbers, functions, etc.\nAn \\(m\\times n\\) matrix is defined as \\[\n\\boldsymbol{A}= \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n}\\\\\na_{21} & a_{22} & \\cdots & a_{2n}\\\\\n\\vdots & \\vdots & & \\vdots,\\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix},\n\\] where \\(a_{ij}\\) is the matrix element in the \\(i^\\text{th}\\) row and the \\(j^\\text{th}\\) column. The bold-faced symbol \\(\\boldsymbol{A}\\) is used to refer to the entire matrix.\n\n2.2.1 Dimension\nThe dimension of a matrix is defined by the number of rows and the number of columns. A matrix with dimension \\(m\\times n\\) has \\(m\\) rows and \\(n\\) columns. We will use the notation \\(\\boldsymbol{A}\\in \\mathcal{R}^{m\\times n}\\) to represent \\(m\\times n\\) vectors of real numbers.\nA matrix of dimension \\(m\\times n\\) is called square if \\(m=n\\).\n\n\n2.2.2 Transpose\nMatrix transpose is denote by \\(\\boldsymbol{A}^T\\), which is defined by interchanging the rows and columns. For example,\n\\[\n\\boldsymbol{A}= \\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n}\\\\\na_{21} & a_{22} & \\cdots & a_{2n}\\\\\n\\vdots & \\vdots & & \\vdots,\\\\\na_{m1} & a_{m2} & \\cdots & a_{mn}\n\\end{bmatrix}, \\text{ then }\n\\boldsymbol{A}^T = \\begin{bmatrix}\na_{11} & a_{21} & \\cdots & a_{m1}\\\\\na_{12} & a_{22} & \\cdots & a_{m2}\\\\\n\\vdots & \\vdots & & \\vdots,\\\\\na_{1n} & a_{2n} & \\cdots & a_{mn}\n\\end{bmatrix}.\n\\]\nThat is, if \\(\\boldsymbol{B}=\\boldsymbol{A}^T\\), then \\(b_{ij} = a_{ji}\\).\nTherefore, if \\(\\boldsymbol{A}\\) is an \\(m\\times n\\) matrix, then \\(\\boldsymbol{A}^T\\) is an \\(n\\times m\\) matrix.\nHere is a Python code computing matrix transpose.\n\nimport numpy as np\n\n# Define a matrix\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Compute the transpose\ntranspose_matrix = np.transpose(matrix)\n\n# Alternatively, you can use the T attribute to compute the transpose\n# transpose_matrix = matrix.T\n\n# Print the original matrix and its transpose\nprint(\"Matrix:\")\nprint(matrix)\n\nprint(\"Transpose:\")\nprint(transpose_matrix)\n\nMatrix:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nTranspose:\n[[1 4 7]\n [2 5 8]\n [3 6 9]]\n\n\n\n\n2.2.3 Matrix Construction from Vectors\nGiven \\(n\\) vectors \\(\\boldsymbol{x}_i\\), for \\(i=1,\\cdots,n\\), each of dimension \\(m\\); we can construct an \\(m\\times n\\) matrix as \\[\\boldsymbol{A}= \\begin{bmatrix}\\boldsymbol{x}_1 & \\boldsymbol{x}_2 & \\cdots & \\boldsymbol{x}_n\\end{bmatrix},\\] and \\[\\boldsymbol{A}^T = \\begin{bmatrix}\\boldsymbol{x}^T_1 \\\\ \\boldsymbol{x}^T_2 \\\\ \\vdots \\\\ \\boldsymbol{x}^T_n\\end{bmatrix}.\\]\nHere is a Python code to construct matrices from vectors.\n\nimport numpy as np\n\n# Define two vectors as NumPy arrays\nvector1 = np.array([1, 2, 3])\nvector2 = np.array([4, 5, 6])\n\n# Construct a matrix by stacking vectors horizontally (as rows)\nmatrix_horizontal = np.hstack((vector1.reshape(-1, 1), vector2.reshape(-1, 1)))\n\n# Construct a matrix by stacking vectors vertically (as columns)\nmatrix_vertical = np.vstack((vector1, vector2))\n\n# Print the original vectors and the constructed matrices\nprint(\"Vector 1:\")\nprint(vector1)\n\nprint(\"Vector 2:\")\nprint(vector2)\n\nprint(\"Matrix Constructed Horizontally (as rows):\")\nprint(matrix_horizontal)\n\nprint(\"Matrix Constructed Vertically (as columns):\")\nprint(matrix_vertical)\n\nVector 1:\n[1 2 3]\nVector 2:\n[4 5 6]\nMatrix Constructed Horizontally (as rows):\n[[1 4]\n [2 5]\n [3 6]]\nMatrix Constructed Vertically (as columns):\n[[1 2 3]\n [4 5 6]]\n\n\n\n\n2.2.4 Rank of a Matrix\nThe rank of a matrix is a fundamental concept in linear algebra and represents the maximum number of linearly independent rows or columns in the matrix. In other words, it quantifies the dimensionality of the vector space spanned by the rows or columns of the matrix. A matrix’s rank can provide important information about its properties and relationships between its rows and columns.\nA matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank.\nThe rank of a matrix can be determined by various methods, including row reduction (Gaussian elimination) and by counting the number of non-zero rows or columns in its reduced row echelon form (RREF).\nHere is a Python code computing rank of a matrix\n\nimport numpy as np\n\n# Define a matrix \nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Compute the rank of the matrix\nrank = np.linalg.matrix_rank(matrix)\n\n# Print the matrix and its rank\nprint(\"Matrix:\")\nprint(matrix)\n\nprint(f\"Rank: {rank}\")\n\nMatrix:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nRank: 2\n\n\n\n\n2.2.5 Eigen-Values and Eigen-Vectors of a Matrix\nEigenvalues, also known as characteristic values or latent roots, are a fundamental concept in linear algebra and matrix theory. They are associated with square matrices and play a crucial role in various applications, including physics, engineering, data analysis, and more.\nAn eigenvalue of a square matrix \\(\\boldsymbol{A}\\) is a scalar (a single number) \\(\\lambda\\) such that when \\(\\boldsymbol{A}\\) is multiplied by a certain nonzero vector \\(\\boldsymbol{v}\\), the result is a scaled version of \\(\\boldsymbol{v}\\):\n\\[ \\boldsymbol{A}\\boldsymbol{v} = \\lambda\\boldsymbol{v}.\\]\nIn this equation:\n\n\\(\\boldsymbol{A}\\) is the square matrix.\n\\(\\boldsymbol{v}\\) is a nonzero vector called the eigenvector associated with the eigenvalue \\(\\lambda\\).\n\\(\\lambda\\) is the eigenvalue.\n\nIn simpler terms, when you multiply a matrix by its eigenvector, the result is a vector that points in the same direction as the original eigenvector, but possibly with a different magnitude (scaled by \\(\\lambda\\)).\nTo find the eigenvalues of a matrix in practice, we typically use numerical methods or specialized libraries like NumPy in Python. The eigenvalues can be computed using functions like numpy.linalg.eigvals in Python, which returns an array of eigenvalues for a given matrix.\nHere is a Python code computing the eigenvalues of a matrix.\n\nimport numpy as np\n\n# Define a square matrix\nmatrix = np.array([[1, 2],\n                   [3, 4]])\n\n# Compute the eigenvalues\neigenvalues = np.linalg.eigvals(matrix)\n\n# Print the eigenvalues\nprint(\"Eigenvalues:\")\nprint(eigenvalues)\n\nEigenvalues:\n[-0.37228132  5.37228132]\n\n\n\n\n2.2.6 Operations\n\n2.2.6.1 Negative Matrix\nIf \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\) is a matrix with elements \\(a_{ij}\\), then \\(-\\boldsymbol{A}\\) is defined by elements \\(-a_{ij}\\).\n\n\n2.2.6.2 Multiplication by a Scalar\nIf \\(\\boldsymbol{A}\\) is a matrix defined by elements \\(a_{ij}\\), then \\(\\alpha\\boldsymbol{A}\\) is defined by the elements \\(\\alpha a_{ij}\\).\n\n\n2.2.6.3 Matrix Equality\nMatrix equality is defined only for two matrices with the same dimension, and is defined elementwise. For two matrics \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\) and \\(\\boldsymbol{B}\\in\\mathcal{R}^{m\\times n}\\), the condition \\(\\boldsymbol{A}=\\boldsymbol{B}\\) is equivalent to \\(mn\\) conditions \\(a_{ij} = b_{ij}\\), for \\(i=1,\\cdots,m\\) and \\(j=1,\\cdots,n\\).\n\n\n2.2.6.4 Matrix Addition\nAddition of two matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) is elementwise, and is only defined if \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) have the same dimension. If \\(\\boldsymbol{C}=\\boldsymbol{A}+\\boldsymbol{B}\\), then \\(c_{ij} = a_{ij} + b_{ij}\\), for \\(i=1,\\cdots,m\\) and \\(j=1,\\cdots,n\\).\n\n\n2.2.6.5 Matrix Product\nProduct between two matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) is denoted by \\(\\boldsymbol{A}\\boldsymbol{B}\\). If \\(\\boldsymbol{C}= \\boldsymbol{A}\\boldsymbol{B}\\), then the elements of \\(\\boldsymbol{C}\\) are defined by \\[c_{ij} = \\sum_{k=1}^p a_{ik}b_{kj},\\] where the dimensions \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) must be compatible. The matrix product \\(\\boldsymbol{A}\\boldsymbol{B}\\) is defined only if the dimension of \\(\\boldsymbol{A}\\) is \\(m\\times p\\) and the dimension of \\(\\boldsymbol{B}\\) is \\(p\\times n\\), i.e. \\(\\boldsymbol{A}\\) must have the same number of columns as the number of rows of \\(\\boldsymbol{B}\\). In that case, the matrix \\(\\boldsymbol{C}=\\boldsymbol{A}\\boldsymbol{B}\\) has dimension \\(m \\times n\\).\nMatrix product satisfies the following conditions: \\[\\begin{align*}\n    &\\alpha\\boldsymbol{A}= \\boldsymbol{A}\\alpha, \\text{ where $\\alpha$ is a scalar};\\\\\n    &\\boldsymbol{A}\\boldsymbol{B}\\neq \\boldsymbol{B}\\boldsymbol{A}, \\\\\n    &\\alpha(\\boldsymbol{A}\\boldsymbol{B}) = (\\alpha\\boldsymbol{A})\\boldsymbol{B}= \\boldsymbol{A}(\\alpha\\boldsymbol{B}) = (\\boldsymbol{A}\\boldsymbol{B})\\alpha,\\\\\n    &\\boldsymbol{A}(\\boldsymbol{B}+\\boldsymbol{C}) = \\boldsymbol{A}\\boldsymbol{B}+ \\boldsymbol{A}\\boldsymbol{C},\\\\\n    &(\\boldsymbol{A}\\boldsymbol{B})^T = \\boldsymbol{B}^T\\boldsymbol{A}^T.\n    \\end{align*}\\]\nHere is Python code showing matrix multiplication.\n\nimport numpy as np\n\n# Define two matrices\nmatrix_A = np.array([[1, 2],\n                     [3, 4]])\n\nmatrix_B = np.array([[5, 6],\n                     [7, 8]])\n\n# Method 1: Using numpy.dot\nresult_dot = np.dot(matrix_A, matrix_B)\n\n# Method 2: Using the @ operator\nresult_operator = matrix_A @ matrix_B\n\n# Print the original matrices and their multiplication results\nprint(\"Matrix A:\")\nprint(matrix_A)\n\nprint(\"Matrix B:\")\nprint(matrix_B)\n\nprint(\"Matrix Multiplication using numpy.dot:\")\nprint(result_dot)\n\nprint(\"Matrix Multiplication using @ operator:\")\nprint(result_operator)\n\nMatrix A:\n[[1 2]\n [3 4]]\nMatrix B:\n[[5 6]\n [7 8]]\nMatrix Multiplication using numpy.dot:\n[[19 22]\n [43 50]]\nMatrix Multiplication using @ operator:\n[[19 22]\n [43 50]]\n\n\n\n\n\n2.2.7 Symmetric Matrices\nA symmetric matrix is a square matrix that satisfies the condition \\(\\boldsymbol{A}= \\boldsymbol{A}^T\\), i.e. \\(a_{ij} = a_{ji}\\). We often denote the space of symmetric matrices as \\(\\mathcal{S}^n\\), representing \\(n\\times n\\) symmetric matrices.\nNote: Eigen values of a symmetric matrix are all real.\nHere is a Python code to generate a symmetric matrix.\n\nimport numpy as np\n\n# Specify the size of the symmetric matrix (e.g., 3x3)\nmatrix_size = 3\n\n# Generate a random square matrix\nrandom_matrix = np.random.rand(matrix_size, matrix_size)\n\n# Make it symmetric by copying the lower triangular part to the upper triangular part\nsymmetric_matrix = (random_matrix + random_matrix.T) / 2\n\n# Print the symmetric matrix\nprint(\"Symmetric Matrix:\")\nprint(symmetric_matrix)\n\n# Compute the eigenvalues\neigenvalues = np.linalg.eigvals(symmetric_matrix)\n\n# All eigen values should be real\nprint(\"Eigen values:\", eigenvalues)\n\n# Check if all elements are real\nare_all_real = np.isreal(eigenvalues).all()\n\n# Assert that all elements are real\nassert are_all_real, \"All eigen values are not real.\"\n\n# If the assertion passes, it means all elements are real\nprint(\"All eigen values are real.\")\n\nSymmetric Matrix:\n[[0.38813146 0.61651741 0.72064304]\n [0.61651741 0.07085289 0.33985745]\n [0.72064304 0.33985745 0.30298993]]\nEigen values: [ 1.4171432  -0.49250478 -0.16266415]\nAll eigen values are real.\n\n\n\n\n2.2.8 Skew Symmetric Matrices\nA skew-symmetric matrix is a square matrix and satisfies the condition \\(\\boldsymbol{A}= -\\boldsymbol{A}^T\\), i.e. \\(a_{ij} = -a_{ji}\\). The diagonal elements of skew-symmetric matrices are always zero, because \\(a_{ii} = -a_{ii}\\) only when \\(a_{ii} = 0\\).\nHere is a Python code to generate a skew-symmetric matrix\n\nimport numpy as np\n\n# Specify the size of the skew-symmetric matrix (e.g., 3x3)\nmatrix_size = 3\n\n# Generate a random square matrix\nrandom_matrix = np.random.rand(matrix_size, matrix_size)\n\n# Make it skew-symmetric by subtracting its transpose\nskew_symmetric_matrix = random_matrix - random_matrix.T\n\n# Print the skew-symmetric matrix\nprint(\"Skew-Symmetric Matrix:\")\nprint(skew_symmetric_matrix)\n\nSkew-Symmetric Matrix:\n[[ 0.          0.7426866   0.05874133]\n [-0.7426866   0.          0.08401395]\n [-0.05874133 -0.08401395  0.        ]]\n\n\n\n\n2.2.9 Positive (Semi) Definite Matrices\n\nA symmetric matrix \\(\\boldsymbol{A}\\) is positive semi-definite if the eigen values are greate than or equal to zero, i.e. \\(\\lambda_i(\\boldsymbol{A}) \\geq 0\\), where \\(\\lambda_i(\\boldsymbol{A})\\) is the \\(i^\\text{th}\\) eigen value of \\(\\boldsymbol{A}\\). We denote positive semi-definite matrices as \\(\\boldsymbol{A}\\ge 0\\). This doesn’t mean element-wise positiveness of the matrix. The space of \\(n\\times n\\) positive semi-definite matrices are denoted by \\(\\mathcal{S}^n_+\\).\nPositive definite matrices are symmetric matrices whose eigen values are strictly greater than zero. i.e. \\(\\lambda_i(\\boldsymbol{A}) &gt; 0\\). We denote positive definite matrices as \\(\\boldsymbol{A}&gt; 0\\). This doesn’t mean element-wise positiveness of the matrix. The space of \\(n\\times n\\) positive definite matrices are denoted by \\(\\mathcal{S}^n_{++}\\).\nFor two matrices \\(\\boldsymbol{X}\\in\\mathcal{S}^n\\) and \\(\\boldsymbol{Y}\\in\\mathcal{S}^n\\), the matrix inequality \\(\\boldsymbol{X}\\geq \\boldsymbol{Y}\\) means \\(\\boldsymbol{Z}:=\\boldsymbol{X}-\\boldsymbol{Y}\\) is postive semi-definite, i.e. \\(\\boldsymbol{Z}\\geq 0\\) or \\(\\lambda_i(\\boldsymbol{Z}) \\geq 0\\).\n\nHere is a Python code generating a positive definite matrix.\n\nimport numpy as np\n\n# Specify the size of the positive definite matrix (e.g., 3x3)\nmatrix_size = 3\n\n# Generate a random symmetric matrix\nrandom_matrix = np.random.rand(matrix_size, matrix_size)\nsymmetric_matrix = (random_matrix + random_matrix.T) / 2\n\n# Ensure the matrix has positive eigenvalues\neigenvalues = np.linalg.eigvals(symmetric_matrix)\nwhile not all(eigenvalues &gt; 0):\n    random_matrix = np.random.rand(matrix_size, matrix_size)\n    symmetric_matrix = (random_matrix + random_matrix.T) / 2\n    eigenvalues = np.linalg.eigvals(symmetric_matrix)\n\n# Print the positive definite matrix\nprint(\"Positive Definite Matrix:\")\nprint(symmetric_matrix)\n\n# Compute the eigenvalues\neigenvalues = np.linalg.eigvals(symmetric_matrix)\n\n# All eigen values should be real and positive\nprint(\"Eigen values:\", eigenvalues)\n\nPositive Definite Matrix:\n[[0.94313008 0.77885996 0.06264793]\n [0.77885996 0.8980718  0.46914673]\n [0.06264793 0.46914673 0.70829851]]\nEigen values: [1.82901826 0.00376417 0.71671795]\n\n\n\n\n2.2.10 Identity Matrix\nAn \\(n\\times n\\) identity matrix is denoted by \\(\\boldsymbol{I}_n\\), and is a matrix with all off-diagonal elements equal to zero, and all diagonal elements equal to one. For example, a \\(3\\times 3\\) identity matrix is defined as \\[\n\\boldsymbol{I}_3 = \\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n\\] For an \\(m\\times n\\) matrix \\(\\boldsymbol{M}\\), the following are true: \\[\n\\boldsymbol{M}\\boldsymbol{I}_n = \\boldsymbol{I}_{m}\\boldsymbol{M} = \\boldsymbol{M}.\n\\]\nHere is a Python code computing the identity matrix.\n\nimport numpy as np\n\n# Specify the size of the identity matrix (e.g., a 3x3 identity matrix)\nmatrix_size = 3\n\n# Compute the identity matrix of the specified size\nidentity_matrix = np.eye(matrix_size)\n\n# Print the identity matrix\nprint(\"Identity Matrix:\")\nprint(identity_matrix)\n\nIdentity Matrix:\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]]\n\n\n\n\n2.2.11 Determininant of a Matrix\nThe determinant of a square matrix \\(\\boldsymbol{A}\\) is denoted by \\(|\\boldsymbol{A}|\\).\nFor a \\(2\\times 2\\) matrix \\[\n\\boldsymbol{A}= \\begin{bmatrix}a&b\\\\c&d\\end{bmatrix},\n\\] the determinant is defined by \\[|\\boldsymbol{A}| = ad-bc.\\]\nFor a \\(3\\times 3\\) matrix, \\[\\boldsymbol{B}= \\begin{bmatrix}a & b & c \\\\ d& e& f\\\\ g & h & i \\end{bmatrix},\\] the determinant is defined by \\[\n|\\boldsymbol{B}| = a\\left|\\begin{matrix}e&f\\\\h&i\\end{matrix}\\right| - b\\left|\\begin{matrix}d&f\\\\g&i\\end{matrix}\\right| + c\\left|\\begin{matrix}d&e\\\\g&h\\end{matrix}\\right| = a(ei-hf) -b(di-gf) + c(dh-ge).\n\\]\nHere is a Python code computing matrix determinant.\n\nimport numpy as np\n\n# Define a square matrix\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Compute the determinant\ndeterminant = np.linalg.det(matrix)\n\n# Print the original matrix and its determinant\nprint(\"Matrix:\")\nprint(matrix)\n\nprint(f\"Determinant: {determinant}\")\n\nMatrix:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nDeterminant: -9.51619735392994e-16\n\n\n\n\n2.2.12 Matrix Inverse\nMatrix inverse is defined only for square matrices and is denoted by \\(\\boldsymbol{M}^{-1}\\). For an \\(n\\times n\\) matrix \\(\\boldsymbol{M}\\), \\(\\boldsymbol{M}^{-1}\\) satisfies the following condition \\[\n\\boldsymbol{M}\\boldsymbol{M}^{-1} = \\boldsymbol{M}^{-1}\\boldsymbol{M} = \\boldsymbol{I}_n.\n\\]\nNote: Matrix inverse exists only when the determinant is non zero.\nFor a \\(2\\times 2\\) matrix \\[\n\\boldsymbol{A}= \\begin{bmatrix}a&b\\\\c&d\\end{bmatrix},\n\\] the inverse is given by \\[\\boldsymbol{A}^{-1} = \\frac{1}{|\\boldsymbol{A}|}\\begin{bmatrix}a&-c\\\\-b&d\\end{bmatrix},\\] assuming \\(|\\boldsymbol{A}|\\neq 0\\).\n\\[\\begin{align*}\n& (\\boldsymbol{A}^{-1})^{-1} = \\boldsymbol{A},\\\\\n&(\\alpha\\boldsymbol{A})^{-1} = \\frac{1}{\\alpha}\\boldsymbol{A}^{-1}, \\text{ for $\\alpha \\neq 0$},\\\\\n&(\\boldsymbol{A}^T)^{-1} = (\\boldsymbol{A}^{-1})^{T},\\\\\n&(\\boldsymbol{A}\\boldsymbol{B})^{-1} = \\boldsymbol{B}^{-1}\\boldsymbol{A}^{-1}, \\text{ assuming $\\boldsymbol{A}$ and $\\boldsymbol{B}$ are invertible};\\\\\n&|\\boldsymbol{A}^{-1}| = \\frac{1}{|\\boldsymbol{A}|}.\n\\end{align*}\\]\nHere is a Python code computing matrix inverse.\n\nimport numpy as np\n\n# Define a matrix\nmatrix = np.array([[1, 2],\n                   [3, 4]])\n\n# Compute the inverse\nmatrix_inverse = np.linalg.inv(matrix)\n\n# Print the original matrix and its inverse\nprint(\"Matrix:\")\nprint(matrix)\n\nprint(\"Inverse:\")\nprint(matrix_inverse)\n\nMatrix:\n[[1 2]\n [3 4]]\nInverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n2.2.13 Moore-Penrose Inverse\nFor a matrix \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\), the Moore-Penrose inverse (or pseudo-inverse) is denoted by \\(\\boldsymbol{A}^+\\) and defined as \\[\n    \\boldsymbol{A}^+ := (\\boldsymbol{A}^T\\boldsymbol{A})^{-1}A^T\n\\]\nSome properties of \\(\\boldsymbol{A}^+\\) include:\n\nFor any matrix \\(\\boldsymbol{A}\\) there is one and only one pseudoinverse \\(\\boldsymbol{A}^+\\).\nIf \\(\\boldsymbol{A}\\) is invertible, then \\(\\boldsymbol{A}^{-1} = \\boldsymbol{A}^+\\)\nFor \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\), left inverse is defined as \\(\\boldsymbol{A}^+ = (\\boldsymbol{A}^T\\boldsymbol{A})^{-1}\\boldsymbol{A}^T\\), and \\(\\boldsymbol{A}^+\\boldsymbol{A}= \\boldsymbol{I}_n\\).\nFor \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\), right inverse is defined as \\(\\boldsymbol{A}^+ = \\boldsymbol{A}^T(\\boldsymbol{A}^T\\boldsymbol{A})^{-1}\\), and \\(\\boldsymbol{A}\\boldsymbol{A}^+= \\boldsymbol{I}_m\\).\n\nHere is a Python code computing Moore-Penrose (or pseudo) inverse\n\nimport numpy as np\n\n# Define a matrix\nmatrix = np.array([[1, 2],\n                   [3, 4]])\n\n# Compute the pseudo-inverse\npseudo_inverse = np.linalg.pinv(matrix)\n\n# Print the original matrix and its pseudo-inverse\nprint(\"Matrix:\")\nprint(matrix)\n\nprint(\"Pseudo-Inverse:\")\nprint(pseudo_inverse)\n\nMatrix:\n[[1 2]\n [3 4]]\nPseudo-Inverse:\n[[-2.   1. ]\n [ 1.5 -0.5]]\n\n\n\n\n2.2.14 Orthonormal Matrices\nAn \\(n\\times n\\) square matrix satisfying the condition \\(\\boldsymbol{A}\\boldsymbol{A}^T = \\boldsymbol{I}_n\\) is orthogonal or orthonormal. Also, if \\(\\boldsymbol{A}\\boldsymbol{A}^T = \\boldsymbol{I}_n\\), then \\(\\boldsymbol{A}^T\\boldsymbol{A}= \\boldsymbol{I}_n\\).\nHere is a Python code checking if a matrix is orthogonal.\n\nimport numpy as np\n\n# Define a square matrix \nmatrix = np.array([[1, 0],\n                   [0, -1]])\n\n# Check if the matrix is square\nif matrix.shape[0] == matrix.shape[1]:\n    # Compute the product of the matrix and its transpose\n    product_matrix_transpose = np.dot(matrix, matrix.T)\n    \n    # Check if the product is close to the identity matrix within a tolerance\n    is_orthonormal = np.allclose(product_matrix_transpose, np.eye(matrix.shape[0]))\n    \n    if is_orthonormal:\n        print(\"The matrix is orthonormal.\")\n    else:\n        print(\"The matrix is not orthonormal.\")\nelse:\n    print(\"The matrix is not square. Orthonormality is not defined.\")\n\nThe matrix is orthonormal.\n\n\n\n\n2.2.15 Kronecker Product\nKronecker product between two matrices \\(\\boldsymbol{A}\\in\\mathcal{R}^{p\\times q}\\) and \\(\\boldsymbol{B}\\in\\mathcal{R}^{m\\times n}\\) results in a new matrix \\(\\boldsymbol{C}\\in\\mathcal{R}^{pm\\times qn}\\) defined by \\[\n\\boldsymbol{C}:= \\boldsymbol{A}\\otimes\\boldsymbol{B}= \\begin{bmatrix}a_{11}\\boldsymbol{B}& a_{12}\\boldsymbol{B}& \\cdots & a_{1n}\\boldsymbol{B}\\\\\na_{21}\\boldsymbol{B}& a_{22}\\boldsymbol{B}& \\cdots & a_{2n}\\boldsymbol{B}\\\\\n\\vdots   & \\vdots& & \\vdots \\\\\na_{m1}\\boldsymbol{B}& a_{m2}\\boldsymbol{B}& \\cdots & a_{mn}\\boldsymbol{B}\n\\end{bmatrix}.\n\\]\nKronecker products have the following properties\n\n\\(\\boldsymbol{A}\\otimes(\\boldsymbol{B}+\\boldsymbol{C}) = \\boldsymbol{A}\\otimes\\boldsymbol{B}+ \\boldsymbol{A}\\otimes\\boldsymbol{C}\\)\n\\((\\boldsymbol{B}+\\boldsymbol{C})\\otimes\\boldsymbol{A}= \\boldsymbol{B}\\otimes\\boldsymbol{A}+ \\boldsymbol{C}\\otimes\\boldsymbol{A}\\)\n\\((\\alpha\\boldsymbol{A})\\otimes\\boldsymbol{B}= \\boldsymbol{A}\\otimes(\\alpha\\boldsymbol{B}) = \\alpha(\\boldsymbol{A}\\otimes\\boldsymbol{B})\\)\n\\((\\boldsymbol{A}\\otimes\\boldsymbol{B})\\otimes\\boldsymbol{C}= \\boldsymbol{A}\\otimes(\\boldsymbol{B}\\otimes\\boldsymbol{C})\\)\n\\(\\boldsymbol{A}\\otimes\\boldsymbol{0} = \\boldsymbol{0}\\otimes\\boldsymbol{A}= \\boldsymbol{0}\\)\n\\((\\boldsymbol{A}\\otimes\\boldsymbol{B})(\\boldsymbol{C}\\otimes\\boldsymbol{D}) = (\\boldsymbol{A}\\boldsymbol{C})\\otimes(\\boldsymbol{B}\\boldsymbol{D})\\)\n\\((\\boldsymbol{A}\\otimes\\boldsymbol{B})^{-1} = \\boldsymbol{A}^{-1}\\otimes\\boldsymbol{B}^{-1}\\)\n\\((\\boldsymbol{A}\\otimes\\boldsymbol{B})^{+} = \\boldsymbol{A}^{+}\\otimes\\boldsymbol{B}^{+}\\)\n\\((\\boldsymbol{A}\\otimes\\boldsymbol{B})^T = \\boldsymbol{A}^T\\otimes\\boldsymbol{B}^T\\)\n\\(\\textbf{det}\\left(\\boldsymbol{A}\\otimes\\boldsymbol{B}\\right) = \\textbf{det}\\left(\\boldsymbol{A}\\right)^m\\textbf{det}\\left(\\boldsymbol{B}\\right)^n\\), for \\(\\boldsymbol{A}\\in\\mathcal{R}^{n\\times n}\\) and \\(\\boldsymbol{B}\\in\\mathcal{R}^{m\\times m}\\).\nFor \\(\\boldsymbol{A}\\boldsymbol{X}\\boldsymbol{B}= \\boldsymbol{C}\\), \\[\\textbf{vec}\\left(\\boldsymbol{C}\\right) = \\textbf{vec}\\left(\\boldsymbol{A}\\boldsymbol{X}\\boldsymbol{B}\\right) = (\\boldsymbol{B}^T\\otimes\\boldsymbol{A})\\textbf{vec}\\left(\\boldsymbol{X}\\right),\\] where \\(\\textbf{vec}\\left(\\cdot\\right)\\) is the vectorization operator that vertically stacks the columns of a matrix.\n\\(\\textbf{rank}\\left(\\boldsymbol{A}\\otimes\\boldsymbol{B}\\right) = \\textbf{rank}\\left(\\boldsymbol{A}\\right)\\textbf{rank}\\left(\\boldsymbol{B}\\right)\\)\n\nHere is a Python code computing Kronecker product.\n\nimport numpy as np\n\n# Define two matrices\nmatrix_A = np.array([[1, 2],\n                     [3, 4]])\n\nmatrix_B = np.array([[0, 5],\n                     [6, 7]])\n\n# Compute the Kronecker product\nkron_product = np.kron(matrix_A, matrix_B)\n\n# Print the original matrices and the Kronecker product\nprint(\"Matrix A:\")\nprint(matrix_A)\n\nprint(\"Matrix B:\")\nprint(matrix_B)\n\nprint(\"Kronecker Product:\")\nprint(kron_product)\n\nMatrix A:\n[[1 2]\n [3 4]]\nMatrix B:\n[[0 5]\n [6 7]]\nKronecker Product:\n[[ 0  5  0 10]\n [ 6  7 12 14]\n [ 0 15  0 20]\n [18 21 24 28]]\n\n\n\n\n2.2.16 Trace of a Square Matrix\nFor \\(\\boldsymbol{A}\\in\\mathcal{R}^{n\\times n}\\), the trace of the matrix is denoted by \\(\\textbf{tr}\\left[\\boldsymbol{A}\\right]\\) and is defined as the sum of the diagonal terms, i.e. \\[\n\\textbf{tr}\\left[\\boldsymbol{A}\\right] = \\sum_i^n a_{ii}.\n\\]\nThe trace operator has the following properties:\n\nIt is a linear operator, i.e. \\[\\textbf{tr}\\left[\\boldsymbol{A}+\\boldsymbol{B}\\right] = \\textbf{tr}\\left[\\boldsymbol{A}\\right] + \\textbf{tr}\\left[\\boldsymbol{B}\\right].\\]\nTrace of a scalar-matrix product \\[\\textbf{tr}\\left[\\alpha \\boldsymbol{A}\\right] = \\alpha\\textbf{tr}\\left[\\boldsymbol{A}\\right].\\]\nTrace of a matrix-matrix product between compatible matrices \\[\\textbf{tr}\\left[\\boldsymbol{A}^T\\boldsymbol{B}\\right] = \\textbf{tr}\\left[A\\boldsymbol{B}^T\\right] = \\textbf{tr}\\left[\\boldsymbol{B}^T\\boldsymbol{A}\\right] = \\textbf{tr}\\left[\\boldsymbol{B}\\boldsymbol{A}^T\\right] = \\sum_{i=1}^m\\sum_{j=1}^n a_{ij}b_{ij}.\\]\nIf \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) are positive semi-definite matrices of the same size, then the following inequality holds \\[\n0 \\leq \\textbf{tr}\\left[\\boldsymbol{A}\\boldsymbol{B}\\right]^2 \\leq \\textbf{tr}\\left[\\boldsymbol{A}^2\\right]\\textbf{tr}\\left[\\boldsymbol{B}^2\\right]\\leq \\textbf{tr}\\left[\\boldsymbol{A}\\right]^2\\textbf{tr}\\left[\\boldsymbol{B}\\right]^2.\n\\]\nTrace of Kronecker Product \\[\\textbf{tr}\\left[\\boldsymbol{A}\\otimes\\boldsymbol{B}\\right] = \\textbf{tr}\\left[\\boldsymbol{A}\\right]\\textbf{tr}\\left[\\boldsymbol{B}\\right].\\]\nTrace and Eigen Values \\[\\textbf{tr}\\left[A\\right] = \\sum_{i=1}^n\\lambda_i(\\boldsymbol{A}).\\]\n\nHere is a Python code computing trace of a matrix.\n\nimport numpy as np\n\n# Define a matrix as a 2D NumPy array\nmatrix = np.array([[1, 2, 3],\n                   [4, 5, 6],\n                   [7, 8, 9]])\n\n# Compute the trace of the matrix\ntrace = np.trace(matrix)\n\n# Print the matrix and its trace\nprint(\"Matrix:\")\nprint(matrix)\nprint(f\"Trace: {trace}\")\n\nMatrix:\n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nTrace: 15\n\n\n\n\n2.2.17 Matrix Inner Product\nThe Frobenius inner product between two matrices is defined as \\[\n\\langle \\boldsymbol{A},\\boldsymbol{B}\\rangle := \\textbf{tr}\\left[\\boldsymbol{A}^T\\boldsymbol{B}\\right].\n\\]\nHere is a Python code checking if two matrices are orthogonal.\n\nimport numpy as np\n\n# Define two square matrices \nmatrix_A = np.array([[1, 0],\n                     [0, -1]])\n\nmatrix_B = np.array([[0, 1],\n                     [-1, 0]])\n\nproduct_AB_transpose = np.dot(matrix_A, matrix_B.T)\n\n# If trace(A'*B) = 0 , A and B are orthogonal. \nprint(\"trace(A'*B) =\",np.trace(np.dot(matrix_A.T, matrix_B)))\n\nif np.allclose(np.trace(np.dot(matrix_A.T, matrix_B)),0):\n  print(\"Matrix A and Matrix B are orthogonal\")\nelse:\n  print(\"Matrix A and Matrix B are not orthogonal\")   \n\ntrace(A'*B) = 0\nMatrix A and Matrix B are orthogonal\n\n\n\n\n2.2.18 Matrix Norms\nFor a matrix \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times n}\\) defined by elements \\(a_{ij}\\),\n\n\\(\\|\\boldsymbol{A}\\|_1 := \\max_{1\\leq j \\leq n} \\sum_{i=1}^m |a_{ij}|\\), which is the maximum absolution column sum of the matrix.\n\\(\\|\\boldsymbol{A}\\|_2 := \\sqrt{\\lambda_\\text{max}(\\boldsymbol{A}^T\\boldsymbol{A})} = \\sigma_\\text{max}(\\boldsymbol{A})\\), where \\(\\sigma_\\text{max}(\\boldsymbol{A})\\) is the largest singular value of \\(\\boldsymbol{A}\\).\n\\(\\|\\boldsymbol{A}\\|_\\infty := \\max_{1\\leq i \\leq m} \\sum_{j=1}^n |a_{ij}|\\), which is the maximum absolution row sum of the matrix.\n\nHere is a Python code computing matrix norms.\n\nimport numpy as np\nfrom numpy.linalg import norm\n\n# Define a matrix\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n\n# Compute different norms\nfrobenius_norm = norm(A, 'fro') # Frobenius norm\nl1_norm = norm(A, 1)           # L1 norm (maximum column sum)\nl2_norm = norm(A, 2)           # L2 norm (largest singular value)\nmax_norm = norm(A, np.inf)     # Max (or infinity) norm (maximum row sum)\n\n# Display the results\nprint(\"Matrix A:\\n\", A)\nprint(\"Frobenius Norm of A:\", frobenius_norm)\nprint(\"L1 Norm of A:\", l1_norm)\nprint(\"L2 Norm of A:\", l2_norm)\nprint(\"Max Norm of A:\", max_norm)\n\nMatrix A:\n [[1 2 3]\n [4 5 6]\n [7 8 9]]\nFrobenius Norm of A: 16.881943016134134\nL1 Norm of A: 18.0\nL2 Norm of A: 16.84810335261421\nMax Norm of A: 24.0\n\n\n\n\n2.2.19 Singular Values of Matrix\nGiven a matrix \\(\\boldsymbol{A}\\) of size \\(m \\times n\\), its singular values are defined as the square roots of the eigenvalues of the matrix \\(\\boldsymbol{A}^T\\boldsymbol{A}\\) (or \\(\\boldsymbol{A}\\boldsymbol{A}^T\\)), i.e. \\[\n\\sigma_i(\\boldsymbol{A}) := \\sqrt{\\lambda_i(\\boldsymbol{A}^T\\boldsymbol{A})}.\n\\]\nSingular values provide insight into the geometric properties of the matrix, such as its rank, range, and null space. They are always non-negative real numbers and are usually presented in descending order.\nThe singular values tell us about the ‘stretching’ effect of the matrix transformation in various directions. In the context of SVD, a matrix \\(\\boldsymbol{A}\\) can be decomposed as \\(\\boldsymbol{A}= \\boldsymbol{U}\\boldsymbol{\\Sigma} \\boldsymbol{V}^T\\), where \\(\\boldsymbol{U}\\) and \\(\\boldsymbol{V}\\) are orthogonal matrices, and \\(\\boldsymbol{\\Sigma}\\) is a diagonal matrix whose diagonal entries are the singular values of \\(\\boldsymbol{A}\\).\nHere’s a Python script using NumPy to compute the singular values of a given matrix:\n\nimport numpy as np\n\n# Define a matrix\nA = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Compute the singular values\nU, singular_values, Vt = np.linalg.svd(A)\n\n# Display the singular values\nprint(\"Singular Values of A:\\n\", singular_values)\n\nSingular Values of A:\n [9.52551809 0.51430058]\n\n\nSince \\(\\boldsymbol{A}^T\\boldsymbol{A}\\) (or \\(\\boldsymbol{A}\\boldsymbol{A}^T\\)) is positive semi-definite, its eigen values will be real and non zero. The largest and smallest singular values of a matrix hold significant information about the properties and behavior of the matrix in various mathematical and practical contexts. Understanding their importance is crucial in linear algebra, data analysis, and machine learning.\nThe largest singular value has several important implications, such as:\n\nIndicates Maximum Stretch: The largest singular value of a matrix represents the maximum “stretching” factor of the matrix. It gives the largest factor by which the matrix can stretch a vector during the transformation.\nNorm of the Matrix: The largest singular value is equal to the 2-norm (or spectral norm) of the matrix. This norm represents the maximum length to which the matrix can stretch a unit vector.\nStability and Conditioning: In numerical linear algebra, the largest singular value is used to assess the conditioning of the matrix. A very large singular value, especially when compared to the smallest singular value, can indicate that the matrix is ill-conditioned, leading to numerical instability in solutions of linear systems involving the matrix.\n\nThe smallest singular value also has several important implications, such as:\n\nIndicates Minimum Stretch and Rank: The smallest singular value provides insight into how much the matrix compresses vectors in the least stretchable direction. If the smallest singular value is zero, it indicates that the matrix is rank-deficient (not full rank), meaning there are linearly dependent columns or rows.\nMeasure of Invertibility: For a square matrix, if the smallest singular value is significantly greater than zero, the matrix is well-conditioned and invertible. A very small (especially zero) singular value in a square matrix implies that the matrix is near singular or singular, and thus not invertible.\nPseudo-Inverse and Regularization: In machine learning and statistics, the smallest singular value plays a critical role in regularization and the computation of the Moore-Penrose pseudo-inverse. When dealing with ill-posed problems or in the presence of collinearity, small singular values are regularized to stabilize the solution.\nData Analysis and Principal Component Analysis (PCA): In PCA, small singular values (and their corresponding singular vectors) often correspond to noise or less significant components of the data. Eliminating these components (dimensionality reduction) can help in focusing on the more significant patterns represented by larger singular values.\n\nIn summary, the largest singular value reflects the maximum stretching ability and overall norm of a matrix, while the smallest singular value indicates its invertibility, conditioning, and the presence of redundant or less significant components. In practical applications, these singular values are crucial for understanding the stability, efficiency, and effectiveness of numerical methods and data analysis techniques.\n\n\n2.2.20 Matrix Decompositions\n\n2.2.20.1 LU Decomposition\nLU decomposition is a method of decomposing a square matrix \\(\\boldsymbol{A}\\) into the product of a lower triangular matrix \\(\\boldsymbol{L}\\) and an upper triangular matrix \\(\\boldsymbol{U}\\). In mathematical terms, if \\(\\boldsymbol{A}\\) is a square matrix, then the LU decomposition is given by \\(\\boldsymbol{A}= \\boldsymbol{LU}\\), where\n\n\\(\\boldsymbol{L}\\) is a lower triangular matrix (all entries above the main diagonal are zero),\n\\(\\boldsymbol{U}\\) is an upper triangular matrix (all entries below the main diagonal are zero).\n\nLU decomposition is used in various applications such as:\n\nSolving Linear Systems: It is used to solve systems of linear equations. Once a matrix is decomposed into \\(\\boldsymbol{L}\\) and \\(\\boldsymbol{U}\\), it is easier to solve the equation \\(\\boldsymbol{Ax = b}\\) by first solving \\(\\boldsymbol{Ly = b}\\) for \\(\\boldsymbol{y}\\) and then solving \\(\\boldsymbol{Ux = y}\\) for \\(\\boldsymbol{x}\\).\nMatrix Inversion: LU decomposition can be used to find the inverse of a matrix, which is significantly faster than direct computation, especially for large matrices.\nDeterminant Calculation: The determinant of \\(\\boldsymbol{A}\\) can be easily calculated as the product of the diagonals of \\(\\boldsymbol{L}\\) and \\(\\boldsymbol{U}\\), as the determinant of a triangular matrix is the product of its diagonal entries.\n\nHere is a Python code computing LU Decomposition of a matrix.\n\nimport numpy as np\nfrom scipy.linalg import lu\n\n# Define a square matrix\nA = np.array([[4, 3], [6, 3]])\n\n# Perform LU Decomposition\nP, L, U = lu(A)\n\n# Display the results\nprint(\"Original Matrix:\\n\", A)\nprint(\"Permutation Matrix (P):\\n\", P)\nprint(\"Lower Triangular Matrix (L):\\n\", L)\nprint(\"Upper Triangular Matrix (U):\\n\", U)\n\n# Verify the decomposition\nprint(\"Verification (P * L * U):\\n\", np.dot(P, np.dot(L, U)))\n\nOriginal Matrix:\n [[4 3]\n [6 3]]\nPermutation Matrix (P):\n [[0. 1.]\n [1. 0.]]\nLower Triangular Matrix (L):\n [[1.         0.        ]\n [0.66666667 1.        ]]\nUpper Triangular Matrix (U):\n [[6. 3.]\n [0. 1.]]\nVerification (P * L * U):\n [[4. 3.]\n [6. 3.]]\n\n\n\n\n2.2.20.2 QR Factorization of a Matrix\nQR factorization is a method for decomposing a matrix into two components: an orthogonal matrix \\(\\boldsymbol{Q}\\) and an upper triangular matrix \\(\\boldsymbol{R}\\). For a given matrix \\(\\boldsymbol{A}\\), the QR factorization is expressed as \\(\\boldsymbol{A}= \\boldsymbol{Q}\\boldsymbol{R}\\), where:\n\n\\(\\boldsymbol{Q}\\) is an orthogonal matrix, i.e., \\(\\boldsymbol{Q}^T \\boldsymbol{Q} = \\boldsymbol{Q} \\boldsymbol{Q}^T = \\boldsymbol{I}\\), where \\(\\boldsymbol{I}\\) is the identity matrix\n\\(\\boldsymbol{R}\\) is an upper triangular matrix.\n\nQR factorization is utilized in various mathematical and computational applications, such as:\n\nSolving Linear Systems: Similar to LU decomposition, QR factorization can be used to solve linear systems \\(\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b}\\). The system is solved by first computing \\(\\boldsymbol{Q}^T \\boldsymbol{b}\\) and then solving the upper triangular system \\(\\boldsymbol{R}\\boldsymbol{x} = \\boldsymbol{Q}^T \\boldsymbol{b}\\).\nEigenvalue Computation: QR factorization is a key component in algorithms for computing eigenvalues of a matrix, particularly in the QR algorithm for eigenvalue computation.\nLeast Squares Fitting: In statistical analysis and data fitting, QR factorization is often used to solve least squares problems efficiently, especially when \\(\\boldsymbol{A}\\) is not square or is ill-conditioned.\n\nHere is a Python code computing QR Decomposition of a matrix.\n\nimport numpy as np\nfrom scipy.linalg import qr\n\n# Define a matrix\nA = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])\n\n# Perform QR Decomposition\nQ, R = qr(A)\n\n# Display the results\nprint(\"Original Matrix:\\n\", A)\nprint(\"Orthogonal Matrix (Q):\\n\", Q)\nprint(\"Upper Triangular Matrix (R):\\n\", R)\n\n# Verify the decomposition\nprint(\"Verification (Q * R):\\n\", np.dot(Q, R))\n\nOriginal Matrix:\n [[ 12 -51   4]\n [  6 167 -68]\n [ -4  24 -41]]\nOrthogonal Matrix (Q):\n [[-0.85714286  0.39428571  0.33142857]\n [-0.42857143 -0.90285714 -0.03428571]\n [ 0.28571429 -0.17142857  0.94285714]]\nUpper Triangular Matrix (R):\n [[ -14.  -21.   14.]\n [   0. -175.   70.]\n [   0.    0.  -35.]]\nVerification (Q * R):\n [[ 12. -51.   4.]\n [  6. 167. -68.]\n [ -4.  24. -41.]]\n\n\n\n\n2.2.20.3 Cholesky Factorization of a Matrix\nCholesky factorization is a method for decomposing a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose. For a given matrix \\(\\boldsymbol{A}\\), the Cholesky factorization is expressed as: \\(\\boldsymbol{A}= \\boldsymbol{L}\\boldsymbol{L}^T\\), where \\(\\boldsymbol{L}\\) is a lower triangular matrix.\nCholesky factorization is used in various applications, including:\n\nSolving Linear Systems: Cholesky factorization is particularly efficient for solving linear systems of the form \\(\\boldsymbol{A}\\boldsymbol{x} = \\boldsymbol{b}\\) when \\(\\boldsymbol{A}\\) is Hermitian and positive-definite. The system can be solved by first solving \\(\\boldsymbol{L}\\boldsymbol{y} = \\boldsymbol{b}\\) for \\(\\boldsymbol{y}\\), and then solving \\(\\boldsymbol{L}^T \\boldsymbol{x} = \\boldsymbol{y}\\).\nComputing Matrix Inverses: For Hermitian, positive-definite matrices, Cholesky factorization can be used to compute the inverse of \\(\\boldsymbol{A}\\) efficiently.\nMonte Carlo Simulations: In financial and engineering simulations, Cholesky factorization is used to generate random samples from multivariate normal distributions.\nOptimization Problems: In optimization, especially quadratic programming, Cholesky factorization is used to solve linear equations and invert matrices efficiently, which is often a computational bottleneck.\n\nHere is a Python code computing Cholesky Decomposition of a matrix.\n\nimport numpy as np\nfrom scipy.linalg import cholesky\n\n# Define a positive definite matrix\nA = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])\n\n# Perform Cholesky Decomposition\nL = cholesky(A, lower=True)\n\n# Display the results\nprint(\"Original Matrix:\\n\", A)\nprint(\"Lower Triangular Matrix (L):\\n\", L)\n\n# Verify the decomposition\nprint(\"Verification (L * \\set{L}_T):\\n\", np.dot(L, L.T))\n\nOriginal Matrix:\n [[  4  12 -16]\n [ 12  37 -43]\n [-16 -43  98]]\nLower Triangular Matrix (L):\n [[ 2.  0.  0.]\n [ 6.  1.  0.]\n [-8.  5.  3.]]\nVerification (L * \\set{L}_T):\n [[  4.  12. -16.]\n [ 12.  37. -43.]\n [-16. -43.  98.]]\n\n\n\n\n2.2.20.4 Matrix Square Root\nThe matrix square root of a matrix \\(\\boldsymbol{A}\\) is a matrix \\(\\boldsymbol{B}\\) such that \\(\\boldsymbol{B}\\boldsymbol{B} = \\boldsymbol{A}\\). This means that when \\(\\boldsymbol{B}\\) is multiplied by itself, the result is the original matrix \\(\\boldsymbol{A}\\). The matrix square root is particularly relevant for positive definite matrices.\nMatrix square roots are used in various applications, including:\n\nControl Theory: In control theory, the matrix square root is used in the analysis and design of control systems, particularly in the context of state transition matrices and system stability.\nQuantum Mechanics: In quantum mechanics, the square root of a density matrix (a matrix representing a quantum state) is often calculated as part of quantum state tomography and other analyses.\nComputer Graphics: In computer graphics, matrix square roots are used in algorithms for transformations and in the manipulation of geometric shapes.\nStatistics and Data Analysis: In statistics, the matrix square root (particularly the square root of a covariance matrix) is used in multivariate analysis, including principal component analysis (PCA) and other forms of data reduction.\n\nThese applications leverage the matrix square root for its ability to simplify and solve complex mathematical problems, particularly in systems analysis and probabilistic modeling.\nTo compute the square root of a matrix in Python, you can use the scipy.linalg library, which provides a function for this purpose. Below is a Python code using SciPy to compute the square root of a given matrix.\nHere is a Python code computng matrix square root.\n\nimport numpy as np\nfrom scipy.linalg import sqrtm\n\n# Define your matrix A (it should be a positive definite matrix)\nA = np.array([[4, 2], [2, 3]])\n\n# Compute the square root of the matrix\nsqrt_A = sqrtm(A)\n\n# Verify the result\nprint(\"Square Root of A:\\n\", sqrt_A)\nprint(\"Verification (Square Root of A multiplied by itself):\\n\", np.dot(sqrt_A, sqrt_A))\n\nSquare Root of A:\n [[1.91936596 0.56216928]\n [0.56216928 1.63828133]]\nVerification (Square Root of A multiplied by itself):\n [[4. 2.]\n [2. 3.]]\n\n\n\n\n2.2.20.5 Singular Value Decomposition\nSingular Value Decomposition (SVD) is a key matrix decomposition technique in linear algebra, used in many scientific and engineering fields. It decomposes any \\(m\\times n\\) matrix \\(\\boldsymbol{A}\\) into three matrices \\(\\boldsymbol{A} = \\boldsymbol{U} \\boldsymbol{\\Sigma} \\boldsymbol{V}^T\\), where\n\n\\(\\boldsymbol{U}\\): An \\(m\\times m\\) unitary matrix. The columns of \\(\\boldsymbol{U}\\), known as the left singular vectors, form an orthonormal basis for the range of \\(\\boldsymbol{A}\\).\n\\(\\boldsymbol{\\Sigma}\\): An \\(m \\times n\\) rectangular diagonal matrix with non-negative real numbers on the diagonal. The diagonal entries, the singular values of \\(\\boldsymbol{A}\\), are typically arranged in descending order. They represent the lengths of the semi-axes of the ellipsoid described by \\(\\boldsymbol{A}\\). The number of non-zero singular values equals the rank of \\(\\boldsymbol{A}\\).\n\\(\\boldsymbol{V}^T\\): An \\(n\\times n\\) unitary matrix. The columns of \\(\\boldsymbol{V}\\), the right singular vectors, form an orthonormal basis for the domain of \\(\\boldsymbol{A}\\).\n\nKey Properties and Uses of SVD are the following:\n\nSVD simplifies the analysis of a matrix by breaking it down into simpler constituent parts.\nIt is instrumental in solving overdetermined or underdetermined linear systems, via computing the pseudo-inverse of a matrix.\nIn data science and machine learning, SVD is crucial for algorithms like Principal Component Analysis (PCA), used in dimensionality reduction, data compression, and noise reduction.\nIt plays a significant role in signal processing and statistics for pattern identification and compact representations.\nSVD is also used in solving linear least squares problems, common in mathematical modeling and scientific computing.\n\nSVD’s versatility and robustness make it an essential tool in computational and applied mathematics.\nHere is a Python code computing SVD of a matrix.\n\nimport numpy as np\n\n# Define a matrix\nA = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Perform Singular Value Decomposition\nU, S, Vt = np.linalg.svd(A)\n\n# Display the results\nprint(\"Original Matrix:\\n\", A)\nprint(\"Left Singular Vectors (U):\\n\", U)\nprint(\"Singular Values (S):\\n\", S)\nprint(\"Right Singular Vectors Transposed (Vt):\\n\", Vt)\n\n# Reconstruct the original matrix\nSigma = np.zeros((A.shape[0], A.shape[1]))\nSigma[:A.shape[1], :A.shape[1]] = np.diag(S)\nA_reconstructed = U @ Sigma @ Vt\n\nprint(\"Reconstructed Matrix (U * Sigma * Vt):\\n\", A_reconstructed)\n\nOriginal Matrix:\n [[1 2]\n [3 4]\n [5 6]]\nLeft Singular Vectors (U):\n [[-0.2298477   0.88346102  0.40824829]\n [-0.52474482  0.24078249 -0.81649658]\n [-0.81964194 -0.40189603  0.40824829]]\nSingular Values (S):\n [9.52551809 0.51430058]\nRight Singular Vectors Transposed (Vt):\n [[-0.61962948 -0.78489445]\n [-0.78489445  0.61962948]]\nReconstructed Matrix (U * Sigma * Vt):\n [[1. 2.]\n [3. 4.]\n [5. 6.]]\n\n\n\n\n2.2.20.6 Schur Complement\nThe Schur complement is a useful concept in linear algebra for analyzing and simplifying block matrices. Given a block matrix: \\[ \\boldsymbol{M} = \\begin{bmatrix} \\boldsymbol{A} & \\boldsymbol{B} \\\\ \\boldsymbol{C} & \\boldsymbol{D} \\end{bmatrix}, \\]\nwhere \\(\\boldsymbol{A}\\), \\(\\boldsymbol{B}\\), \\(\\boldsymbol{C}\\), and \\(\\boldsymbol{D}\\) are submatrices with \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{D}\\) being square, and \\(\\boldsymbol{A}\\) is invertible, the Schur complement of \\(\\boldsymbol{A}\\) in \\(\\boldsymbol{M}\\) is defined as: \\[ \\boldsymbol{S} = \\boldsymbol{D} - \\boldsymbol{C} \\boldsymbol{A}^{-1} \\boldsymbol{B}.\\]\nApplications of the Schur Complement include\n\nMatrix Inversion: Simplifies the inversion of block matrices.\nDeterminant Calculation: Allows expressing the determinant of \\(\\boldsymbol{M}\\) in terms of the determinant of \\(\\boldsymbol{A}\\) and its Schur complement.\nSolving Linear Systems: Used in solving partitioned systems of linear equations.\nControl Theory: Applied in stability analysis and controller design.\n\nHere is a Python code computing Schur Complement.\n\nimport numpy as np\n\n# Define the block matrix components\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[5, 6], [7, 8]])\nC = np.array([[9, 10], [11, 12]])\nD = np.array([[13, 14], [15, 16]])\n\n# Ensure A is invertible\nif np.linalg.det(A) == 0:\n    raise ValueError(\"Matrix A is not invertible.\")\n\n# Calculate the Schur complement of A in M\n# Schur complement S = D - C * A^-1 * B\nA_inv = np.linalg.inv(A)\nS = D - np.dot(C, np.dot(A_inv, B))\n\nprint(\"Schur Complement of A in M:\\n\", S)\n\nSchur Complement of A in M:\n [[ 0.00000000e+00 -5.32907052e-15]\n [ 0.00000000e+00 -1.06581410e-14]]\n\n\n\n\n2.2.20.7 Schur Decomposition\nSchur Decomposition is a key technique in linear algebra for decomposing square matrices. It represents any square matrix \\(\\boldsymbol{A}\\) as a product of three matrices \\[ \\boldsymbol{A} = \\boldsymbol{Q}\\boldsymbol{U}\\boldsymbol{Q}^T\\]\nWhere:\n\n\\(\\boldsymbol{Q}\\) is a unitary matrix, satisfying \\(\\boldsymbol{Q}^T\\boldsymbol{Q} = \\boldsymbol{I}\\).\n\\(\\boldsymbol{U}\\) is an upper triangular matrix.\n\\(\\boldsymbol{Q}^T\\) is the transpose of \\(\\boldsymbol{Q}\\).\n\nSchur Decomposition is used in various applications in numerical linear algebra, including eigenvalue computations and matrix exponentials.\nHere is a Python code computing Schur Decomposition of a matrix.\n\nimport numpy as np\nfrom scipy.linalg import schur\n\n# Define a square matrix A\nA = np.array([[4, -2, 1],\n              [7,  0, 5],\n              [6,  3, 2]])\n\n# Perform Schur decomposition\nT, U = schur(A)\n\n# Print the original matrix and the Schur decomposition matrices\nprint(\"Matrix A:\")\nprint(A)\n\nprint(\"Schur Decomposition - T:\")\nprint(T)\n\nprint(\"Schur Decomposition - U:\")\nprint(U)\n\nMatrix A:\n[[ 4 -2  1]\n [ 7  0  5]\n [ 6  3  2]]\nSchur Decomposition - T:\n[[ 4.39316337  9.00965681 -2.70594944]\n [-0.68611856  4.39316337 -2.9444486 ]\n [ 0.          0.         -2.78632674]]\nSchur Decomposition - U:\n[[ 0.07068893 -0.99321124  0.09238243]\n [-0.70664211 -0.11522857 -0.69812557]\n [-0.70403125  0.01593157  0.70999027]]\n\n\n\n\n2.2.20.8 Matrix Exponential\nThe matrix exponential is a significant concept in linear algebra, extending the exponential function to square matrices. For any square matrix \\(\\boldsymbol{A}\\), the matrix exponential is defined by the series: \\[ e^{\\mathbf{A}} = \\sum_{k=0}^{\\infty} \\frac{1}{k!} \\mathbf{A}^k.\\]\nNote: This is not element-wise exponential.\nMatrix exponentials are crucial in solving linear differential equations, control theory, and quantum mechanics, among other applications.\nHere is a Python code showing the computation of a matrix exponential.\n\nimport numpy as np\nfrom scipy.linalg import expm\n\n# Define a square matrix\nA = np.array([[0, -1], [1, 0]])\n\n# Compute the matrix exponential of A\nexp_A = expm(A)\n\n# Display the result\nprint(\"Matrix A:\\n\", A)\nprint(\"Matrix Exponential of A (exp(A)):\\n\", exp_A)\n\nMatrix A:\n [[ 0 -1]\n [ 1  0]]\nMatrix Exponential of A (exp(A)):\n [[ 0.54030231 -0.84147098]\n [ 0.84147098  0.54030231]]"
  },
  {
    "objectID": "functions.html#functions",
    "href": "functions.html#functions",
    "title": "3  Functions & Function Spaces",
    "section": "3.1 Functions",
    "text": "3.1 Functions\nIn this book, we will consider functions to be mappings from a set numbers to another set of numbers. In general it is represented as \\(\\boldsymbol{f}(\\boldsymbol{x}):\\mathcal{R}^n \\mapsto \\mathcal{R}^m\\), which states that \\(\\boldsymbol{f}\\) maps an object \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) to objects in \\(\\mathcal{R}^m\\). That is, \\[\\boldsymbol{y}:= \\boldsymbol{f}(\\boldsymbol{x}) = \\begin{bmatrix} f_1(\\boldsymbol{x}) \\\\ \\vdots \\\\ f_m(\\boldsymbol{x}) \\end{bmatrix},\\] or componentwise \\[y_i := f_i(\\boldsymbol{x}),\\] where \\(f_i(\\boldsymbol{x}):\\mathcal{R}^n \\mapsto \\mathcal{R}\\).\nWe can define functions between sets of complex numbers as well.\n\n3.1.1 Derivatives\n\n3.1.1.1 Gradient\nGiven scalar function with vector arguments, i.e. \\(f(\\boldsymbol{x}): \\mathcal{R}^n \\mapsto \\mathcal{R}\\), the gradient of \\(f(\\boldsymbol{x})\\) is denoted by \\(\\boldsymbol{\\nabla}f(\\boldsymbol{x})\\) and defined as \\[\n\\boldsymbol{\\nabla}f(\\boldsymbol{x}) := \\begin{bmatrix}\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f(\\boldsymbol{x})}{\\partial x_n}\\end{bmatrix}, \\text{ where  } \\boldsymbol{x}:=\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}.\n\\] Therefore, the gradient of a scalar function with vector inputs, is a vector.\n\n\n3.1.1.2 Jacobian\nGiven vector function with vector arguments, i.e. \\(\\boldsymbol{f}(\\boldsymbol{x}): \\mathcal{R}^n \\mapsto \\mathcal{R}^m\\), the jacobian of \\(\\boldsymbol{f}(\\boldsymbol{x})\\) is denoted by \\(\\boldsymbol{\\nabla}\\boldsymbol{f}(\\boldsymbol{x})\\) and defined as \\[\n\\boldsymbol{J} = \\boldsymbol{\\nabla}\\boldsymbol{f}(\\boldsymbol{x}) := \\begin{bmatrix}\n\\frac{\\partial f_1(\\boldsymbol{x})}{\\partial x_1} & \\cdots & \\frac{\\partial f_1(\\boldsymbol{x})}{\\partial x_n}\\\\\n\\vdots & & \\vdots\\\\\n\\frac{\\partial f_m(\\boldsymbol{x})}{\\partial x_1} & \\cdots & \\frac{\\partial f_m(\\boldsymbol{x})}{\\partial x_n}\\\\\n\\end{bmatrix},\n\\] where \\[\n\\boldsymbol{f}(\\boldsymbol{x}) :=\\begin{bmatrix} f_1(\\boldsymbol{x}) \\\\ \\vdots \\\\ f_m(\\boldsymbol{x})\\end{bmatrix} \\text{ and } \\boldsymbol{x}:=\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n\\end{bmatrix}.\n\\] Therefore, the jacobian of a vector function with vector inputs, is a matrix.\n\n\n3.1.1.3 Hessian\nHessian of a scalar function with vector arguments is a symmetric matrix. It is (ambiguously) denoted by \\(\\boldsymbol{\\nabla}^2 f(\\boldsymbol{x})\\) or the letter \\(\\boldsymbol{H}\\) and defined by, \\[\n\\boldsymbol{H} = \\boldsymbol{\\nabla}^2 f(\\boldsymbol{x}) := \\boldsymbol{\\nabla}(\\boldsymbol{\\nabla}f(\\boldsymbol{x})) = \\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2} & \\cdots &\\frac{\\partial^2 f}{\\partial x_1\\partial x_n}\\\\\n\\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots &\\frac{\\partial^2 f}{\\partial x_2\\partial x_n}\\\\\n\\vdots & \\vdots & & \\vdots\\\\\n\\frac{\\partial^2 f}{\\partial x_n\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n^2} & \\cdots &\\frac{\\partial^2 f}{\\partial x_n^2}\\\\\n\\end{bmatrix}.\n\\]\n\n\n3.1.1.4 Divergence of a Vector Function\nThe divergence of a vector function with vector arguments \\(\\boldsymbol{f}(\\boldsymbol{x}): \\mathcal{R}^n \\mapsto \\mathcal{R}^n\\) is a scalar. It is represented as \\(\\boldsymbol{\\nabla}\\cdot \\boldsymbol{f}(\\boldsymbol{x})\\) and defined as \\[\n\\boldsymbol{\\nabla}\\cdot\\boldsymbol{f}(\\boldsymbol{x}) := \\sum_{i=1}^n \\frac{\\partial f_i(\\boldsymbol{x})}{\\partial x_i}.\n\\]\n\n\n\n3.1.2 Automatic Differentiation\nAutomatic Differentiation (AD), also known as algorithmic differentiation or autodiff, is a powerful computational technique employed in various scientific, engineering, and mathematical fields. AD is different from symbolic differentiation (e.g., symbolic algebra) and numerical differentiation (e.g., finite differences) in that it provides exact gradients efficiently, even for complex and high-dimensional functions. We can efficiently use AD to compute gradients, Jacobians, Hessians, and divergences of various functions, even if these functions are realized as computer code.\nAt its core, AD is based on the principles of the chain rule from calculus. It takes complex functions and decomposes them into a sequence of elementary operations. These operations are then evaluated not only for their values but also for their derivatives with respect to the input variables. AD computes gradients efficiently by combining these derivatives through the chain rule, making it a valuable tool for numerical optimization, sensitivity analysis, and machine learning.\nElementary Operations\nIn AD, elementary operations such as addition, subtraction, multiplication, division, and trigonometric functions are integral components of the process. Specialized functions are designed to evaluate these operations, and they ensure that both the function’s value and its derivative are computed in a single pass.\nForward Mode vs. Reverse Mode\nAD can be executed in two primary modes: forward mode and reverse mode. Forward mode AD calculates derivatives one input variable at a time while keeping others constant. It is efficient when the number of input variables is limited. In contrast, reverse mode AD computes derivatives for all input variables simultaneously, making it more suitable for functions with numerous input variables.\nComputational Graph\nAD often employs a computational graph to represent the sequence of elementary operations. The graph consists of nodes representing variables and operations and edges denoting dependencies. During graph evaluation, both function values and derivatives are computed, enabling a systematic and precise differentiation process.\nAutomatic vs. Manual Differentiation\nWhile AD can be manually implemented, automatic implementations using specialized software tools and libraries offer greater flexibility and scalability. These automatic implementations can handle complex functions without the need for manual intervention, saving time and effort in computing gradients.\nAdvantages of Automatic Differentiation\nThe precision and efficiency of AD make it an invaluable tool for gradient-based optimization, sensitivity analysis, and scientific computing. AD provides exact gradients with minimal numerical errors, avoiding the numerical instability issues often associated with finite differences.\nTo perform automatic differentiation in Python, we can use specialized libraries like NumPy and autograd. Here’s a Python code example using the autograd library to demonstrate automatic differentiation:\n\nimport autograd.numpy as np\nfrom autograd import grad\n\n# Define a function for which you want to compute the derivative\ndef my_function(x):\n    return x**3 + 2*x**2 + 1\n\n# Use autograd to compute the derivative of the function\nderivative_func = grad(my_function)\n\n# Evaluate the derivative at a specific point\nx_value = 2.0\nderivative_at_x = derivative_func(x_value)\n\n# Print the result\nprint(f\"The derivative of the function at x = {x_value} is {derivative_at_x}\")\n\nThe derivative of the function at x = 2.0 is 20.0\n\n\nMachine learning frameworks such as TensorFlow, JAX, and PyTorch provide very efficient AD libraries that can leverage GPUs and TPUs to perform differentiations for very complex functions.\nDual Numbers and Their Application in Automatic Differentiation Dual numbers are a mathematical construct that extends real numbers to enable the automatic computation of derivatives. They are a fundamental tool in the field of automatic differentiation (AD), a technique widely used in scientific computing, optimization, and machine learning.\nA dual number is represented as \\(a + b\\epsilon\\), where \\(a\\) and \\(b\\) are real numbers, and \\(\\epsilon\\) is a symbol with the property \\(\\epsilon^2 = 0\\). This property is crucial, as it allows us to calculate derivatives efficiently.\nDual numbers follow specific arithmetic rules:\n\nAddition: \\((a + b\\epsilon) + (c + d\\epsilon) = (a + c) + (b + d)\\epsilon\\)\nSubtraction: \\((a + b\\epsilon) - (c + d\\epsilon) = (a - c) + (b - d)\\epsilon\\)\nMultiplication: \\((a + b\\epsilon) (c + d\\epsilon) = (ac) + (ad + bc)\\epsilon\\)\nDivision: \\((a + b\\epsilon) / (c + d\\epsilon) = (a / c) + ((bc - ad) / c^2)\\epsilon\\)\n\nDual numbers play a pivotal role in AD, allowing for the precise and efficient computation of these derivatives. The fundamental idea is to replace an input variable \\(x\\) in a function \\(f(x)\\) with \\(x + \\epsilon\\). By evaluating \\(f(x + \\epsilon)\\), we obtain a dual number representing both the function’s value and its derivative at the point \\(x\\). The coefficient of \\(\\epsilon\\) in this dual number is the exact derivative of \\(f(x)\\).\nDual numbers are primarily used in the forward mode of AD. In this mode, derivatives are computed incrementally in a “forward pass” from input variables to the output of a function. This allows for the efficient calculation of gradients, making it well-suited for problems with many input variables.\nThe following Python code implements a dual number class and uses it to compute derivatives of functions.\n\nclass DualNumber:\n    def __init__(self, real, dual):\n        self.real = real  # Real part\n        self.dual = dual  # Dual part\n\n    def __add__(self, other):\n        return DualNumber(self.real + other.real, \n            self.dual + other.dual)\n\n    def __mul__(self, other):\n        return DualNumber(self.real * other.real, \n             self.real * other.dual + self.dual * other.real)\n\n    def __str__(self):\n        return f\"{self.real} + {self.dual}\"\n\n# Function to differentiate\ndef f(x):\n    # Example function: f(x) = x^2\n    return x * x\n\n# Compute derivative at x = 3\nx = 3\ndx = DualNumber(x, 1)\ny = f(dx)\n\nprint(f\"Value of f(x) at x = {x}: {y.real}\")\nprint(f\"Derivative of f(x) at x = {x}: {y.dual}\")\n\nValue of f(x) at x = 3: 9\nDerivative of f(x) at x = 3: 6"
  },
  {
    "objectID": "functions.html#function-spaces",
    "href": "functions.html#function-spaces",
    "title": "3  Functions & Function Spaces",
    "section": "3.2 Function Spaces",
    "text": "3.2 Function Spaces\nIn machine learning, function spaces are mathematical constructs that describe sets of functions with common properties. These spaces are crucial for understanding the types of functions that can be learned by models and for analyzing the behavior of algorithms.\n\n3.2.1 Vector Spaces\nVector spaces are a fundamental concept in linear algebra and are integral to many aspects of machine learning. Understanding vector spaces provides a foundation for grasping more complex machine learning algorithms and concepts.\nA vector space is a collection of objects known as vectors, which can be added together and multiplied by numbers (scalars). Mathematically, a vector space must satisfy a set of axioms related to vector addition and scalar multiplication. In machine learning, vectors often represent data points, and the operations on these vectors follow the rules of vector spaces.\nLet \\(\\mathcal{V}\\) be a vector spaces (often \\(\\mathcal{V} \\subset \\mathcal{R}^n\\)), the for elements in \\(\\mathcal{V}\\), the following properties hold:\n\nClosure: The sum of any two vectors in the space is also in the space, i.e., \\[\\forall \\boldsymbol{x},\\boldsymbol{y}\\in\\mathcal{V},\\; \\boldsymbol{x}+\\boldsymbol{y}\\in\\mathcal{V}.\\]\nAssociativity of Addition: The order of addition does not change the result, i.e., \\[\\boldsymbol{x}+ \\boldsymbol{y}= \\boldsymbol{y}+ \\boldsymbol{x}.\\]\nExistence of Additive Identity: There is a vector (the zero vector \\(\\boldsymbol{0}\\in\\mathcal{V}\\)) that, when added to any vector, leaves the vector unchanged, i.e., \\[\\boldsymbol{x}+ \\boldsymbol{0} = \\boldsymbol{x}.\\]\nExistence of Additive Inverse: For every vector \\(\\boldsymbol{x}\\in\\mathcal{V}\\), there is another vector \\(-\\boldsymbol{x}\\in\\mathcal{V}\\) that, when added together, results in the zero vector, i.e., \\[ \\boldsymbol{x}+ (-\\boldsymbol{x}) = \\boldsymbol{0}.\\]\nDistributivity and Scalar Multiplication: Scalars can multiply vectors, and this operation is distributive over vector addition and scalar addition, i.e., for \\(\\alpha, \\beta \\in\\mathcal{R}\\) \\[\\begin{align*}\n& \\alpha\\boldsymbol{x}\\in \\mathcal{V}, \\\\\n& \\alpha(\\boldsymbol{x}+\\boldsymbol{y}) = \\alpha\\boldsymbol{x}+ \\alpha\\boldsymbol{y},\\\\\n& (\\alpha+\\beta)\\boldsymbol{x}= \\boldsymbol{x}(\\alpha + \\beta) = \\alpha\\boldsymbol{x}+ \\beta\\boldsymbol{x}.\n\\end{align*}\\]\n\n\nApplications in Machine Learning\nFeature Representation: In machine learning, data points (like images, text, or audio) are often represented as vectors in a high-dimensional space. Each dimension corresponds to a feature of the data point.\nModel Parameters: Many machine learning models, such as linear regression or neural networks, have parameters that are represented as vectors. For example, the weights in a neural network can be thought of as vectors in a vector space.\nOperations on Data: Operations like calculating the distance between data points, dot products for similarity, or vector addition and subtraction are all based on vector space properties.\nDimensionality Reduction: Techniques like Principal Component Analysis (PCA) transform data into a lower-dimensional vector space while trying to preserve the variance in the data.\nText Analysis (NLP): In natural language processing, words or sentences can be represented as vectors in a space where distances between vectors are related to semantic similarity (word embeddings like Word2Vec).\nImage Recognition: In image processing, an image can be represented as a vector where each dimension corresponds to the intensity of a pixel.\nTime Series Analysis: Each point in a time series can be considered a vector in a multi-dimensional space, where each dimension corresponds to a different time point.\nUnderstanding vector spaces allows machine learning practitioners to manipulate and analyze data effectively and provides a foundation for more advanced topics like optimization, kernel methods, and deep learning.\n\n\n\n3.2.2 Hilbert Spaces\nHilbert spaces are an essential concept in functional analysis and play a significant role in various machine learning algorithms. They provide a mathematical framework for infinite-dimensional spaces, extending the methods of vector algebra and calculus from the two-dimensional plane and three-dimensional space to spaces with any finite or infinite number of dimensions.\nA Hilbert space is a complete vector space equipped with an inner product. It generalizes the notion of Euclidean space to infinite dimensions, allowing for the rigorous handling of limits and convergence.\nA Hilbert space \\(\\mathcal{H}\\) is defined by two main properties:\n\nInner Product: There is an inner product in \\(\\mathcal{H}\\), denoted as \\(\\langle \\boldsymbol{x}, \\boldsymbol{y}\\rangle\\), for vectors \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{y}\\). This inner product satisfies:\n\nConjugate Symmetry: \\(\\langle \\boldsymbol{x}, \\boldsymbol{y}\\rangle = \\overline{\\langle \\boldsymbol{y}, \\boldsymbol{x}\\rangle}\\).\nLinearity in the First Argument: \\(\\langle a\\boldsymbol{x}+ b\\boldsymbol{y}, \\boldsymbol{z}\\rangle = a\\langle \\boldsymbol{x}, \\boldsymbol{z}\\rangle + b\\langle \\boldsymbol{y}, \\boldsymbol{z}\\rangle\\), where \\(a, b\\) are scalars.\nPositive Definiteness: \\(\\langle \\boldsymbol{x}, \\boldsymbol{x}\\rangle \\geq 0\\), with equality if and only if \\(\\boldsymbol{x}= \\boldsymbol{0}\\).\n\nCompleteness: \\(\\mathcal{H}\\) is complete, meaning every Cauchy sequence in \\(\\mathcal{H}\\) converges to a limit within \\(\\mathcal{H}\\). A sequence \\(\\{\\boldsymbol{x}_n\\}\\) is Cauchy if for every \\(\\varepsilon &gt; 0\\), there exists an \\(N\\) such that \\(\\|\\boldsymbol{x}_n - \\boldsymbol{x}_m\\| &lt; \\varepsilon\\) for all \\(m, n &gt; N\\), with the norm defined as \\(\\|\\boldsymbol{x}\\| = \\sqrt{\\langle \\boldsymbol{x}, \\boldsymbol{x}\\rangle}\\).\n\nFew examples of Hilberts spaces include:\nEuclidean Space: The standard Euclidean space \\(\\mathcal{R}^n\\) with the usual dot product is a Hilbert space.\nSequence Space (\\(\\mathcal{l}_2\\)): The space of square-summable sequences, \\(\\mathcal{l}_2\\), is a Hilbert space where a sequence \\(\\{a_n\\}\\) belongs if \\(\\sum_{n=1}^{\\infty} |a_n|^2 &lt; \\infty\\). The inner product is \\(\\langle \\{a_n\\}, \\{b_n\\} \\rangle = \\sum_{n=1}^{\\infty} a_n \\overline{b_n}\\).\nFunction Space (\\(\\mathcal{L}_2\\)): Spaces of square-integrable functions, such as \\(\\mathcal{L}_2\\) spaces, are Hilbert spaces. A function \\(f\\) is in \\(\\mathcal{L}_2\\) if \\(\\int |f(x)|^2 dx &lt; \\infty\\). The inner product is \\(\\langle f, g \\rangle = \\int f(x) \\overline{g(x)} dx\\).\nHilbert spaces are fundamental in understanding complex systems and algorithms in machine learning and quantum mechanics, especially due to their properties related to orthogonality, projections, and spectral theory.\n\nApplications in Machine Learning\nHilbert spaces play a crucial role in various machine learning algorithms and techniques. They provide a mathematical foundation for handling infinite-dimensional spaces and enable the application of geometric and algebraic concepts to complex datasets. Here are some examples of how Hilbert spaces are used in machine learning:\nKernel Methods in Support Vector Machines (SVMs): SVMs are a popular machine learning algorithm used for classification and regression tasks. Kernel methods, a key component of SVMs, allow these algorithms to operate in high-dimensional Hilbert spaces without explicitly computing the coordinates of the data in that space.\nThe kernel trick maps input data into a higher-dimensional Hilbert space (feature space) where linear separation of the data is easier. For instance, the Radial Basis Function (RBF) kernel implicitly maps data to an infinite-dimensional Hilbert space.\nGaussian Processes: Gaussian processes are used for probabilistic regression and classification. They are powerful in providing uncertainty estimates along with predictions.\nGaussian processes can be viewed as defining a distribution over functions, where these functions belong to a Hilbert space. This perspective allows for the use of kernel functions to measure similarities in this space, enabling the modeling of complex data relationships.\nQuantum Machine Learning: Quantum machine learning algorithms leverage the principles of quantum mechanics for data processing, often surpassing the capabilities of classical algorithms in certain tasks.\nIn quantum computing, the state space of quantum systems is a Hilbert space. Understanding the structure of these spaces is crucial for developing and analyzing quantum machine learning algorithms.\nFeature Spaces in Neural Networks: In deep learning, neural networks transform input data through multiple layers, each potentially representing a different feature space.\nWhile not always explicitly stated, the transformations applied by neural networks can be thought of as mapping data into different Hilbert spaces at each layer, particularly in the context of reproducing kernel Hilbert spaces (RKHS).\nPrincipal Component Analysis (PCA): PCA is a technique used for dimensionality reduction, feature extraction, and data visualization.\nPCA involves projecting data onto the principal components (directions of maximum variance) in a high-dimensional Hilbert space. This projection helps in reducing the dimensionality of the data while preserving as much variance as possible.\nIn summary, Hilbert spaces provide a theoretical foundation for many machine learning algorithms, particularly those involving kernel methods, probabilistic modeling, and high-dimensional data transformations. Understanding Hilbert spaces enhances the comprehension of the geometric and functional aspects of these algorithms.\n\n\n\n3.2.3 Banach Spaces\nA Banach space is a type of vector space that is both normed and complete. Formally, a Banach space is defined as follows:\nA Banach space is a pair \\((\\mathcal{V}, \\|\\cdot\\|)\\) consisting of a vector space \\(\\mathcal{V}\\) over the field \\(\\mathcal{R}\\) or \\(\\mathcal{C}\\) and a norm \\(\\|\\cdot\\|: \\mathcal{V} \\rightarrow \\mathcal{R}\\) satisfying the following properties:\n\nPositive definiteness: \\(\\|x\\| \\geq 0\\) for all \\(x \\in \\mathcal{V}\\) and \\(\\|x\\| = 0\\) if and only if \\(x = 0\\)\nHomogeneity: \\(\\|\\lambda x\\| = |\\lambda|\\|x\\|\\) for all \\(\\lambda\\) in the field and \\(x \\in \\mathcal{V}\\).\nTriangle Inequality: \\(\\|x + y\\| \\leq \\|x\\| + \\|y\\|\\) for all \\(x, y \\in \\mathcal{V}\\).\n\nAdditionally, the space \\(\\mathcal{V}\\) is complete, i.e., every Cauchy sequence in \\(X\\) converges to a limit that is also in \\(\\mathcal{V}\\).\nIn simpler terms, a Banach space is a vector space where the concept of length or size is well-defined (via the norm) and in which every sequence that intuitively should have a limit actually has a limit within the space.\nSome examples of Banach space include:\n\nThe set of all continuous real-valued functions on a closed interval \\([a, b]\\), denoted \\(C[a, b]\\), with the norm defined as \\(\\|f\\| = \\sup\\{|f(x)| : x \\in [a, b]\\}\\), is a Banach space.\nThe space \\(\\mathcal{L}_p\\) for \\(1 \\leq p &lt; \\infty\\), which consists of all sequences \\(\\{a_n\\}\\) for which the series \\(\\sum_{n=1}^{\\infty} |a_n|^p\\) converges, is a Banach space with the norm \\(\\|a\\|_p = \\left(\\sum_{n=1}^{\\infty} |a_n|^p\\right)^{1/p}\\).\n\nBanach spaces are a central object of study in functional analysis and have applications in various areas of mathematics, including analysis, differential equations, and the theory of function spaces.\n\nComparison between Hilbert and Banach Space\nHilbert spaces and Banach spaces are both fundamental concepts in functional analysis, but they have distinct characteristics:\n\nInner Product vs. Norm:\n\nHilbert Space: A Hilbert space is a vector space equipped with an inner product. This inner product allows for the definition of angles and lengths, similar to Euclidean spaces. The norm in a Hilbert space is derived from the inner product, \\(\\|x\\| = \\sqrt{\\langle x, x \\rangle}\\).\nBanach Space: A Banach space is a vector space equipped with a norm, which may or may not be derived from an inner product. The norm in a Banach space measures the size or length of vectors but does not inherently define angles between them.\n\nCompleteness:\n\nBoth Hilbert and Banach spaces are complete, meaning that every Cauchy sequence in the space converges to a point within the space. This is a common feature of both spaces.\n\nGeometric Intuition:\n\nHilbert Space: Due to the presence of an inner product, Hilbert spaces have a stronger geometric structure, allowing for concepts like orthogonality and projection, which are critical in many applications.\nBanach Space: Banach spaces, lacking a general inner product, do not inherently have these geometric properties, focusing more on the analysis through the norm.\n\nExamples and Applications:\n\nHilbert Space: Common examples include the space of square-summable sequences (\\(\\mathcal{l}_2\\)) and the space of square-integrable functions (\\(\\mathcal{L}_2\\)). Applications are found in quantum mechanics, signal processing, and machine learning (e.g., in kernel methods).\nBanach Space: Examples include \\(\\mathcal{L}p\\) spaces (for \\(1 \\leq p \\leq \\infty\\)) and the space of continuous functions. Applications span a broad range of mathematical areas, including functional analysis, differential equations, and optimization.\n\n\nIn summary, while both Hilbert and Banach spaces are complete normed vector spaces, the key difference lies in the presence of an inner product in Hilbert spaces, giving them additional geometric properties. Banach spaces are more general and do not necessarily have these geometric features. All Hilbert spaces are Banach spaces, but not all Banach spaces are Hilbert spaces.\n\n\nApplications in Machine Learning\nIn machine learning, Banach spaces often appear in the context of optimization and function approximation. While Hilbert spaces are more commonly cited due to their geometric properties, Banach spaces are equally important, particularly in scenarios where specific norms are utilized to measure the error or regularize the models. Here are a few examples where Banach spaces are relevant in machine learning:\n\\(\\mathcal{L}_p, \\mathcal{l}_p\\) Spaces in Regularization: Regularization techniques in machine learning, such as Lasso (1-norm regularization) and Ridge Regression (2-norm regularization), involve minimizing a cost function that includes a term based on the \\(\\|\\cdot\\|_1\\) or \\(\\|\\cdot\\|_2\\) norm of the model parameters.\nFunction Spaces in Kernel Methods: While kernel methods such as Support Vector Machines (SVMs) and Gaussian Processes are often associated with Hilbert spaces, some kernel functions, particularly those not associated with an inner product, lead to Banach space formulations.\nSpaces of Bounded Functions: In some machine learning models, particularly in deep learning, the functions approximated by the networks may naturally reside in spaces of bounded, continuous functions, which are examples of Banach spaces.\nOptimization Algorithms: Several optimization algorithms used in training machine learning models can be analyzed within the framework of Banach spaces, especially when dealing with non-differentiable functions or when using specific norms for convergence analysis. Subgradient methods in optimization, commonly used for non-differentiable functions, can be formulated in terms of Banach spaces, particularly when considering convergence properties and robustness.\nThese examples illustrate how Banach spaces, though less explicitly mentioned than Hilbert spaces, play a significant role in the theoretical foundation and practical implementation of various machine learning algorithms and techniques.\n\n\n\n3.2.4 Sobolev Spaces\nSobolev spaces are essential in functional analysis, with significant applications in the study of partial differential equations and approximation theory. They extend the concept of differentiability and integrability to a broader class of functions.\nA Sobolev space, denoted as \\(\\mathcal{W}_{k,p}(\\Omega)\\), is defined for functions with weak derivatives up to order \\(k\\) that are \\(\\mathcal{L}_p\\)-integrable.\n\n\\(\\Omega\\) is an open subset of \\(\\mathcal{R}^n\\).\n\\(k\\) is a non-negative integer, the order of derivatives.\n\\(p\\) is a real number, \\(1 \\leq p \\leq \\infty\\), representing integrability.\n\nFor \\(f \\in \\mathcal{W}_{k,p}(\\Omega)\\), the following conditions must be satisfied:\n\nFor all multi-indices \\(\\alpha\\) with \\(|\\alpha| \\leq k\\), the weak derivatives \\(D^\\alpha f\\) exist and \\(D^\\alpha f \\in \\mathcal{L}_p(\\Omega)\\).\nThe norm in \\(\\mathcal{W}_{k,p}(\\Omega)\\) is defined as:\n\\[ \\|f\\|_{\\mathcal{W}_{k,p}(\\Omega)} = \\left( \\sum_{|\\alpha| \\leq k} \\|D^\\alpha f\\|_{\\mathcal{L}_p(\\Omega)}^p \\right)^{1/p}. \\]\nFor \\(p = \\infty\\), the norm is:\n\\[ \\|f\\|_{W_{k,\\infty}(\\Omega)} = \\max_{|\\alpha| \\leq k} \\|D^\\alpha f\\|_{\\mathcal{L}_\\infty(\\Omega)}. \\]\n\nHere are some simple examples to illustrating Sobolev spaces are:\nExample 1: First Order Sobolev Space \\(\\mathcal{W}_{1,2}(\\Omega)\\) on a Real Interval: Consider the Sobolev space \\(\\mathcal{W}_{1,2}([a, b])\\), where \\([a, b]\\) is a closed interval on the real line. This space consists of all real-valued functions on \\([a, b]\\) that have square-integrable first derivatives.\nA function \\(f\\) belongs to \\(\\mathcal{W}_{1,2}([a, b])\\) if both \\(f\\) and its weak derivative \\(f'\\) are in \\(\\mathcal{L}_2([a, b])\\). This means \\[\\int_a^b |f(x)|^2 dx &lt; \\infty \\quad \\text{and} \\quad \\int_a^b |f'(x)|^2 dx &lt; \\infty. \\]\nAn example of a function in this space could be \\(f(x) = x^3\\) on the interval \\([0, 1]\\). Both the function and its derivative \\(f'(x) = 3x^2\\) are square-integrable over this interval.\nExample 2: Sobolev Space \\(\\mathcal{W}_{2,p}(\\mathcal{R})\\) for \\(p &gt; 1\\): The Sobolev space \\(\\mathcal{W}_{2,p}(\\mathcal{R})\\) consists of functions whose first and second weak derivatives are in the Lebesgue space \\(\\mathcal{L}_p(\\mathcal{R})\\).\nMathematically a function \\(g\\) is in \\(W_{2,p}(\\mathcal{R})\\) if \\(g, g',\\) and \\(g''\\) (first and second derivatives) are all \\(\\mathcal{L}_p\\)-integrable. For \\(p = 2\\), this space is also a Hilbert space.\nAn example could be \\(g(x) = e^{-x^2}\\). This function, along with its first and second derivatives, are \\(p\\)-integrable for any \\(p &gt; 1\\).\nExample 3: Zeroth Order Sobolev Space \\(\\mathcal{W}_{0,p}(\\Omega)\\): The Sobolev space \\(\\mathcal{W}_{0,p}(\\Omega)\\) is essentially the Lebesgue space \\(\\mathcal{L}_p(\\Omega)\\). It includes functions that are \\(p\\)-integrable over the domain \\(\\Omega\\).\nFor \\(\\mathcal{W}_{0,p}(\\Omega)\\), no derivatives are involved. A function \\(h\\) is in this space if \\[ \\int_\\Omega |h(x)|^p dx &lt; \\infty.\\]\nA simple function like \\(h(x) = \\sin(x)\\) on a bounded domain like \\([0, \\pi]\\) is in \\(\\mathcal{W}_{0,p}([0, \\pi])\\) for any \\(p \\geq 1\\).\nThese examples illustrate the concept of Sobolev spaces with varying orders and integrability conditions, showcasing their flexibility in accommodating different degrees of smoothness and integrability requirements for functions.\n\nApplications in Machine Learning\nSobolev spaces are used in machine learning primarily in the context of function approximation, regularization, and understanding the behavior of learning algorithms, especially for problems involving differential equations and smoothness constraints. Here’s a breakdown of their applications:\nDeep Learning and Neural Networks: In deep learning, neural networks can be thought of as function approximators. Sobolev spaces provide a theoretical framework for analyzing the smoothness and differentiability of the functions that these networks can approximate. Specifically, they help in understanding how well neural networks can approximate functions with certain smoothness (derivatives) properties.\nKernel Methods: In machine learning algorithms like Support Vector Machines (SVMs) and Gaussian Processes, kernel functions implicitly map input data into high-dimensional feature spaces. These feature spaces can often be associated with Sobolev spaces, especially when considering the smoothness and regularity of the functions represented by these kernels.\nSobolev Norms for Regularization: In machine learning, regularization is used to prevent overfitting by penalizing complex models. Sobolev norms, which measure the smoothness of a function, can be used as a regularization term in the loss function of a learning algorithm. This is particularly relevant in regression problems where the goal includes maintaining a certain degree of smoothness in the learned function.\nLearning Solutions to PDEs: Machine learning, particularly deep learning, is increasingly used to find approximate solutions to complex PDEs. Sobolev spaces are inherently tied to the theory of PDEs, and understanding these spaces is crucial when using machine learning methods to approximate solutions of PDEs, ensuring that the learned solutions possess the desired smoothness and differentiability properties.\nGeneralization and Complexity Analysis: In theoretical machine learning, Sobolev spaces can be used to analyze the complexity and generalization capabilities of learning algorithms. They offer a way to quantify the capacity of a model class (e.g., a class of neural networks) to approximate functions with certain smoothness properties.\nImage Processing and Computer Vision: In tasks like image segmentation and edge detection, maintaining the smoothness of the output (e.g., segmented regions) can be crucial. Machine learning models trained with Sobolev space-based regularization can yield smoother and more coherent results.\nScientific Computing: In scientific fields that use machine learning to model physical phenomena (e.g., climate modeling, fluid dynamics), ensuring that the learned models adhere to physical laws often expressible by PDEs can be aided by the theoretical underpinnings of Sobolev spaces.\nIn summary, Sobolev spaces in machine learning are mainly behind the scenes, offering a theoretical foundation for understanding the smoothness, differentiability, and general behavior of the functions that machine learning models learn and represent. They are particularly important in areas where the smoothness of the learned function is crucial, both for performance and interpretability.\n\n\n\n3.2.5 Measure Spaces\nA measure space is a fundamental concept in measure theory, a branch of mathematical analysis that deals with generalizations of notions of size and length. It provides a formal framework for defining and working with measures, which can be thought of as a way to assign a size, volume, or probability to subsets of a given set.\nA measure space is defined as a triple \\((\\mathcal{X}, \\mathcal{M}, \\mu)\\) where:\n\n\\(\\mathcal{X}\\) is a set, often referred to as the ‘universe’ or ‘space’.\n\\(\\mathcal{M}\\) is a \\(\\sigma\\)-algebra over \\(\\mathcal{X}\\). A \\(\\sigma\\)-algebra is a collection of subsets of \\(\\mathcal{X}\\) that includes the empty set, is closed under complementation, and is closed under countable unions. In other words, if \\(A\\) is in \\(\\mathcal{M}\\), then so is its complement \\(\\mathcal{X} \\setminus A\\), and if \\(A_1, A_2, A_3, \\ldots\\) are in \\(\\mathcal{M}\\), then so is \\(\\bigcup_{i=1}^{\\infty} A_i\\).\n\\(\\mu\\) is a measure on \\(\\mathcal{M}\\). A measure is a function \\(\\mu: \\mathcal{M} \\rightarrow [0, \\infty]\\) that assigns a non-negative extended real number to each set in \\(\\mathcal{M}\\) in a way that satisfies the following properties:\n\n\nNon-negativity: For every \\(A \\in \\mathcal{M}\\), \\(\\mu(A) \\geq 0\\).\nNull empty set: \\(\\mu(\\emptyset) = 0\\).\nCountable additivity (or \\(\\sigma\\)-additivity): For any countable collection \\(\\{A_i\\}_{i=1}^{\\infty}\\) of pairwise disjoint sets in \\(\\mathcal{M}\\), \\(\\mu\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} \\mu(A_i)\\).\n\nSome examples of measure space include:\n\nLebesgue Measure: On the real line \\(\\mathcal{R}\\), the Lebesgue measure is a classic example. It extends the intuitive concept of length to a wider class of sets. In this case, \\(\\mathcal{X} = \\mathcal{R}\\), \\(\\mathcal{M}\\) is the \\(\\sigma\\)-algebra of Lebesgue measurable sets, and \\(\\mu\\) is the Lebesgue measure.\nProbability Space: In probability theory, a measure space with the total measure of 1 (i.e., \\(\\mu(\\mathcal{X}) = 1\\)) is called a probability space. Here, the measure of a set represents the probability of that event.\n\nMeasure spaces are essential in various fields of mathematics, including probability theory, statistical theory, and real analysis. They allow for the rigorous treatment of integrals, probabilities, and sizes in more abstract settings than traditional notions of length or volume.\n\n\n3.2.6 Lebesgue Spaces\nLebesgue spaces, often denoted as \\(\\mathcal{L}_p\\) spaces, are a fundamental concept in functional analysis and measure theory. They generalize the notion of spaces of integrable functions beyond the traditional framework provided by Riemann integration, using the more powerful tool of Lebesgue integration.\nFor a given measure space \\((\\mathcal{X}, \\mathcal{M}, \\mu)\\) and a real number \\(p \\geq 1\\), the \\(\\mathcal{L}_p\\) space, denoted as \\(\\mathcal{L}_p(\\mathcal{X}, \\mathcal{M}, \\mu)\\) or simply \\(\\mathcal{L}_p(\\mathcal{X})\\), is defined as follows:\n\nIt consists of all measurable functions \\(f: \\mathcal{X} \\rightarrow \\mathcal{R}\\) (or \\(\\mathcal{C}\\)) for which the \\(p^\\text{th}\\) power of the absolute value is Lebesgue integrable, i.e.,\n\n\\[ \\int_X |f(x)|^p \\, d\\mu &lt; \\infty \\]\n\\(\\mathcal{L}_p\\) spaces have the following properties\n\nNorm: Each \\(\\mathcal{L}_p\\) space is equipped with a norm, known as the \\(\\mathcal{L}_p\\)-norm, defined as:\n\n\\[ \\|f\\|_{\\mathcal{L}_p} = \\left( \\int_X |f(x)|^p \\, d\\mu \\right)^{1/p} \\]\n\nCompleteness: Every \\(\\mathcal{L}_p\\) space is complete, meaning that every Cauchy sequence in \\(\\mathcal{L}_p\\) converges to a limit within \\(\\mathcal{L}_p\\).\nSpecial Cases:\n\n\\(\\mathcal{L}_2\\) Space: This is a Hilbert space with the inner product defined as \\(\\langle f, g \\rangle = \\int_X f(x) \\overline{g(x)} \\, d\\mu\\). It is widely used in quantum mechanics and signal processing.\n\\(\\mathcal{L}_1\\) Space: Consists of absolutely integrable functions. It’s important in probability theory and various branches of analysis.\n\\(\\mathcal{L}_\\infty\\) Space: Defined as the space of essentially bounded functions, with the norm given by the essential supremum of the function.\n\n\nIn general, \\(\\mathcal{L}_p\\) Lebesgue spaces are defined by integrability conditions. They are always Banach spaces and become Hilbert spaces specifically when \\(p=2\\).\n\nApplications in Machine Learning\nLebesgue spaces, especially \\(\\mathcal{L}_p\\) spaces, find various applications in machine learning, primarily in the formulation of loss functions, regularization techniques, and in the theoretical underpinnings of learning algorithms. Here are some key applications:\nCross-Entropy Loss in Classification: In classification problems, particularly with neural networks, the cross-entropy loss function is often used. While not directly an \\(\\mathcal{L}_p\\) norm, it is related to the concept of integrability and measurement in Lebesgue spaces.\nFeature Space Mapping in Kernel Methods: In Support Vector Machines (SVM) and other kernel-based methods, data is implicitly mapped into a high-dimensional feature space. Understanding these feature spaces often involves concepts from Lebesgue spaces, particularly when analyzing the smoothness and integrability of functions in that space.\nProbabilistic Models and Density Estimation: In probabilistic models and density estimation techniques, the properties of functions in \\(\\mathcal{L}_p\\) spaces help in understanding the behavior and properties of probability density functions, especially in terms of integrability and convergence.\nDeep Learning Theory: In deep learning, particularly in the analysis of neural network functions and their approximation capabilities, Lebesgue spaces provide a framework for understanding function spaces that neural networks can represent.\nFourier Analysis and Signal Reconstruction: In signal processing, many problems involve working with signals in the \\(\\mathcal{L}_2\\) space, particularly when using Fourier analysis for signal reconstruction and processing.\nIn summary, Lebesgue spaces in machine learning are crucial for formulating various models and algorithms, especially in understanding and controlling the behavior of learning algorithms through loss functions and regularization. They provide a mathematical foundation for many of the tools and techniques commonly used in the field."
  },
  {
    "objectID": "random_variables.html#definitions-and-probability-axioms",
    "href": "random_variables.html#definitions-and-probability-axioms",
    "title": "4  Random Variables",
    "section": "4.1 Definitions and Probability Axioms",
    "text": "4.1 Definitions and Probability Axioms\nProbability theory begins with the fundamental notion of a probability space, which consists of three elements: a sample space \\(\\Omega\\), a set of events \\(F\\), and a probability measure \\(P\\).\n\nSample Space (\\(\\Omega\\)): The set of all possible outcomes of a random experiment. For example, in a coin toss, \\(\\Omega = \\{\\text{heads}, \\text{tails}\\}\\).\nEvents (\\(F\\)): A collection of outcomes (a subset of \\(\\Omega\\)). An event is said to occur if the outcome of the experiment is in this set.\nProbability Measure (\\(P\\)): A function that assigns a probability to each event in \\(F\\), satisfying the following axioms:\n\nNon-negativity: For every event \\(A \\in F\\), \\(P(A) \\geq 0\\).\nNormalization: \\(P(\\Omega) = 1\\).\nAdditivity: For any sequence of mutually exclusive events \\(A_1, A_2, \\ldots\\), \\(P(\\bigcup_{i} A_i) = \\sum_{i} P(A_i)\\)."
  },
  {
    "objectID": "random_variables.html#conditional-probability",
    "href": "random_variables.html#conditional-probability",
    "title": "4  Random Variables",
    "section": "4.2 Conditional Probability",
    "text": "4.2 Conditional Probability\nConditional probability refers to the probability of an event given that another event has occurred, denoted as \\(P(A|B)\\)for events \\(A\\)and \\(B\\). It is defined as: \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\] provided that \\(P(B) &gt; 0\\)."
  },
  {
    "objectID": "random_variables.html#bayes-theorem",
    "href": "random_variables.html#bayes-theorem",
    "title": "4  Random Variables",
    "section": "4.3 Bayes’ Theorem",
    "text": "4.3 Bayes’ Theorem\nBayes’ theorem provides a way to update our probability estimates based on new information. It is expressed as: \\[ P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \\] where:\n\n\\(P(A|B)\\) is the posterior probability of \\(A\\) given \\(B\\).\n\\(P(B|A)\\) is the likelihood of \\(B\\) given \\(A\\).\n\\(P(A)\\) is the prior probability of \\(A\\).\n\\(P(B)\\) is the marginal probability of \\(B\\).\n\nBayes’ theorem is foundational in various machine learning algorithms, especially in Bayesian inference, where it is used to update the probability estimate for a hypothesis as more evidence or information becomes available."
  },
  {
    "objectID": "random_variables.html#discrete-random-variables",
    "href": "random_variables.html#discrete-random-variables",
    "title": "4  Random Variables",
    "section": "4.4 Discrete Random Variables",
    "text": "4.4 Discrete Random Variables\n\nDefinition: A discrete random variable is a type of random variable that can take on a countable number of distinct outcomes. Common examples include the number of heads in a coin toss or the number of successes in a series of Bernoulli trials.\nProbability Mass Function (PMF): The PMF of a discrete random variable provides the probabilities of all possible outcomes. For a discrete random variable \\(X\\), the PMF \\(P(X=x)\\) gives the probability that \\(X\\) equals a particular value \\(x\\).\n\n\n4.4.1 Continuous Random Variables\n\nDefinition: A continuous random variable is a random variable that can take on an uncountably infinite number of possible values. For example, the exact time it takes for a chemical reaction to occur or the precise temperature at a given location.\nProbability Density Function (PDF): For continuous random variables, the PDF is used to specify the probability of the random variable falling within a particular range of values. The probability of the variable falling within an interval is given by the integral of the PDF over that interval."
  },
  {
    "objectID": "random_variables.html#cumulative-distribution-functions-cdfs",
    "href": "random_variables.html#cumulative-distribution-functions-cdfs",
    "title": "4  Random Variables",
    "section": "4.5 Cumulative Distribution Functions (CDFs)",
    "text": "4.5 Cumulative Distribution Functions (CDFs)\n\nDefinition: The CDF is a function that gives the probability that a random variable is less than or equal to a certain value. It applies to both discrete and continuous random variables.\nExpression: For a random variable \\(X\\), the CDF \\(F(x)\\) is defined as \\(F(x) = P(X \\leq x)\\). In the case of a discrete random variable, it is the sum of the probabilities up to \\(x\\); for a continuous variable, it is the integral of the PDF up to \\(x\\).\n\nWe’ll use Python to generate illustrations for PMFs, PDFs, and CDFs. For PMFs, we’ll consider a simple binomial distribution, and for PDFs and CDFs, we’ll use a normal distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, norm\n\n# PMF of a Binomial Distribution\nn, p = 10, 0.5  # Parameters for the binomial distribution\nx_binom = np.arange(0, n+1)\npmf_binom = binom.pmf(x_binom, n, p)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.bar(x_binom, pmf_binom)\nplt.title('PMF of a Binomial Distribution')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\n\n# PDF and CDF of a Normal Distribution\nmu, sigma = 0, 1  # Mean and standard deviation\nx_norm = np.linspace(-4, 4, 1000)\npdf_norm = norm.pdf(x_norm, mu, sigma)\ncdf_norm = norm.cdf(x_norm, mu, sigma)\n\nplt.subplot(1, 3, 2)\nplt.plot(x_norm, pdf_norm)\nplt.title('PDF of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.subplot(1, 3, 3)\nplt.plot(x_norm, cdf_norm)\nplt.title('CDF of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Cumulative Probability')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nThe Python code creates a figure with three subplots: the first showing the PMF of a binomial distribution, the second showing the PDF of a normal distribution, and the third displaying the CDF of the same normal distribution."
  },
  {
    "objectID": "random_variables.html#expected-value",
    "href": "random_variables.html#expected-value",
    "title": "4  Random Variables",
    "section": "4.6 Expected Value",
    "text": "4.6 Expected Value\n\nDefinition: The expected value (or mean) of a random variable is a measure of the central tendency of the variable’s probability distribution. Mathematically, for a discrete random variable \\(X\\) with possible values \\(x_1, x_2, \\ldots\\) and a probability mass function \\(P(X)\\), the expected value \\(\\mathbb{E}\\left[X\\right]\\) is defined as: \\[ \\mathbb{E}\\left[X\\right] = \\sum_{i} x_i P(X = x_i).\\] For a continuous random variable with a probability density function \\(f(x)\\), it is defined as: \\[ \\mathbb{E}\\left[X\\right] = \\int_{-\\infty}^{\\infty} x f(x) dx \\]\nProperties: The expected value is linear, meaning for any two random variables \\(X\\) and \\(Y\\), and constants \\(a\\) and \\(b\\), we have \\(\\mathbb{E}\\left[aX + bY\\right] = a\\mathbb{E}\\left[X\\right] + b\\mathbb{E}\\left[Y\\right]\\).\nSignificance in Machine Learning: The expected value is used in various machine learning contexts, such as defining the loss functions (e.g., mean squared error) and as a measure of central tendency in data analysis."
  },
  {
    "objectID": "random_variables.html#variance",
    "href": "random_variables.html#variance",
    "title": "4  Random Variables",
    "section": "4.7 Variance",
    "text": "4.7 Variance\n\nDefinition: The variance of a random variable measures the spread of its values. It is defined as the expected value of the squared deviation from the mean. For a random variable \\(X\\) with mean \\(\\mu = \\mathbb{E}\\left[X\\right]\\), the variance \\(\\text{Var}(X)\\) is given by: \\[ \\text{Var}(X) = \\mathbb{E}\\left[(X - \\mu\\right]^2] \\] which can also be written as: \\[\\text{Var}(X) = \\mathbb{E}\\left[X^2\\right] - (\\mathbb{E}\\left[X\\right])^2\\]\nProperties: Variance measures the dispersion of the data. A high variance indicates that the data points are spread out from the mean, while a low variance indicates that they are clustered closely around the mean.\nSignificance in Machine Learning: In machine learning, variance is a key concept in understanding the model’s behavior, particularly in the bias-variance tradeoff, which is crucial for understanding model performance and overfitting.\n\nWe’ll use Python to illustrate the computation of expected value and variance for a simple discrete distribution (e.g., rolling a fair six-sided die) and a continuous distribution (e.g., normal distribution).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import randint, norm\n\n# Expected Value and Variance of a Discrete Random Variable (Die Roll)\nvalues = np.arange(1, 7)\nprobabilities = np.full(6, 1/6)  # Fair die\nexpected_value_die = np.sum(values * probabilities)\nvariance_die = np.sum((values - expected_value_die)**2 * probabilities)\n\nprint(\"Expected Value (Die Roll):\", expected_value_die)\nprint(\"Variance (Die Roll):\", variance_die)\n\n# Expected Value and Variance of a Continuous Random Variable (Normal Distribution)\nmu, sigma = 0, 1  # Mean and standard deviation\nx = np.linspace(-5, 5, 1000)\npdf = norm.pdf(x, mu, sigma)\nexpected_value_normal = np.sum(x * pdf) * (x[1] - x[0])  # Approximate integral\nvariance_normal = np.sum(((x - mu)**2) * pdf) * (x[1] - x[0])\n\nprint(\"Expected Value (Normal Distribution):\", expected_value_normal)\nprint(\"Variance (Normal Distribution):\", variance_normal)\n\n# Plotting\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.bar(values, probabilities)\nplt.title('PMF of a Fair Die')\nplt.xlabel('Die Value')\nplt.ylabel('Probability')\n\nplt.subplot(1, 2, 2)\nplt.plot(x, pdf)\nplt.title('PDF of a Normal Distribution')\nplt.xlabel('Value')\nplt.ylabel('Density')\n\nplt.show()\n\n\n\nExpected Value (Die Roll): 3.5\nVariance (Die Roll): 2.9166666666666665\nExpected Value (Normal Distribution): 7.112539897498498e-17\nVariance (Normal Distribution): 0.9999849286984711\n\n\n\n\n\n\n\nThis code calculates and prints the expected value and variance for both a fair die roll (discrete) and a normal distribution (continuous). It also provides visualizations of their probability distributions. These examples demonstrate how expected value and variance are computed and their importance in describing the characteristics of a distribution."
  },
  {
    "objectID": "random_variables.html#law-of-large-numbers",
    "href": "random_variables.html#law-of-large-numbers",
    "title": "4  Random Variables",
    "section": "4.8 Law of Large Numbers",
    "text": "4.8 Law of Large Numbers\nThe Law of Large Numbers (LLN) is a fundamental theorem in probability theory that describes the result of performing the same experiment a large number of times. It states that as the number of trials or observations increases, the average of the results obtained from these trials converges to the expected value. This theorem provides a solid foundation for the concept of statistical stability and the predictability of outcomes in probability and statistics.\nThere are two types of Law of Large Numbers:\n\nWeak Law of Large Numbers (WLLN): This version, also known as Khinchin’s Law, asserts that the sample mean converges in probability towards the expected value as the sample size increases. Mathematically, for a sequence of independent and identically distributed (i.i.d.) random variables \\(X_1, X_2, ..., X_n\\) with expected value \\(\\mathbb{E}\\left[X\\right] = \\mu\\), the WLLN states that for any positive number \\(\\epsilon\\), \\[ P\\left( \\left| \\frac{1}{n}\\sum_{i=1}^n X_i - \\mu \\right| &lt; \\epsilon \\right) \\to 1 \\text{ as } n \\to \\infty. \\] This means the probability that the sample mean differs from the true mean by more than \\(\\epsilon\\) tends to zero as \\(n\\) becomes large.\nStrong Law of Large Numbers (SLLN): The SLLN states that the sample mean almost surely converges to the expected value as the sample size goes to infinity. In other words, the sample mean and the expected value will be equal with probability 1 in the limit of an infinite number of trials. This is a stronger statement than the WLLN and requires slightly stronger conditions.\n\nSome application of LLNs include:\n\nEmpirical Predictability: LLN explains why averages of larger samples are more stable and reliable than those of smaller samples, a principle that underpins much of empirical science.\nStatistics and Data Analysis: In statistics, LLN is crucial for the justification of using sample means as estimates for population means.\nFinancial Modeling: In finance, LLN helps in predicting long-term investment outcomes based on historical averages.\nQuality Control: In industrial processes, LLN is used to understand that averaging the results of a process over a long period will give a good estimation of the overall process performance.\n\nThe Law of Large Numbers is a cornerstone of probability theory and statistics, providing a rationale for the apparent regularity of large systems and the basis for making inferences about population parameters based on sample statistics. It essentially underlines the reliability of averages in large datasets, a key concept in many practical applications across various disciplines.\nA simple demonstration of LLN is achieved by the following Python code.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the number of trials\ntrials = [10, 50, 100, 500, 1000, 5000, 10000]\n\n# Define a random variable (e.g., a normal distribution)\nmean = 0\nstd_dev = 1\nnp.random.seed(0)  # for reproducibility\n\n# Store the average outcomes\naverages = []\n\n# Simulate the trials\nfor n in trials:\n    samples = np.random.normal(mean, std_dev, n)\n    average = np.mean(samples)\n    averages.append(average)\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(trials, averages, marker='o', linestyle='-', color='b')\nplt.axhline(y=mean, color='r', linestyle='--')\nplt.title('Demonstration of the Law of Large Numbers')\nplt.xscale('log')  # Log scale for better visualization\nplt.xlabel('Number of Trials (log scale)')\nplt.ylabel('Sample Mean')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nIn the plot, the x-axis represents the number of trials (on a logarithmic scale for clarity), ranging from small to large sample sizes. The y-axis shows the sample mean for each set of trials. The red dashed line indicates the true mean of the distribution (0 in this case). We observe that as the number of samples increases, the emprical mean converges to the true mean.\n\n\n\n\n\n\nThe Python script demonstrates the Law of Large Numbers (LLN) by simulating random samples from a normal distribution with a mean of 0 and a standard deviation of 1. As the number of trials increases, the script calculates the sample mean for each set of trials and plots these means. As illustrated in the plot, as the number of trials increases, the sample mean converges towards the true mean of the distribution, consistent with the Law of Large Numbers. This demonstrates that with a larger number of observations, the average of the outcomes tends to get closer to the expected value, showcasing the LLN’s fundamental principle in probability and statistics."
  },
  {
    "objectID": "random_variables.html#central-limit-theorem",
    "href": "random_variables.html#central-limit-theorem",
    "title": "4  Random Variables",
    "section": "4.9 Central Limit Theorem",
    "text": "4.9 Central Limit Theorem\nThe Central Limit Theorem (CLT) is a fundamental principle in probability theory that states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables will be approximately normally distributed, regardless of the underlying distribution of the individual variables. This theorem is pivotal in statistics because it justifies the use of the normal distribution in many real-world situations.\nKey Points of the CLT:\n\nNormal Approximation: As the sample size grows, the distribution of the sample mean will approach a normal distribution, even if the original variables are not normally distributed.\nConditions: The CLT holds under certain conditions, primarily that the variables are independent and identically distributed with a finite mean and variance.\nSample Mean and Variance: The mean of the sample means will be equal to the mean of the original distribution, and the variance of the sample means will be equal to the variance of the original distribution divided by the sample size.\n\nThe Central Limit Theorem and the Law of Large Numbers (LLN) are related but address different aspects of convergence:\n\nLaw of Large Numbers: The LLN focuses on the convergence of the sample mean to the expected value as the sample size increases. It asserts that the average of a large number of i.i.d. random variables will be close to the expected value, providing a strong foundation for statistical estimation.\nCentral Limit Theorem: The CLT takes this a step further by describing the shape of the distribution of the sample mean. It not only asserts that the sample mean will converge to the expected value but also that the way in which it converges will follow a normal distribution if the sample size is large enough. This is crucial for hypothesis testing and confidence interval estimation.\n\nIn summary, while the LLN tells us that the sample mean will be a good estimate of the population mean for large sample sizes, the CLT tells us about the distribution of this estimate, enabling the use of normal distribution-based inference methods even when the underlying data does not follow a normal distribution.\nThe following Python script demonstrates the Central Limit Theorem (CLT) by generating distributions of sample means from an original normal distribution (with a mean of 5 and standard deviation of 2) for different sample sizes. The script creates multiple samples for each specified sample size and computes the mean of each sample.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Settings for the demonstration\nsample_sizes = [10, 30, 50, 100, 500]\nnumber_of_samples = 1000\noriginal_distribution_mean = 5\noriginal_distribution_std = 2\n\n# Function to generate sample means\ndef generate_sample_means(sample_size, number_of_samples, mean, std):\n    return [np.mean(np.random.normal(mean, std, sample_size)) for _ in range(number_of_samples)]\n\n# Plot the distributions of sample means\nplt.figure(figsize=(12, 8))\nfor sample_size in sample_sizes:\n    sample_means = generate_sample_means(sample_size, number_of_samples, original_distribution_mean, original_distribution_std)\n    plt.hist(sample_means, bins=30, alpha=0.5, label=f'Sample Size = {sample_size}')\n\nplt.title('Central Limit Theorem Demonstration')\nplt.xlabel('Sample Mean')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nIn the figure, each histogram represents the distribution of sample means for a particular sample size. As the sample size increases, the distribution of the sample means becomes more tightly clustered around the true mean of the original distribution (5 in this case). The shape of these distributions of sample means increasingly resembles a normal distribution, especially as the sample size gets larger.\n\n\n\n\n\n\nThe plot effectively illustrates the essence of the CLT: regardless of the original distribution (which is normal in this case but could be any distribution with a defined mean and variance), the distribution of the sample means approaches a normal distribution as the sample size increases. This phenomenon underpins many statistical methods, especially those involving hypothesis testing and confidence interval estimation."
  },
  {
    "objectID": "random_variables.html#common-distributions",
    "href": "random_variables.html#common-distributions",
    "title": "4  Random Variables",
    "section": "4.10 Common Distributions",
    "text": "4.10 Common Distributions\n\nBinomial Distribution\n\nDescription: The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.\nParameters: Number of trials \\(n\\) and probability of success \\(p\\) in each trial.\nPMF: \\(P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\\) for \\(k = 0, 1, 2, \\ldots, n\\).\n\n\n\nPoisson Distribution\n\nDescription: The Poisson distribution models the number of times an event occurs in a fixed interval of time or space.\nParameter: Average number of occurrences \\(\\lambda\\).\nPMF: \\(P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\\) for \\(k = 0, 1, 2, \\ldots\\).\n\n\n\nGaussian (Normal) Distribution\n\nDescription: The Gaussian distribution is a continuous distribution that is symmetric around its mean, showing the familiar bell-shaped curve.\nParameters: Mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nPDF: \\(f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\\).\n\n\n\nUniform Distribution\n\nDescription: The uniform distribution describes an experiment where each outcome is equally likely.\nParameters: Lower bound \\(a\\) and upper bound \\(b\\).\nPDF: \\(f(x) = \\frac{1}{b - a}\\) for \\(x\\) between \\(a\\) and \\(b\\).\n\n\n\nExponential Distribution\n\nDescription: The exponential distribution models the time between events in a Poisson point process.\nParameter: Rate \\(\\lambda\\).\nPDF: \\(f(x) = \\lambda e^{-\\lambda x}\\) for \\(x \\geq 0\\).\n\nLet’s illustrate these distributions using Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom, poisson, norm, uniform, expon\n\n# Parameters\nn, p = 10, 0.5  # Binomial parameters\nlambda_ = 3     # Poisson parameter\nmu, sigma = 0, 1  # Gaussian parameters\na, b = 0, 10     # Uniform parameters\nlambda_exp = 1   # Exponential parameter\n\n# Data for plotting\nx_binom = np.arange(0, n+1)\nx_poisson = np.arange(0, 20)\nx_gaussian = np.linspace(-4, 4, 1000)\nx_uniform = np.linspace(a, b, 1000)\nx_exponential = np.linspace(0, 10, 1000)\n\n# Plotting\nplt.figure(figsize=(15, 3))\n\nplt.subplot(1, 5, 1)\nplt.bar(x_binom, binom.pmf(x_binom, n, p))\nplt.title('Binomial Distribution')\n\nplt.subplot(1, 5, 2)\nplt.bar(x_poisson, poisson.pmf(x_poisson, lambda_))\nplt.title('Poisson Distribution')\n\nplt.subplot(1, 5, 3)\nplt.plot(x_gaussian, norm.pdf(x_gaussian, mu, sigma))\nplt.title('Gaussian Distribution')\n\nplt.subplot(1, 5, 4)\nplt.plot(x_uniform, uniform.pdf(x_uniform, a, b-a))\nplt.title('Uniform Distribution')\n\nplt.subplot(1, 5, 5)\nplt.plot(x_exponential, expon.pdf(x_exponential, scale=1/lambda_exp))\nplt.title('Exponential Distribution')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nThis code will generate a series of plots illustrating the probability mass or density functions for each of the five distributions. These visualizations help in understanding the characteristics and differences of these common distributions, which are widely used in various machine learning and statistical modeling scenarios."
  },
  {
    "objectID": "random_variables.html#joint-marginal-and-conditional-distributions",
    "href": "random_variables.html#joint-marginal-and-conditional-distributions",
    "title": "4  Random Variables",
    "section": "4.11 Joint, Marginal, and Conditional Distributions",
    "text": "4.11 Joint, Marginal, and Conditional Distributions\nUnderstanding the relationships between multiple random variables is crucial in statistics and machine learning. Here we’ll discuss joint, marginal, and conditional distributions.\n\n4.11.1 Joint Distributions\nJoint distributions are fundamental in understanding the relationship between two or more random variables. They can be categorized based on whether the variables involved are discrete, continuous, or a combination of both.\nFor discrete random variables, the joint distribution is defined by a joint probability mass function (PMF). This function gives the probability that each of the random variables falls within a specific range of values. For discrete random variables \\(X\\) and \\(Y\\), the joint PMF \\(P(X = x, Y = y)\\) provides the probability that \\(X = x\\) and \\(Y = y\\) simultaneously. Probabilities are calculated by summing the joint PMF over the desired range of values. They have the following properties:\n\nThe sum of the joint PMF over all possible values of \\(X\\) and \\(Y\\) is 1.\nThe joint PMF is always non-negative: \\(P(X = x, Y = y) \\geq 0\\).\n\nIn the case of continuous random variables, the joint distribution is described by a joint probability density function (PDF). The joint PDF \\(f_{X,Y}(x, y)\\) represents the density of probabilities at any point in the range of \\(X\\) and \\(Y\\). Probabilities are calculated by integrating the joint PDF over the region of interest. They have the following properties:\n\nThe integral of the joint PDF over the entire space is 1.\nThe joint PDF is non-negative: \\(f_{X,Y}(x, y) \\geq 0\\).\n\nJoint distributions are used in machine learning for modeling the relationships between features, in probabilistic classifiers, and in Bayesian inference, where understanding the dependence between variables is crucial. Joint distributions also form a core concept in statistics and machine learning, as they provide a comprehensive picture of how multiple random variables interact with each other.\n\nHomogenous Joint Distributions\nHomogeneous joint distributions refer to joint probability distributions where the involve random variables that follow identical types of distributions. They also refer to probability distributions where the statistical properties and relationships between two or more random variables remain consistent across the entire range of their values. The following are some examples of homegenous joint distributions:\nJoint Probability Mass Function (Discrete Random Variables) Consider the outcomes from tossing two dice. Let \\(X\\) be the outcome of the first die, and \\(Y\\) be the outcome of the second die. Both \\(X\\) and \\(Y\\) are discrete random variables taking values from 1 to 6. The joint PMF \\(P(X = x, Y = y)\\) is \\(\\frac{1}{36}\\) for \\(x, y \\in \\{1, 2, 3, 4, 5, 6\\}\\), assuming fair dice.\nJoint Probability Density Function (Continuous Random Variables) Consider a height and weight distribution. Let \\(X\\) be a person’s height (in cm) and \\(Y\\) their weight (in kg). Suppose \\(X\\) and \\(Y\\) have a joint PDF given by \\(f(x, y) = k \\cdot e^{-2x - 3y}\\) for \\(x, y &gt; 0\\) (a hypothetical example). Here, \\(k\\) is a normalizing constant to ensure that the total probability integrates to 1.\nBivariate Normal Distribution A common example in statistics is the bivariate normal distribution. Let \\(X\\) and \\(Y\\) be jointly normally distributed with means \\(\\mu_X, \\mu_Y\\), standard deviations \\(\\sigma_X, \\sigma_Y\\), and correlation coefficient \\(\\rho\\). The joint PDF is: \\[ f(x, y) = \\frac{1}{2\\pi\\sigma_X\\sigma_Y\\sqrt{1-\\rho^2}} \\exp\\left(-\\frac{1}{2(1-\\rho^2)}\\left[\\frac{(x-\\mu_X)^2}{\\sigma_X^2} + \\frac{(y-\\mu_Y)^2}{\\sigma_Y^2} - \\frac{2\\rho(x-\\mu_X)(y-\\mu_Y)}{\\sigma_X\\sigma_Y}\\right]\\right).\\]\nMultivariate Gaussian Distribution The Multivariate Gaussian (or Normal) Distribution is a generalization of the one-dimensional Gaussian distribution to multiple dimensions. It describes a random vector in \\(n\\)-dimensional space with a specific correlation structure between its elements.\nA random vector \\(\\boldsymbol{x}= [x_1, x_2, ..., x_n]^T\\) is said to follow a multivariate Gaussian distribution if its probability density function (PDF) is given by: \\[ f(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\boldsymbol{\\Sigma}|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right),\\]\nwhere:\n\n\\(\\boldsymbol{x}\\) is a realization of the random vector \\(\\boldsymbol{x}\\).\n\\(\\boldsymbol{\\mu}\\) is the mean vector, and $$is the covariance matrix.\n\\(|\\boldsymbol{\\Sigma}|\\) is the determinant of the covariance matrix.\n\\(\\boldsymbol{\\Sigma}^{-1}\\) is the inverse of the covariance matrix.\n\nIt has the following properties:\n\nMean Vector: \\(\\boldsymbol{\\mu}\\) specifies the mean of each component of the vector \\(\\boldsymbol{x}\\).\nCovariance Matrix: \\(\\boldsymbol{\\Sigma}\\) describes the variance of each component and the covariances between them. The diagonal elements of \\(\\boldsymbol{\\Sigma}\\) are the variances of the individual components, and the off-diagonal elements are the covariances.\nCorrelation Structure: The covariance matrix determines how the components of \\(\\boldsymbol{x}\\) are linearly related to each other.\nShape and Geometry: The shape of the distribution in \\(n\\)-dimensional space can range from spherical (when \\(\\boldsymbol{\\Sigma}\\) is proportional to the identity matrix) to elliptical (when \\(\\boldsymbol{\\Sigma}\\) has distinct eigenvalues).\n\nUniform Distribution Over a Region Consider distribution of random points in a rectangle. Let \\(X\\) and \\(Y\\) represent the \\(x\\) and \\(y\\) coordinates of a point uniformly distributed over a rectangle \\([a, b] \\times [c, d]\\). The joint PDF is \\(f(x, y) = \\frac{1}{(b-a)(d-c)}\\) for \\(a \\leq x \\leq b\\) and \\(c \\leq y \\leq d\\), and 0 otherwise.\n\n\nHeterogenous Joint Distributions\nJoint distibutions need not be homogenous. They can be heterogenous as well. Heterogeneous joint distributions involve random variables that may follow different types of distributions, and they can capture complex relationships in probabilistic models. Here are some mathematical examples:\nJoint Distribution of a Discrete and a Continuous Variable Suppose \\(X\\) is a discrete random variable representing the number of defects in a product (following a Poisson distribution with parameter \\(\\lambda\\)), and \\(Y\\) is a continuous random variable representing the time (in hours) until the first defect is detected (following an Exponential distribution with rate \\(\\lambda\\)). The joint probability function is not a standard PMF or PDF but a mixed distribution that combines the characteristics of both. The joint distribution might be defined as \\(f(x, y) = P(X = x) \\cdot f_Y(y|x)\\), where \\(f_Y(y|x)\\) is the conditional density of \\(Y\\) given \\(X = x\\).\nJoint Distribution with Conditional Dependence Let \\(X\\) be a Bernoulli random variable indicating whether an event occurs (1) or not (0), and \\(Y\\) be a normally distributed variable representing the outcome measurement, whose mean depends on \\(X\\). For instance, if \\(X = 1\\), \\(Y\\) follows \\(N(\\mu_1, \\sigma^2)\\), and if \\(X = 0\\), \\(Y\\) follows \\(N(\\mu_0, \\sigma^2)\\). The joint distribution is a combination of a Bernoulli and a conditional normal distribution.\nBivariate Distribution with Different Marginals Consider a scenario with two random variables \\(X\\) and \\(Y\\), where \\(X\\) follows a Uniform distribution \\(U(a, b)\\) and \\(Y\\), conditionally on \\(X\\), follows a Normal distribution with mean \\(X\\) and variance \\(\\sigma^2\\). The joint distribution in this case would be \\(f(x, y) = \\frac{1}{b-a} \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y-x)^2}{2\\sigma^2}\\right)\\) for \\(a \\leq x \\leq b\\).\nPiecewise Joint Distribution Imagine a joint distribution where \\(X\\) and \\(Y\\) follow different distributions in different regions of their support. For example, for \\(x &lt; 0\\), \\(X\\) and \\(Y\\) might jointly follow one distribution (e.g., bivariate normal), and for \\(x \\geq 0\\), they follow a different distribution (e.g., bivariate exponential).\nThese heterogeneous joint distributions are particularly useful in modeling complex real-world phenomena where different variables exhibit different types of probabilistic behavior or where their behavior changes under different conditions. Such models are widely used in areas like econometrics, biostatistics, and environmental science.\nSome practical examples illustrating such heterogeneous joint distributions are:\n\nConsider a situation where \\(X\\) is a discrete random variable representing the number of customers arriving at a store (which can be modeled by a Poisson distribution), and \\(Y\\) is a continuous random variable representing the total amount spent by these customers (which could be modeled by a normal distribution). The joint distribution of \\(X\\) and \\(Y\\) would be a mix of a discrete and a continuous distribution.\nSuppose \\(X\\) is a binary variable indicating whether a machine is in operation (1) or not (0), and \\(Y\\) is a continuous variable representing the temperature of the machine when it is operational. The joint distribution of \\(X\\) and \\(Y\\) combines a Bernoulli distribution (for \\(X\\)) and a conditional continuous distribution (for \\(Y\\) given \\(X = 1\\)). This is an example of a joint distribution with binary and continuous variables.\nImagine a survey where \\(X\\) is a nominal variable representing a respondent’s preferred type of music (e.g., rock, jazz, classical), and \\(Y\\) is an ordinal variable indicating satisfaction level (e.g., unsatisfied, neutral, satisfied). The joint distribution of \\(X\\) and \\(Y\\) would be a cross-tabulation of these categories, showing the frequencies or probabilities of each combination. This is an example of a joint distribution of nominal and ordinal variables.\nConsider a scenario where \\(X\\) is the number of products sold (following a Poisson distribution), and \\(Y\\) is the number of customer complaints (which could follow a different discrete distribution, such as a binomial distribution). The joint distribution of \\(X\\) and \\(Y\\) would describe the relationship between sales and complaints, but each variable follows its own distinct distribution. This is an example of a joint distribution with multiple discrete variables with different distributions.\n\nThese examples of heterogeneous joint distributions highlight the complexity and diversity of real-world data, where variables can be of different types and follow different distributions, yet their interdependence can be crucial for analysis and decision-making.\n\n\n\n4.11.2 Marginal Distribution\nMarginal distributions are used to describe the probability distribution of a subset of a collection of variables, irrespective of the values of the other variables. They are particularly important in multivariate analysis, where we have multiple interrelated random variables. The marginal distribution of a variable is the probability distribution of that variable alone, obtained by summing (in the discrete case) or integrating (in the continuous case) the joint distribution over the other variable.\nFor discrete variables, \\[ P(X = x) = \\sum_{y} P(X = x, Y = y). \\]\nFor continuous random variables, the marginal distribution of a variable is obtained by integrating the joint probability density function (PDF) over the range of the other variables. Suppose \\(f(x, y)\\) is the joint PDF of two continuous random variables \\(X\\) and \\(Y\\). The marginal PDFs of \\(X\\) and \\(Y\\) are given by:\n\nMarginal PDF of \\(X\\): \\[ f_X(x) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dy.\\]\nMarginal PDF of \\(Y\\): \\[ f_Y(y) = \\int_{-\\infty}^{\\infty} f(x, y) \\, dx.\\]\n\nThese integrals sum up the joint probability over all possible values of the other variable, effectively ‘collapsing’ the joint distribution into a single-variable distribution.\nMarginal distributions are significant in machine learning in the following way:\n\nFeature Analysis: Understanding the marginal distribution of each feature can provide insights into the data’s structure and inform preprocessing and feature engineering steps.\nModel Assumptions: Certain models assume independence between features, which can be examined by comparing joint and marginal distributions.\nProbabilistic Models: In probabilistic modeling, marginal distributions are crucial for making predictions and understanding the behavior of a system when only partial information is available.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\nfrom matplotlib.ticker import NullFormatter\n\n# Parameters for the bivariate Gaussian\nmean = [0, 0]\ncov = [[1, 0.5], [0.5, 1]]  # Covariance matrix\n\n# Create a grid of (x,y) values\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nX, Y = np.meshgrid(x, y)\n\n# Bivariate Gaussian distribution\nrv = multivariate_normal(mean, cov)\nZ = rv.pdf(np.dstack((X, Y)))\n\n# Marginal distributions\nmarginal_x = multivariate_normal(mean[0], cov[0][0])\nmarginal_y = multivariate_normal(mean[1], cov[1][1])\n\n# Create figure\nfig = plt.figure(figsize=(8, 8))\n\n# Define axes\nleft, width = 0.1, 0.65\nbottom, height = 0.1, 0.65\nspacing = 0.005\n\nrect_scatter = [left, bottom, width, height]\nrect_histx = [left, bottom + height + spacing, width, 0.2]\nrect_histy = [left + width + spacing, bottom, 0.2, height]\n\n# Add axes to the figure\nax_scatter = plt.axes(rect_scatter)\nax_histx = plt.axes(rect_histx)\nax_histy = plt.axes(rect_histy)\n\n# No labels for the additional axes\nax_histx.xaxis.set_major_formatter(NullFormatter())\nax_histy.yaxis.set_major_formatter(NullFormatter())\n\n# Contour plot\nax_scatter.contour(X, Y, Z)\n\n# Plot marginal distributions\nax_histx.plot(x, marginal_x.pdf(x))\nax_histy.plot(marginal_y.pdf(y), y)\n\n# Set limits and labels\nax_scatter.set_xlim(-3, 3)\nax_scatter.set_ylim(-3, 3)\nax_histx.set_xlim(ax_scatter.get_xlim())\nax_histy.set_ylim(ax_scatter.get_ylim())\nax_scatter.set_xlabel(\"X\")\nax_scatter.set_ylabel(\"Y\")\nax_histx.set_ylabel('Density')\nax_histy.set_xlabel('Density')\n\nplt.show()\n\n\n\n\n\n\nThe plot shows a joint distribution of two continuous random variables \\(X\\) and \\(Y\\) as a contour plot in the center and the marginal distributions along the X and Y axes. The use of additional axes for the marginal distributions alongside the contour plot offers a comprehensive view of how the marginals relate to the joint distribution.\n\n\n\n\n\n\n\n\n4.11.3 Conditional Distribution\nThe conditional distribution describes the probability of one random variable given the occurrence of another random variable. It essentially gives the distribution of a variable contingent on the value of another.\nFor discrete random variables \\(X\\) and \\(Y\\), the conditional probability mass function (PMF) of \\(X\\) given \\(Y = y\\) is defined as: \\[ P(X = x | Y = y) = \\frac{P(X = x, Y = y)}{P(Y = y)}.\\] provided \\(P(Y = y) &gt; 0\\).\nFor continuous random variables, the conditional probability density function (PDF) is defined similarly. If \\(f_{X,Y}(x, y)\\) is the joint PDF of \\(X\\) and \\(Y\\), and \\(f_Y(y)\\) is the marginal PDF of \\(Y\\), then the conditional PDF of \\(X\\) given \\(Y = y\\) is: \\[ f_{X|Y}(x | y) = \\frac{f_{X,Y}(x, y)}{f_Y(y)},\\] again, provided \\(f_Y(y) &gt; 0\\).\nWe will create a bivariate distribution using sampling from a Beta distribution and a Gaussian distribution, and then compute the conditional distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta, norm\n\n# Generating a bivariate distribution: Beta and Gaussian\nnp.random.seed(0)\nx = beta.rvs(2, 5, size=10000)  # Beta distribution\ny = x + norm.rvs(scale=0.1, size=10000)  # Gaussian distribution, dependent on x\n\n# Compute conditional distribution: P(Y|X=x0)\nx0 = 0.5  # Condition on this value of X\nindices = (x &gt; x0 - 0.05) & (x &lt; x0 + 0.05)\nconditional_samples = y[indices]\n\n# Plotting\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(x, y, 1, alpha=0.5)\nplt.axvline(x=x0, color='red', linestyle='--')\nplt.xlabel('X (Beta distributed)')\nplt.ylabel('Y (Gaussian distributed)')\nplt.title('Bivariate Distribution (Beta and Gaussian)')\n\nplt.subplot(1, 2, 2)\nplt.hist(conditional_samples, bins=20, density=True, alpha=0.7, color='g')\nplt.xlabel('Y values')\nplt.ylabel('Density')\nplt.title(f'Conditional Distribution P(Y|X={x0})')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure shows a joint Beta-Gaussian for (x,y). It also shows the conditional distribution of y for x = 0.5. This conditional distribution is visualized in the form of a histogram. The scatter plot of the bivariate distribution helps visualize the dependency between x and y.\n\n\n\n\n\n\n\n\n4.11.4 Covariance\nCovariance measures the joint variability of two random variables. It indicates the direction of the linear relationship between variables.\nFor discrete variables \\(X\\) and \\(Y\\) with mean values \\(\\mu_X\\) and \\(\\mu_Y\\), the covariance is: \\[\\text{Cov}(X, Y) = \\sum (x_i - \\mu_X)(y_i - \\mu_Y)P(x_i, y_i).\\]\nFor continuous variables, it is: \\[\\text{Cov}(X, Y) = \\int \\int (x - \\mu_X)(y - \\mu_Y)f_{X,Y}(x, y) dx dy.\\]\nPositive covariance indicates that higher values of one variable are associated with higher values of the other, and vice versa. Negative covariance indicates the opposite.\n\n\n4.11.5 Correlation\nCorrelation, specifically the Pearson correlation coefficient, measures the strength and direction of a linear relationship between two variables. Unlike covariance, it is dimensionless and normalized. It is defined as \\[\\rho_{X,Y} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y},\\] where \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\), respectively.The correlation coefficient ranges from -1 to 1. A value of 1 implies a perfect positive linear relationship, -1 implies a perfect negative linear relationship, and 0 implies no linear relationship.\nWe’ll use a simple dataset to compute and visualize the covariance and correlation between two variables:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate a simple dataset\nnp.random.seed(0)\nX = np.random.rand(1000)\nY = 2 * X + np.random.normal(0, 0.1, 1000)  # Y is linearly related to X\n\n# Compute Covariance\ncovariance = np.cov(X, Y)[0, 1]\n\n# Compute Correlation\ncorrelation = np.corrcoef(X, Y)[0, 1]\n\n# Print the computed values\nprint(\"Covariance between X and Y:\", covariance)\nprint(\"Correlation between X and Y:\", correlation)\n\n# Plotting the variables\nplt.scatter(X, Y, 1, alpha=0.7)\nplt.title(\"Scatter Plot of X vs Y\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.grid(True)\nplt.show()\n\n\n\nCovariance between X and Y: 0.1682441569776687\nCorrelation between X and Y: 0.9863442594104381\n\n\n\n\n\n\n\nThe script generates two variables, X and Y, where Y is linearly dependent on X with some added noise. It calculates the covariance and correlation between these variables and then plots them to show their relationship visually. The scatter plot provides a clear picture of how Y changes with X, and the covariance and correlation values quantify this relationship."
  },
  {
    "objectID": "random_variables.html#various-sampling-techniques-in-statistics-and-machine-learning",
    "href": "random_variables.html#various-sampling-techniques-in-statistics-and-machine-learning",
    "title": "4  Random Variables",
    "section": "4.12 Various Sampling Techniques in Statistics and Machine Learning",
    "text": "4.12 Various Sampling Techniques in Statistics and Machine Learning\nSampling techniques are methods used to select a subset of data from a larger dataset. In statistics and machine learning, different sampling methods are employed based on the nature of the data and the specific goals of the analysis. Here are some common sampling techniques:\n\n4.12.1 Simple Random Sampling\nSimple Random Sampling (SRS) is a fundamental statistical method where each member of a population has an equal chance of being selected for a sample. This method is highly valued for its ability to provide unbiased representations of a larger group. In SRS, the selection of one individual is completely independent of the selection of any other, ensuring that every subset of the population has an equal probability of being chosen. This randomness can be achieved through various means, such as using lottery methods or random number generators.\nOne of the primary advantages of SRS is its simplicity and the reduction of selection bias, making it an effective tool for obtaining representative samples. However, its practicality can be challenged in large populations due to logistical constraints, and its reliance on randomness does not always guarantee a perfectly representative sample, especially with smaller sample sizes. In the realm of machine learning and data analysis, SRS is often employed for tasks like creating balanced training and test datasets or for statistical estimations of population parameters. For example, in Python, the numpy.random.choice function can be utilized to perform SRS, allowing for the efficient selection of a random subset from a larger dataset.\n\n\n4.12.2 Stratified Sampling\nStratified Sampling is a statistical technique designed to improve the representativeness and efficiency of a sample by dividing the population into distinct subgroups, or strata, based on shared characteristics before sampling. This method ensures that each subgroup is adequately represented in the final sample, addressing the potential shortcomings of simple random sampling, especially in diverse populations. The process involves first identifying relevant strata within the population – these could be based on factors like age, income, education level, or any other relevant criteria. Once the strata are established, samples are drawn independently from each stratum, typically through simple random sampling or systematic sampling methods. The size of the sample from each stratum can be proportional to the stratum’s size in the population or can be equal-sized to give equal representation to each stratum regardless of its size in the population.\nStratified sampling is particularly useful in surveys and research studies where certain subgroups within a population may be underrepresented or have significant variability. By ensuring that these subgroups are adequately represented, stratified sampling enhances the accuracy and reliability of results, making it a powerful tool for obtaining a comprehensive understanding of the entire population.\n\n\n4.12.3 Cluster Sampling\nCluster sampling is a practical and efficient sampling method, especially useful in situations where the population is large and geographically dispersed. This technique involves dividing the entire population into groups or clusters, often based on geographical regions or other natural groupings. These clusters should ideally represent small-scale versions of the population. In cluster sampling, instead of selecting individual members from the entire population, a random sample of these clusters is chosen for the study, and all individuals within these selected clusters are included in the sample.\nThis approach can be particularly advantageous in large-scale surveys or field studies where reaching every individual is logistically challenging and cost-prohibitive. In Python, cluster sampling can be simulated by dividing a population dataset into clusters and then randomly selecting a subset of these clusters. The data from the chosen clusters are then combined to form the sample. This method reduces the cost and time of data collection significantly, making it a preferred choice for many large-scale research projects and surveys.\n\n\n4.12.4 Systematic Sampling\nSystematic sampling is a streamlined and efficient approach to sampling, particularly effective when dealing with large, ordered populations. This method starts by arranging the population in a sequence, after which a fixed interval or step size, known as the sampling interval, is determined. This interval is typically calculated by dividing the total population size by the desired sample size. The key to systematic sampling is the selection of a random starting point within the first interval, which ensures an element of randomness in the process. From this point, every \\(k^\\text{th}\\) member of the population is selected to be part of the sample, where \\(k\\) is the sampling interval.\nOne of the significant advantages of systematic sampling is its simplicity and the uniform coverage it provides across the population. It’s particularly useful when a comprehensive list of the population is available, and the sampling process needs to be quick and straightforward. However, it’s important to be aware of the potential for bias, especially if the population’s ordering has a hidden periodicity that aligns with the sampling interval. Despite this, systematic sampling is a popular choice in various fields due to its ease of implementation, which can be efficiently executed in Python and other programming environments. This sampling method strikes a balance between the randomness of simple random sampling and the convenience of having a structured approach to selecting a sample.\nHere’s a Python script illustrating these sampling techniques:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Population Data\npopulation = np.random.randn(1000)  # Normally distributed data\n\n# 1. Simple Random Sampling\nsimple_random_sample = np.random.choice(population, 100)\n\n# 2. Stratified Sampling\n# Assuming a binary characteristic for simplicity\nstrata_1 = population[population &lt; 0]  # Negative values\nstrata_2 = population[population &gt;= 0] # Non-negative values\nstratified_sample = np.concatenate([np.random.choice(strata_1, 50), np.random.choice(strata_2, 50)])\n\n# 3. Cluster Sampling\n# Dividing into 10 clusters\nclusters = np.array_split(population, 10)\nselected_clusters = np.random.choice(np.arange(10), 3, replace=False)  # Select 3 clusters\ncluster_sample = np.concatenate([clusters[i] for i in selected_clusters])\n\n# 4. Systematic Sampling\nstart = np.random.randint(0, 10)\nsystematic_sample = population[start::10]\n\n# Plotting\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\naxes[0, 0].hist(simple_random_sample, bins=20, color='skyblue')\naxes[0, 0].set_title('Simple Random Sampling')\naxes[0, 1].hist(stratified_sample, bins=20, color='lightgreen')\naxes[0, 1].set_title('Stratified Sampling')\naxes[1, 0].hist(cluster_sample, bins=20, color='salmon')\naxes[1, 0].set_title('Cluster Sampling')\naxes[1, 1].hist(systematic_sample, bins=20, color='gold')\naxes[1, 1].set_title('Systematic Sampling')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nHistograms visualizing various distributions of the samples obtained through each technique described above."
  },
  {
    "objectID": "random_variables.html#sampling-from-distributions",
    "href": "random_variables.html#sampling-from-distributions",
    "title": "4  Random Variables",
    "section": "4.13 Sampling from Distributions",
    "text": "4.13 Sampling from Distributions\nSampling from distributions is a crucial technique in statistics and machine learning, particularly in simulations, probabilistic modeling, and Bayesian inference. Various techniques can be used to sample from different kinds of distributions, each with its own advantages and applicable scenarios.\n\n4.13.1 Inverse Transform Sampling\nInverse Transform Sampling, also known as the Inverse Probability Integral Transform, is a method used to generate random samples from any probability distribution, given its cumulative distribution function (CDF). This technique is particularly useful when direct sampling from the distribution is challenging.\nIt works in the following way:\n\nCumulative Distribution Function (CDF): Start with the CDF of the desired probability distribution. The CDF, \\(F(x)\\), of a random variable \\(X\\) is defined as the probability that \\(X\\) will take a value less than or equal to \\(x\\).\nUniform Random Variable: Generate a random sample \\(U\\) from a standard uniform distribution, i.e., \\(U \\sim \\text{Uniform}(0,1)\\).\nInverse of CDF: Use the inverse of the CDF, \\(F^{-1}(u)\\), to transform the uniformly distributed sample \\(U\\) into a sample that follows the desired distribution. The inverse CDF method hinges on the principle that if \\(U\\) is a uniform random variable on the interval \\([0,1]\\), then the variable \\(X = F^{-1}(U)\\) has the desired distribution.\n\nThis method effectively transforms uniformly distributed data into data that follows any given distribution, using the inverse of the CDF of that distribution. It can be applied to any distribution, provided its CDF is known and is invertible. The method is computationally efficient and widely used, especially when other sampling methods are not feasible. Inverse transform sampling is extensively used in simulations, Monte Carlo methods, and various areas where generating random samples from specific distributions is required.\nSuppose we want to generate random samples from an exponential distribution using inverse transform sampling. Here’s how we might implement it in Python:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Number of samples to generate\nn_samples = 1000\n\n# Generate uniform samples\nuniform_samples = np.random.uniform(0, 1, n_samples)\n\n# Inverse CDF (percent point function) of the exponential distribution\nexponential_samples = expon.ppf(uniform_samples)\n\n# Plotting the generated samples\nplt.hist(exponential_samples, bins=30, density=True, alpha=0.7, label='Sampled Data')\nx = np.linspace(0, 4, 100)\nplt.plot(x, expon.pdf(x), label='Target Distribution', color='red')\nplt.title('Random Samples from an Exponential Distribution')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()\n\n\n\n\nSamples from exponential distribution using inverse CDF technique.\n\n\n\n\nThis example demonstrates the simplicity and power of inverse transform sampling for generating random samples from a specified distribution.\n\n\n4.13.2 Rejection Sampling\nRejection sampling, also known as the acceptance-rejection method, is a technique used to generate samples from a target probability distribution \\(f(x)\\) when direct sampling is difficult. The method involves using a simpler proposal distribution \\(g(x)\\) from which we can easily sample.\nIt works in the following way:\n\nChoose a Proposal Distribution \\(g(x)\\): Select a distribution \\(g(x)\\) that is easy to sample from and for which there exists a constant \\(M\\) such that \\(M \\cdot g(x) \\geq f(x)\\) for all \\(x\\).\nGenerate Samples: Sample a point \\(x\\) from \\(g(x)\\) and a uniform random number \\(u\\) from the interval \\([0, M \\cdot g(x)]\\).\nAccept or Reject: Accept the sample \\(x\\) if \\(u \\leq f(x)\\); otherwise, reject it and repeat the process.\nRepeat: Repeat steps 2 and 3 until the desired number of samples is obtained.\n\nThe efficiency of rejection sampling depends on how closely \\(g(x)\\) approximates \\(f(x)\\) and on the value of \\(M\\). If \\(M\\) is too large or if \\(g(x)\\) is a poor approximation of \\(f(x)\\), the rejection rate will be high, making the method inefficient.\nLet’s demonstrate rejection sampling in Python, where we sample from an exponential distribution using a uniform proposal distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import expon\n\n# Target distribution: Exponential distribution\ndef target_distribution(x):\n    return expon.pdf(x)\n\n# Proposal distribution: Uniform distribution\ndef proposal_distribution(x):\n    return np.ones_like(x)\n\n# Rejection Sampling\ndef rejection_sampling(target_dist, proposal_dist, M, size=1000):\n    samples = []\n    while len(samples) &lt; size:\n        x_proposal = np.random.uniform(0, 4, size=size)\n        u = np.random.uniform(0, M * proposal_dist(x_proposal), size=size)\n        accepted = x_proposal[u &lt;= target_dist(x_proposal)]\n        samples.extend(accepted.tolist())\n    return np.array(samples[:size])\n\n# Constants\nM = 1.5  # Choose M such that M * g(x) &gt;= f(x)\n\n# Generate samples\nsamples = rejection_sampling(target_distribution, proposal_distribution, M, size=1000)\n\n# Plotting the sampled data\nplt.hist(samples, bins=30, density=True, alpha=0.7, label='Sampled Data')\nx = np.linspace(0, 4, 100)\nplt.plot(x, expon.pdf(x), label='Target Distribution', color='red')\nplt.title('Rejection Sampling from an Exponential Distribution')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nSamples from the exponential distribution using rejection sampling and uniform proposal distribution.\n\n\n\n\n\n\nRejection sampling is particularly useful in scenarios where direct sampling from the target distribution is not feasible or too complex. The key is to choose an appropriate proposal distribution and constant \\(M\\) to ensure a reasonable acceptance rate.\n\n\n4.13.3 Importance Sampling\nImportance Sampling is a statistical technique used in probability and statistics, particularly in the fields of Monte Carlo simulation and data science. It is a method for estimating properties of a particular distribution, while only having samples from a different distribution. This technique is especially useful in scenarios where direct sampling from the desired distribution is challenging or inefficient.\nIt works in the following way:\n\nSelect a Proposal Distribution: Choose a distribution \\(g(x)\\) from which it is easier to sample (known as the proposal distribution). This distribution should ideally be similar to the target distribution \\(f(x)\\) but also ensure that where \\(f(x)\\) is significantly non-zero, \\(g(x)\\) is also non-zero.\nGenerate Samples: Draw samples from the proposal distribution \\(g(x)\\).\nWeighting Samples: Calculate weights for each sampled point. The weight for a sample \\(x\\) is given by the ratio of the target probability density to the proposal probability density at \\(x\\), i.e., \\(w(x) = \\frac{f(x)}{g(x)}\\).\nEstimate Target Expectation: The expected value of a function under the target distribution is estimated by the weighted average of that function over the samples drawn from the proposal distribution.\n\nImportance Sampling can be more efficient than simple random sampling, especially when the area of interest under the target distribution occupies a small part of the space. It allows for the estimation of properties of one distribution using samples from another, providing flexibility in situations where sampling from the target distribution is hard.\nHowever, it has some disadvantages. The efficiency of importance sampling heavily depends on the choice of the proposal distribution. A poor choice can lead to high variance in estimates and inefficient sampling. In some cases, a few samples may end up with very high weights, dominating the estimate and leading to high variance.\nThe following Python code demonstrates a simple example of importance sampling where we estimate the mean of a target exponential distribution using samples from a uniform distribution:\n\nimport numpy as np\nfrom scipy.stats import expon, uniform\n\n# Target distribution: Exponential\ntarget_dist = expon()\n\n# Proposal distribution: Uniform\nproposal_dist = uniform()\n\n# Number of samples\nn_samples = 10000\n\n# Draw samples from the proposal distribution\nsamples = proposal_dist.rvs(size=n_samples)\n\n# Compute weights\nweights = target_dist.pdf(samples) / proposal_dist.pdf(samples)\n\n# Estimate the mean of the target distribution\nestimated_mean = np.sum(weights * samples) / np.sum(weights)\n\nprint(\"Estimated Mean of the Target Distribution:\", estimated_mean)\n\nEstimated Mean of the Target Distribution: 0.41768407894841997\n\n\nIn this code, we use importance sampling to estimate the mean of an exponential distribution, using samples drawn from a uniform distribution. The weights are calculated based on the ratio of the probability densities of the target and proposal distributions at each sampled point. This method provides an estimation of the desired expectation under the target distribution.\n\n\n4.13.4 Markov Chain Monte Carlo (MCMC)\nMarkov Chain Monte Carlo (MCMC) is a set of algorithms used for sampling from probability distributions where direct sampling is difficult or impossible. MCMC enables the estimation of the distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution.\nThe Key concepts in MCMC are:\n\nMarkov Chain: A Markov chain is a stochastic model describing a sequence of possible events, where the probability of each event depends only on the state attained in the previous event. MCMC utilizes this property to generate samples.\nMonte Carlo Integration: MCMC methods use Monte Carlo integration, where random samples are generated and used to compute estimates of desired quantities, such as means, variances, or probabilities.\nConvergence to Target Distribution: The Markov chain is constructed so that it converges to the target distribution as its stationary distribution. After a ‘burn-in’ period, samples drawn from the Markov chain are used as samples from the target distribution.\n\nThe following are two popular MCMC algorithms:\n\nMetropolis-Hastings Algorithm: This algorithm generates a Markov chain using a proposal distribution and an acceptance criterion based on the ratio of the target densities.\nGibbs Sampling: A special case of the Metropolis-Hastings algorithm that is particularly useful when sampling from high-dimensional distributions. It samples successively from the conditional distribution of each variable.\n\nMCMC methods are widely used in various fields for estimating complex probability distributions, especially in Bayesian statistics for computing posterior distributions, in statistical physics, and in financial modeling.\nTheir strentgth is in versatility and ability to handle high dimensional spaces. MCMC methods can sample from virtually any probability distribution. They are particularly powerful in high-dimensional spaces where other sampling methods fail.\nHowever, determining whether the Markov chain has converged to the target distribution can be challenging. Also, these methods can be slow, especially for complex or high-dimensional distributions.\nIn Bayesian statistics, MCMC is used to estimate the posterior distribution of parameters. For instance, if the likelihood function is complex or the prior distribution is not conjugate to the likelihood, traditional analytical approaches may not work, and MCMC methods like Metropolis-Hastings or Gibbs Sampling can be employed to approximate the posterior distribution. These samples then allow for statistical inference about the parameters, such as estimating means, variances, or constructing credible intervals.\nMCMC methods, due to their flexibility and power, have become a cornerstone technique in modern statistical inference, particularly in scenarios where other methods are impractical or infeasible.\nImplementing a simple Markov Chain Monte Carlo (MCMC) algorithm in Python can be educational for understanding how MCMC works, especially in Bayesian inference contexts. We’ll demonstrate the Metropolis-Hastings algorithm, a widely used MCMC method, to sample from a probability distribution.\nFor simplicity, let’s assume we want to sample from a unimodal Gaussian distribution, but the same approach can be extended to more complex distributions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Target distribution (Gaussian in this example)\ndef target_distribution(x):\n    return norm.pdf(x, loc=0, scale=1)\n\n# Metropolis-Hastings Algorithm\ndef metropolis_hastings(target_pdf, initial_value, iterations, proposal_width):\n    x = initial_value\n    samples = [x]\n    for i in range(iterations):\n        # Proposal distribution: Gaussian centered at the current sample\n        x_proposal = np.random.normal(x, proposal_width)\n\n        # Calculate acceptance probability\n        acceptance_probability = min(target_pdf(x_proposal) / target_pdf(x), 1)\n\n        # Accept or reject the proposal\n        if np.random.random() &lt; acceptance_probability:\n            x = x_proposal\n\n        samples.append(x)\n\n    return np.array(samples)\n\n# Run the algorithm\ninitial_value = 0\niterations = 10000\nproposal_width = 0.5\nsamples = metropolis_hastings(target_distribution, initial_value, iterations, proposal_width)\n\n# Plotting the results\nplt.hist(samples, bins=30, density=True, alpha=0.6, label='Sampled Distribution')\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, norm.pdf(x), label='Target Gaussian Distribution', color='red')\nplt.title('Sampling from a Gaussian Distribution using MCMC')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\n\n\nSamples from a unimodal Gaussian using the Metropolis-Hastings algorithm.\n\n\n\n\nIn this script:\n\nWe define a target distribution as a standard Gaussian.\nThe metropolis_hastings function implements the MCMC algorithm. It generates a proposal for the next sample based on the current sample (Gaussian proposal distribution) and then decides whether to accept or reject this proposal based on the acceptance probability.\nWe run the algorithm for a number of iterations and collect the samples.\nThe resulting samples are plotted against the true Gaussian distribution for comparison.\n\nThis example demonstrates how MCMC can be used to sample from a given distribution. The Metropolis-Hastings algorithm, in particular, is effective for complex distributions where direct sampling is challenging."
  },
  {
    "objectID": "random_variables.html#distance-between-distributions.",
    "href": "random_variables.html#distance-between-distributions.",
    "title": "4  Random Variables",
    "section": "4.14 Distance between Distributions.",
    "text": "4.14 Distance between Distributions.\nIn machine learning, measuring the distance between probability distributions is crucial for various tasks, including statistical inference, clustering, and classification. These distances provide a way to quantify how similar or different two distributions are. Here are some commonly used distance metrics:\n\n4.14.1 Kullback-Leibler Divergence (KL Divergence)\nKL Divergence, named after Solomon Kullback and Richard Leibler, is a measure of how one probability distribution diverges from a second, reference probability distribution. It’s a concept from information theory, often used in statistics and machine learning.\nFor discrete distributions, the KL divergence of a distribution \\(P\\) from a distribution \\(Q\\) over the same probability space is defined as: \\[ D_{KL}(P \\parallel Q) = \\sum_{i} P(i) \\log\\left(\\frac{P(i)}{Q(i)}\\right).\\]\nFor continuous distributions, it is given by: \\[ D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) dx.\\]\nIt quantifies the amount of information lost when \\(Q\\) is used to approximate \\(P\\). A KL divergence of 0 indicates that the two distributions are identical (in the continuous case, almost everywhere).\nIt has the following properties:\n\nNon-Negativity: \\(D_{KL}(P \\parallel Q) \\geq 0\\), with equality if and only if \\(P = Q\\) (almost everywhere).\nNot Symmetric: \\(D_{KL}(P \\parallel Q) \\neq D_{KL}(Q \\parallel P)\\), meaning it’s not a distance in the traditional sense.\nNot a Distance Metric: Since it’s not symmetric and doesn’t satisfy the triangle inequality, it’s not a true metric.\n\nIt has the following applications in machine learning:\n\nModel Evaluation and Selection: In Bayesian statistics, KL divergence measures how much a model’s predicted distribution deviates from the true distribution, helping in model comparison and selection.\nVariational Inference: It’s used in variational inference as a part of the Evidence Lower Bound (ELBO), to approximate complex posterior distributions in Bayesian models.\nFeature Selection and Dimensionality Reduction: Methods like t-Distributed Stochastic Neighbor Embedding (t-SNE) use KL divergence to preserve small pairwise distances or similarities between high-dimensional data points when mapping them to a lower-dimensional space.\nTraining Generative Models: In generative models, like Variational Autoencoders (VAEs), KL divergence is used to regularize the encoder by penalizing deviations of its output distribution from a prior distribution, typically a Gaussian.\nInformation Theory: In information retrieval and natural language processing, it helps in quantifying the information gain between different stages of the model or different models.\n\nKL Divergence is particularly useful in scenarios where understanding the difference or the information gain between distributions is crucial. Its ability to quantify the ‘distance’ between probability distributions makes it a valuable tool for probabilistic modeling, particularly in Bayesian frameworks and in scenarios where approximation of complex distributions is necessary.\n\n\n4.14.2 Jensen-Shannon Divergence (JSD)\nJensen-Shannon Divergence is a method to measure the similarity between two probability distributions. It is a symmetrized and smoothed version of the Kullback-Leibler Divergence (KL Divergence). JSD overcomes some of the limitations of KL Divergence, particularly its asymmetry and the fact that it can be infinite if the two distributions being compared do not overlap.\nThe Jensen-Shannon Divergence between two probability distributions \\(P\\) and \\(Q\\) is defined as the average of the KL Divergences of \\(P\\) and \\(Q\\) from the mean distribution \\(M = \\frac{1}{2}(P + Q)\\): \\[ \\text{JSD}(P \\parallel Q) = \\frac{1}{2} D_{KL}(P \\parallel M) + \\frac{1}{2} D_{KL}(Q \\parallel M).\\]\nJSD quantifies the similarity between two probability distributions, with a value of 0 indicating identical distributions. It is always a finite value, bounded between 0 and 1, making it a more stable measure compared to KL Divergence.\nIt has the following properties:\n\nSymmetric: Unlike KL Divergence, JSD is symmetric, i.e., \\[\\text{JSD}(P \\parallel Q) = \\text{JSD}(Q \\parallel P).\\]\nBounded: The values of JSD range from 0 to 1, where 0 indicates identical distributions and 1 indicates maximal divergence.\n\nIt has the following applications in machine learning:\n\nModel Evaluation: In machine learning models, especially in natural language processing and information retrieval, JSD can be used to compare word distribution, topic distribution, or any other probability distributions that arise.\nGenerative Models: In training generative models, such as Generative Adversarial Networks (GANs), JSD can be used as an objective function to measure the difference between the generated data distribution and the real data distribution.\nClustering and Similarity Measurement: JSD is used in clustering algorithms to measure the similarity between different data points or clusters when the data is represented as probability distributions.\nFeature Selection: It can be applied in feature selection to measure the amount of information gained by including a particular feature, especially when features can be represented probabilistically.\n\nJSD’s symmetric and bounded nature makes it a versatile tool for comparing probability distributions in various machine learning tasks. Its ability to provide a smooth and finite measure of divergence is particularly useful in scenarios where KL Divergence might be too sensitive or undefined.\n\n\n4.14.3 Earth Mover’s Distance (EMD) or Wasserstein Distance\nEarth Mover’s Distance (EMD), also known as the Wasserstein Distance, is a measure of the distance between two probability distributions over a given space. The name “Earth Mover’s” stems from a practical analogy: it represents the minimum amount of “work” required to transform one distribution into the other, where “work” is quantified as the product of the amount of “mass” moved and the distance it’s moved.\nIn a discrete setting, if we have two distributions \\(P\\) and \\(Q\\) with the same total mass, EMD is the minimum cost of turning one distribution into the other, given a “ground distance” between individual points.\nIn continuous spaces, EMD is defined via the solution to the transportation problem from the field of optimization, where the goal is to find the most efficient way to move a distribution of mass to match another distribution.\nThe Wasserstein distance of order 1 (often used in practice) between two probability distributions \\(P\\) and \\(Q\\) is defined as: \\[ W(P, Q) = \\inf_{\\gamma \\in \\Pi(P, Q)} \\int_{X \\times Y} d(x, y) \\, d\\gamma(x, y),\\] where \\(\\Pi(P, Q)\\) is the set of all joint distributions \\(\\gamma(x, y)\\) whose marginals are \\(P\\) and \\(Q\\), and \\(d(x, y)\\) is a ground distance between points \\(x\\) and \\(y\\).\nUnlike other distances like KL divergence, EMD provides a more intuitive geometric interpretation. EMD is effective even when the compared distributions do not overlap, a scenario where other distances can fail or give misleading results.\nEMD’s ability to provide a meaningful and geometrically interpretable measure of distance between distributions has made it a valuable tool in various machine learning applications, particularly in areas where understanding the “transport” of mass or information between distributions is important.\nIt has the following applications in machine learning:\n\nOptimal Transport: EMD provides a natural way to compare distributions, making it useful in optimal transport problems, where the goal is to find the most efficient way to redistribute resources.\nGenerative Models: In training Generative Adversarial Networks (GANs), EMD can be used as a loss function (Wasserstein loss) to measure the distance between the distribution of generated data and the distribution of real data. It has been shown to improve the stability and performance of GAN training.\nDomain Adaptation: EMD is used in domain adaptation to measure the discrepancy between source and target domains, guiding the learning process to minimize this discrepancy.\nImage Retrieval and Computer Vision: EMD is applied in image retrieval systems to compare images represented as distributions of features. It’s also used in other computer vision tasks for comparing histograms and texture matching.\n\n\n\n4.14.4 Hellinger Distance\nThe Hellinger Distance is a metric used to quantify the similarity between two probability distributions. It’s derived from the Bhattacharyya coefficient and is used in various applications in statistics and machine learning.\nThe Hellinger Distance between two discrete or continuous probability distributions \\(P\\) and \\(Q\\) is defined as follows:\n\nFor Discrete Distributions: If \\(P\\) and \\(Q\\) are discrete distributions with the same support, \\[ H(P, Q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\sum_{i} (\\sqrt{P(i)} - \\sqrt{Q(i)})^2}.\\]\nFor Continuous Distributions: For continuous distributions with density functions \\(p(x)\\) and \\(q(x)\\), \\[ H(p, q) = \\frac{1}{\\sqrt{2}} \\sqrt{\\int (\\sqrt{p(x)} - \\sqrt{q(x)})^2 dx}.\\]\n\nThe Hellinger Distance is a measure of the “overlap” between two probability distributions. A value of 0 indicates identical distributions, while a value of 1 indicates no overlap. It is symmetric and bounded between 0 and 1, making it a true metric.\nAs a metric, it is non-negative, symmetric, and obeys the triangle inequality, making it a reliable measure for comparing distributions. The bounded range (from 0 to 1) offers an intuitive interpretation of results. Suitable for both discrete and continuous distributions and is particularly effective in scenarios where the distributions have non-overlapping support.\nIn summary, the Hellinger Distance provides a robust and interpretable way to measure the similarity or difference between two probability distributions, making it a valuable tool in various machine learning tasks, particularly those involving probabilistic modeling and analysis.\nIt has the following application in machine learning:\n\nClustering and Classification: In clustering algorithms and classification models, the Hellinger Distance can be used as a similarity measure between distributions. This is particularly useful when dealing with probability histograms or distributions as features.\nModel Evaluation: In probabilistic models, it can be used to compare the estimated probability distribution with the true distribution, providing a measure of model performance.\nFeature Selection: In scenarios where features are represented as distributions (such as word distributions in text data), the Hellinger Distance can aid in assessing the importance of features.\nKernel Methods: The Hellinger Distance can be used to construct kernels for SVMs and other kernel-based methods, especially in applications dealing with probability distributions.\n\n\n\n4.14.5 Total Variation Distance (TVD)\nTotal Variation Distance is a measure of the difference between two probability distributions. It is a metric that quantifies how much two distributions differ from each other.\nFor discrete distributions, the total variation distance between two probability distributions \\(P\\) and \\(Q\\) over a finite or countably infinite set is defined as: \\[ TVD(P, Q) = \\frac{1}{2} \\sum_{i} |P(i) - Q(i)|.\\]\nFor continuous distributions with probability density functions \\(p(x)\\) and \\(q(x)\\), TVD is given by: \\[ TVD(p, q) = \\frac{1}{2} \\int |p(x) - q(x)| dx.\\]\nTVD ranges from 0 to 1. A value of 0 indicates that the two distributions are identical, while a value of 1 indicates that the distributions are completely disjoint.\nIt has the following properties:\n\nSymmetric and Bounded: TVD is symmetric (i.e., \\(TVD(P, Q) = TVD(Q, P)\\)) and bounded between 0 and 1.\nInterpretable: It provides an easily interpretable measure of the distance between distributions.\n\nTVD is a robust metric for comparing distributions, providing a clear and bounded measure of their difference. It can be applied to both discrete and continuous distributions, making it widely applicable in various machine learning contexts.\nTotal Variation Distance is particularly valuable in scenarios where a straightforward and robust measure of distributional difference is required. Its bounded and symmetric nature makes it an intuitive and reliable tool for comparing probability distributions in various machine learning applications.\nIt has the following applications in machine learing:\n\nModel Comparison and Selection: In machine learning, especially in probabilistic modeling, TVD can be used to compare different models by measuring how close their output distributions are to the true distribution.\nGenerative Models: In the training of generative models, such as Generative Adversarial Networks (GANs), TVD can be used to measure the difference between the distribution of generated data and the real data distribution.\nReinforcement Learning: In reinforcement learning, TVD can help compare the policy distributions over different iterations, aiding in the analysis of policy convergence.\nStatistical Learning Theory: TVD plays a role in theoretical aspects of machine learning, such as understanding the behavior of learning algorithms and their convergence properties.\nFeature Engineering: In tasks involving feature engineering, TVD can be useful for comparing feature distributions across different classes or groups, helping to identify features that provide the most discriminative power."
  },
  {
    "objectID": "random_variables.html#functions-of-random-variables",
    "href": "random_variables.html#functions-of-random-variables",
    "title": "4  Random Variables",
    "section": "4.15 Functions of Random Variables",
    "text": "4.15 Functions of Random Variables\nIn probability and statistics, a function of a random variable is a new random variable formed by applying a function to an existing random variable. Functions of random variables are used extensively in statistics for hypothesis testing, in generating derived distributions, and in modeling relationships between variables. In machine learning, transformations of random variables are common in feature engineering and in the development of probabilistic models. Understanding functions of random variables allows one to comprehend how transformations affect the behavior and properties of the data, which is crucial in data analysis, inference, and predictive modeling.\nGiven a random variable \\(X\\) and a real-valued function \\(g\\), a new random variable \\(Y\\) can be defined as \\(Y = g(X)\\). The probability distribution of \\(Y\\) is determined by the function \\(g\\) and the distribution of \\(X\\). In general, we can have linear or nonlinear transformation of random variables,\n\nLinear Transformations: For example, \\(Y = aX + b\\), where \\(a\\) and \\(b\\) are constants. Such transformations linearly scale and shift the distribution of \\(X\\).\nNon-linear Transformations: For example, \\(Y = X^2\\) or \\(Y = e^X\\). These can significantly alter the shape of the original distribution.\n\n\n4.15.1 Calculating the Distribution of \\(Y = g(X)\\)\n\n4.15.1.1 Functions of Discrete Random Variables\nWhen dealing with discrete random variables, a function of a random variable results in another discrete random variable. The distribution of this transformed variable can be computed both analytically and numerically.\nExample: Analytical Approach\nIf \\(X\\) is a discrete random variable and \\(g\\) is a function, then \\(Y = g(X)\\) is also a discrete random variable. The PMF of \\(Y\\), denoted as \\(p_Y(y)\\), is computed by summing the probabilities of all values \\(x\\) of \\(X\\) that map to the same value \\(y\\) under the function \\(g\\). Mathematically, \\[ p_Y(y) = \\sum_{x: g(x) = y} p_X(x), \\] where \\(p_X(x)\\) is the PMF of \\(X\\).\nConsider a fair six-sided die. Let \\(X\\) be the outcome of a roll (1 to 6), and let \\(Y = X^2\\). We find the PMF of \\(Y\\) as follows:\n\nThe possible values of \\(Y\\) are \\(\\{1, 4, 9, 16, 25, 36\\}\\).\nFor each \\(y\\) in \\(\\{1, 4, 9, 16, 25, 36\\}\\), compute \\(p_Y(y)\\) using the formula. For example, \\(p_Y(1) = P(X=1) = 1/6\\), \\(p_Y(4) = P(X=2) = 1/6\\), and so on.\n\nExample: Numerical Approach\nWe can also apply a numerical approach to compute the distribution of a transformed variable. We first generate a large number of values of \\(X\\) and apply \\(g\\) to each value to get corresponding values of \\(Y\\). We then count the frequency of each value of \\(Y\\) and divide by the total number of simulations to approximate the PMF. For die roll problem, we first simulate rolling a die a large number of times, say 10,000 times. For each roll, we compute compute the square of the outcome. Then we count the frequency of each squared value and divide by 10,000 to estimate the PMF of \\(Y\\).\nThis is shown in the following Python code.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation of die roll\nnp.random.seed(0)  # for reproducibility\nrolls = np.random.randint(1, 7, 10000)  # simulate 10,000 die rolls\nsquared_rolls = rolls ** 2  # square each roll\n\n# Compute empirical PMF\nvalues, counts = np.unique(squared_rolls, return_counts=True)\npmf = counts / counts.sum()\n\n# Plotting the PMF\nplt.bar(values, pmf)\nplt.xlabel('Y values')\nplt.ylabel('Probability')\nplt.title('Empirical PMF of Y = X^2')\nplt.xticks(values)\nplt.show()\n\n\n\n\n\n\nEmprical determination of the distributions of a transformed variable.\n\n\n\n\n\n\nThis code simulates the square of a die roll and estimates its PMF numerically. It also visualizes the PMF, offering a clear understanding of the distribution of \\(Y = X^2\\) when \\(X\\) is the outcome of a die roll. This approach is particularly useful when the analytical computation is complex or infeasible.\n\n\n4.15.1.2 Functions of Continuous Random Variables\nWhen a function is applied to a continuous random variable, it results in another continuous random variable whose distribution can be derived from the original one. This transformation is key in statistical modeling and analysis.\nLet \\(X\\) be a continuous random variable with a probability density function (PDF) \\(f_X(x)\\). Consider a function \\(g\\) that maps \\(X\\) to \\(Y = g(X)\\). The PDF of \\(Y\\), denoted as \\(f_Y(y)\\), can be found using the change-of-variable formula, which involves the derivative of the inverse function of \\(g\\) and the PDF of \\(X\\). The formula is: \\[ f_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{d}{dy} g^{-1}(y) \\right|.\\]\nNote: This approach requires \\(g\\) to be a monotonic function (i.e., either strictly increasing or decreasing).\nExample: Analytical Approach\nLet \\(X\\) be uniformly distributed between 0 and 1 (\\(U(0,1)\\)), and consider the transformation \\(Y = e^X\\). The inverse function of \\(g(x) = e^x\\) is \\(g^{-1}(y) = \\ln(y)\\). The derivative of \\(g^{-1}(y)\\) is \\(\\frac{1}{y}\\). Since \\(X\\) is uniform, \\(f_X(x) = 1\\) for \\(x\\) in [0,1]. The PDF of \\(Y\\) is then \\(f_Y(y) = 1/y\\) for \\(y\\) in [1, \\(e\\)].\nExample: Numerical Approach The numerical approach consists of the following two steps:\n\nWe first generate a large number of samples from the PDF of \\(X\\) and apply \\(g\\) to obtain samples of \\(Y\\). This step involves two main actions: simulation and transformation.\n\n\nSimulation: First, we simulate a large number of values from the probability density function (PDF) of the random variable \\(X\\). This simulation process requires selecting a distribution that \\(X\\) follows and generating random samples from this distribution. This can be done using statistical software or programming languages like Python, which have built-in functions to generate random samples from various distributions.\nTransformation with Function \\(g(\\cdot)\\): Once we have these samples, the next step is to apply a function \\(g\\) to each sampled value. This function \\(g\\) transforms the original variable \\(X\\) into a new variable \\(Y\\). The nature of \\(g\\) can vary greatly depending on the analysis; it could be a simple linear function, a power function, an exponential function, or any other form of mathematical function. The crucial point is that each value of \\(X\\) is independently put through the function \\(g\\) to generate a corresponding value of \\(Y\\).\n\n\nAfter transforming the simulated values, the next step is to analyze the distribution of the transformed values. There are many ways of achieving this.\n\n\nEmpirical Distribution: This involves plotting the values of \\(Y\\) to visualize their distribution. One common method is constructing a histogram, where the range of \\(Y\\) values is divided into bins, and the frequency of values in each bin is plotted. This histogram provides a visual approximation of the PDF of \\(Y\\).\nDensity Estimation: To get a smoother estimate of the PDF of \\(Y\\), kernel density estimation (KDE) can be used. KDE is a non-parametric way to estimate the probability density function of a random variable and can provide a clearer view of the distribution, especially when the transformation \\(g\\) leads to a complex distribution.\n\nThe following Python code illustrates the steps involved in empirically determing the distribution of the square of a normal distribution. Let \\(X\\) be a standard normal variable. We first simulate a large number of values from \\(X\\) and compute \\(Y = X^2\\). We then plot a histogram of the \\(Y\\) values to approximate its distribution.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation of a standard normal variable\nnp.random.seed(0)\nx_values = np.random.normal(0, 1, 10000)  # 10,000 samples from N(0,1)\ny_values = x_values ** 2  # square each value\n\n# Plotting the empirical distribution of Y\nplt.hist(y_values, bins=50, density=True)\nplt.title('Empirical Distribution of Y = X^2')\nplt.xlabel('Y values')\nplt.ylabel('Density')\nplt.show()\n\n\n\n\n\n\nNumerical estimation of the distribution of \\(Y=X^2\\), where \\(X\\) is normally distributed.\n\n\n\n\n\n\nIn summary, numerical approaches are essential when the analytical derivation of \\(Y\\)’s distribution is complex or intractable. It provides a practical means to understand how transformations affect the distribution of a variable, which is a cornerstone in statistical analysis and data science.\n\n\n\n4.15.2 Expectation and Variance\nComputing the mean and variance of functions of random variables is a fundamental task in probability and statistics. The methods depend on whether the random variable is discrete or continuous, and the nature of the function applied to it.\n\n4.15.2.1 Mean of a Function of a Random Variable\n\nDiscrete Random Variables: Let \\(X\\) be a discrete random variable and \\(g(X)\\) be a function of \\(X\\). The mean or expected value of \\(g(X)\\) is computed as: \\[ \\mathbb{E}\\left[g(X)\\right] = \\sum_{x} g(x) \\cdot P(X = x).\\] Here, the sum is taken over all possible values of \\(X\\), and \\(P(X = x)\\) is the probability mass function (PMF) of \\(X\\).\nContinuous Random Variables: For a continuous random variable \\(X\\) with a probability density function (PDF) \\(f_X(x)\\), the mean of \\(g(X)\\) is: \\[ \\mathbb{E}\\left[g(X)\\right] = \\int_{-\\infty}^{\\infty} g(x) \\cdot f_X(x) \\, dx.\\] The integral is taken over the entire range of \\(X\\).\n\n\n\n4.15.2.2 Variance of a Function of a Random Variable\nThe variance of \\(g(X)\\) measures how much \\(g(X)\\) is expected to deviate from its mean. It is calculated as: \\[ Var[g(X)] = \\mathbb{E}\\left[g(X)^2\\right] - (\\mathbb{E}\\left[g(X)\\right])^2.\\]\n\nFor Discrete Random Variables: Compute \\(\\mathbb{E}\\left[g(X)^2\\right]\\) as: \\[ \\mathbb{E}\\left[g(X)^2\\right] = \\sum_{x} g(x)^2 \\cdot P(X = x), \\] and then, use the formula for variance.\nFor Continuous Random Variables: Compute \\(\\mathbb{E}\\left[g(X)^2\\right]\\) by integrating: \\[\\mathbb{E}\\left[g(X)^2\\right] = \\int_{-\\infty}^{\\infty} g(x)^2 \\cdot f_X(x) \\, dx, \\] and then apply the variance formula.\n\n\n\n4.15.2.3 Example\nConsider \\(X\\) is a random variable representing the roll of a fair six-sided die, and \\(g(X) = X^2\\) (the square of the roll).\n\nMean of \\(g(X)\\): \\[\\mathbb{E}\\left[g(X)\\right] = \\sum_{x=1}^{6} x^2 \\cdot \\frac{1}{6} = \\frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6}.\\]\nVariance of \\(g(X)\\): First, compute \\(\\mathbb{E}\\left[g(X)^2\\right] = \\sum_{x=1}^{6} x^4 \\cdot \\frac{1}{6}\\). Then, use \\(Var[g(X)] = \\mathbb{E}\\left[g(X)^2\\right]- (\\mathbb{E}\\left[g(X)\\right])^2\\)."
  },
  {
    "objectID": "random_variables.html#probabilistic-programming-languages",
    "href": "random_variables.html#probabilistic-programming-languages",
    "title": "4  Random Variables",
    "section": "4.16 Probabilistic Programming Languages",
    "text": "4.16 Probabilistic Programming Languages\nA probabilistic programming language (PPL) is a high-level programming language designed to describe probabilistic models and perform statistical inference within those models. Essentially, it allows us to define models in terms of probability distributions and then automatically perform complex computations like Bayesian inference.\nProbabilistic Programming Languages (PPLs) and are particularly useful in machine learning for several reasons:\n\nPPLs allow for defining complex probabilistic models where functions of random variables can represent various stochastic processes or data generation mechanisms. For example, a function might transform a Gaussian random variable to model non-normal data.\nFunctions of random variables can be used to incorporate domain knowledge or specific hypotheses into a model. For example, a linear combination of random variables might represent a regression model, while more complex functions can represent non-linear relationships.\nPPLs provide the flexibility to define arbitrary functions of random variables, making them highly expressive for modeling a wide range of phenomena. This includes transformations, nonlinear relationships, or hierarchical structures.\n\nKey features of PPLs include:\n\nModel Specification: Simplifies the specification of complex probabilistic models, often using syntax similar to standard programming languages.\nInbuilt Inference Engines: PPLs typically come with in-built algorithms for performing inference, such as Markov Chain Monte Carlo (MCMC), variational inference, or other sampling methods.\nFlexibility and Extensibility: They allow for easy modification and extension of models, making them suitable for a wide range of applications from simple statistical tasks to complex machine learning models.\n\nCurrent state of the art packages for PPL are:\n\nStan: A state-of-the-art platform for statistical modeling and high-performance statistical computation.\nPyMC3: A Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms.\nTensorFlow Probability (TFP): A Python library built on TensorFlow for probabilistic reasoning and statistical analysis.\nEdward: A Python library for probabilistic modeling, inference, and criticism, integrated with TensorFlow.\nJAGS/BUGS: Software for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation.\n\nHere’s a simple example using TensorFlow Probability (TFP) to infer the distribution of \\(Y = e^X\\), where \\(X\\) is normal.\n\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntfd = tfp.distributions\n\n# Define a standard normal distribution for X\ndist_X = tfd.Normal(loc=0., scale=1.)\n\n# Define a sample size\nsample_size = 10000\n\n# Sample from X\nsamples_X = dist_X.sample(sample_size)\n\n# Apply the transformation g(X) = exp(X) to get Y\nsamples_Y = tf.exp(0.5*samples_X)\n\n# # Convert to numpy for plotting\n# samples_Y_np = samples_Y.numpy()\n\n# Plot the histogram of X and Y\nplt.subplot(1,2,1);\nplt.hist(samples_X.numpy(), bins=50, density=True)\nplt.title('Histogram of X (normal distribution)')\nplt.xlabel('X')\nplt.ylabel('Density')\n\nplt.subplot(1,2,2);\nplt.hist(samples_Y.numpy(), bins=50, density=True)\nplt.title('Histogram of Y = exp(X)')\nplt.xlabel('Y')\nplt.ylabel('Density')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nUse of TensorFlow Probability to estimate distribution of \\(Y=e^X\\), where \\(X\\) is normal.\n\n\n\n\n\n\nThe above script uses TensorFlow and TensorFlow Probability to create a standard normal distribution, samples from it, applies a nonlinear transformation \\(Y=e^X\\), and then plots the histogram of the transformed samples. This is a practical demonstration of how functions of random variables can be explored within the TensorFlow framework, which is especially useful for complex probabilistic models in machine learning. We can use this approach to determine transformation of distributions by more complex functions, such as neural networks."
  },
  {
    "objectID": "random_processes.html#discrete-time-processes",
    "href": "random_processes.html#discrete-time-processes",
    "title": "5  Random Processes and Sequences",
    "section": "5.1 Discrete-Time Processes",
    "text": "5.1 Discrete-Time Processes\nDiscrete-time processes are a type of stochastic or random process where the set of indices (usually representing time) is discrete. This means that the process is observed or defined only at specific, separated points in time.\nKey characteristics of discrete-time processes are\n\nTime Index: In discrete-time processes, the time index, often denoted as \\(t\\), takes values from a discrete set, like integers. It’s as if the process is observed at distinct time steps (e.g., daily stock prices).\nRandom Variables: At each time index, the process is described by a random variable. These variables can be independent or have some form of dependency.\nExamples: Common examples of discrete-time processes include time series in economics, daily weather records, and signal processing data.\n\nThe following are three important types of discete-time processes:\n\n5.1.1 Bernoulli Process\nA Bernoulli Process is a fundamental type of discrete-time stochastic process. It is a sequence of independent and identically distributed (i.i.d.) random variables that take values from a binary set, typically {0, 1}, representing two outcomes, commonly termed “success” and “failure.”\nLet’s denote the random variables in the sequence as \\(X_1, X_2, ..., X_n\\). Each \\(X_i\\) is a Bernoulli random variable, which means:\n\n\\(X_i\\) takes the value 1 with probability \\(p\\) (success).\n\\(X_i\\) takes the value 0 with probability \\(1 - p\\) (failure).\n\nSo, mathematically, the probability mass function (PMF) of each \\(X_i\\) is given by:\n\\[ P(X_i = x) = p^x (1 - p)^{1 - x},\\]\nfor \\(x \\in \\{0, 1\\}\\), where \\(p\\) is the probability of success.\nNotably:\n\nEach trial (or each random variable in the sequence) is independent of the others.\nEach random variable follows the same Bernoulli distribution with the same probability of success \\(p\\).\nThe process is defined at discrete time intervals (e.g., coin flips in a series of experiments).\n\nSome examples where Bernoulli process is used to model sequences are:\n\nCoin Flipping: A sequence of coin flips where each flip is independent, and the probability of getting heads (success) is \\(p\\).\nQuality Control: In manufacturing, where each item produced either passes (success) or fails (failure) quality control with some probability.\nBinary Data Modeling: In scenarios where data can be modeled as a sequence of binary outcomes, like click/no-click, buy/not buy in user behavior analysis.\n\nThe number of successes in a sequence of Bernoulli trials can be modeled by a Binomial distribution. If you conduct \\(n\\) Bernoulli trials (each with success probability \\(p\\)), the total number of successes follows a Binomial distribution with parameters \\(n\\) and \\(p\\).\nA Bernoulli Process is a simple yet powerful model for representing scenarios where events or outcomes are binary and independent, a common situation in many practical applications in statistics and machine learning.\nSimulating a Bernoulli process in Python is straightforward and can be done using the numpy library, which offers efficient array operations and random number generation capabilities. Here’s an example of how to simulate a Bernoulli process:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef bernoulli_process(p, n):\n    \"\"\"\n    Simulate a Bernoulli process.\n    \n    Parameters:\n        p (float): Probability of success for each trial.\n        n (int): Number of trials.\n\n    Returns:\n        numpy.ndarray: An array representing the Bernoulli process (successes and failures).\n    \"\"\"\n    # Each trial results in 1 (success) with probability p and 0 (failure) with probability 1-p\n    return np.random.binomial(1, p, n)\n\n# Parameters for the Bernoulli process\nprobability_of_success = 0.5  # Example probability (like flipping a fair coin)\nnumber_of_trials = 50        # Total number of trials\n\n# Simulate the Bernoulli process\nprocess = bernoulli_process(probability_of_success, number_of_trials)\n\n# Plotting the results\nplt.figure(figsize=(10, 4))\nplt.stem(process)\nplt.title('Bernoulli Process Simulation')\nplt.xlabel('Trial')\nplt.ylabel('Outcome (0=Failure, 1=Success)')\nplt.yticks([0, 1]);\nplt.show()\n\n\n\n\n\n\nSequence of events following a Bernoulli distribution. This models occurence of head or tail from flipping a fair coin.\n\n\n\n\n\n\nIn this script, the bernoulli_process function simulates the Bernoulli process. It uses numpy.random.binomial with 1 trial for each step, which effectively makes it a Bernoulli trial. We define the probability of success (e.g., 0.5 for a fair coin toss) and the number of trials. The function returns an array of 0s and 1s, representing failures and successes, respectively. The resulting Bernoulli process is visualized using a stem plot, where each stem represents the outcome of a single trial. This code demonstrates a basic simulation of a Bernoulli process and is useful for understanding the behavior of binary outcomes over a series of independent trials.\n\n\n5.1.2 Random Walk\nA random walk is a mathematical model that describes a path consisting of a succession of random steps. This model is widely used in various fields such as physics, finance, biology, and computer science.\nIn its simplest form, a one-dimensional random walk can be defined as follows:\n\nInitial Position: Start at a fixed point, usually \\(x = 0\\).\nRandom Steps: At each time step \\(t\\), take a step either to the left or to the right. The direction of each step is determined randomly.\nDiscrete Steps: If the step is to the right, the position increases by 1 (i.e., \\(x_{t+1} = x_t + 1\\)). If the step is to the left, the position decreases by 1 (i.e., \\(x_{t+1} = x_t - 1\\)).\n\nMathematically, if \\(S_n\\) denotes the position after \\(n\\) steps, and each step \\(X_i\\) is a random variable that takes the value +1 or -1 with equal probability, then:\n\\[S_n = \\sum_{i=1}^n X_i.\\]\nNotably:\n\nThe steps \\(X_i\\) are independent of each other.\nEach step has an equal probability of being +1 or -1.\nThe process takes place in discrete time steps and discrete spatial steps.\nThe expected position after \\(n\\) steps is 0, as the walk is symmetric.\nThe variance after \\(n\\) steps is \\(n\\), reflecting the increasing uncertainty about the position over time.\nThe concept extends to higher dimensions, where at each step, the move is made in one of the possible directions in the plane or space.\n\nRandom walks are used to model many phenomenon. For example:\n\nPhysics: Modeling diffusion processes, such as the movement of molecules.\nFinance: Stock price movements are often modeled as random walks.\nBiology: Pathways of motile organisms or molecules within cells.\nComputer Science: Algorithms for searching or optimization.\n\nA random walk is a fundamental stochastic process and provides a simple yet powerful model for various phenomena. It is a cornerstone model in the study of stochastic processes and has a profound impact on our understanding of random behavior in natural and artificial systems.\nHere’s a Python example that simulates a one-dimensional random walk:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef random_walk(steps):\n    # Steps can be -1 or 1\n    walk_steps = np.random.choice([-1, 1], size=steps)\n    # Cumulative sum to simulate the walk\n    walk = np.cumsum(walk_steps)\n    return walk\n\n# Number of steps\nn_steps = 100\n\n# Simulate a random walk\nwalk = random_walk(n_steps)\n\n# Plot the random walk\nplt.figure(figsize=(10, 6))\nplt.plot(walk)\nplt.title('1D Random Walk')\nplt.xlabel('Steps')\nplt.ylabel('Position')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nSimulation of a random walk as random -1s and 1s, representing moves in opposite directions. The position at each step is the cumulative sum of the steps taken up to that point.\n\n\n\n\n\n\n\n\n5.1.3 Markov Chain\nA Markov Chain is a stochastic process that undergoes transitions from one state to another within a finite or countably infinite number of possible states. It is characterized by the property that the future state depends only on the current state, not on the sequence of events that preceded it. This property is known as the Markov property or memorylessness.\nMathematically, a Markov Chain is defined by:\n\nStates: A set of states \\(S = \\{s_1, s_2, ..., s_n\\}\\).\nTransition Probability: The probability of moving from one state to another. For a state \\(s_i\\) and \\(s_j\\), the transition probability is denoted as \\(P_{ij}\\), which is the probability of transitioning from state \\(s_i\\) to state \\(s_j\\).\nTransition Matrix: A matrix \\(P\\) where each element \\(P_{ij}\\) represents the transition probability from state \\(s_i\\) to state \\(s_j\\). For a Markov chain, the sum of the probabilities in each row of the matrix is 1, i.e., \\(\\sum_{j} P_{ij} = 1\\) for all \\(i\\).\n\nThey have the following properties:\n\nDiscrete Time Steps: The process moves from one state to another at discrete time steps.\nMemorylessness: The next state depends only on the current state, not on the past states.\n\nThere are three kinds of Markov Chains:\n\nIrreducible: A Markov chain is irreducible if it is possible to get to any state from any state.\nPeriodic/Aperiodic: A state has a period if the chain can return to the state only at multiples of some integer. A Markov chain is aperiodic if it has no periodic states.\nTransient and Recurrent States: A state is transient if the chain can leave it and never return; otherwise, it is recurrent.\n\nMarkov Chains are used in various fields, including economics, genetics, game theory, and computer science.\nAs an example, consider the following the Markov chain.\n\n\n\n\n\n\nA three state Markov chain.\n\n\n\n\nThe transition matrix is given by \\[\n\\boldsymbol{M} = \\begin{bmatrix}\n0.1 & 0.6& 0.3\\\\\n0.4 & 0.2& 0.4\\\\\n0.3& 0.3& 0.4\n\\end{bmatrix}.\n\\] If \\(\\boldsymbol{p}(k) = \\begin{bmatrix} p_0(k) & p_1(k) & p_2(k) \\end{bmatrix}\\) is the probability (row) vector at time \\(k\\), defined by the propabilities \\(p_0(k), p_1(k),\\) and \\(p_2(k)\\) that the system is in state \\(S_0\\), \\(S_1\\), and \\(S_2\\) respectively, the probabilities in the next time step is given by \\[\n\\boldsymbol{p}(k+1) = \\boldsymbol{p}(k)\\boldsymbol{M}.\n\\] We start with \\(\\boldsymbol{p}(0)\\) and iterate \\(\\boldsymbol{p}(k)\\) with the above update equation.\nHere’s a Python script to simulate the Markov Chain shown above:\n\nimport numpy as np\nM = np.array([[0.1, 0.6, 0.3],\n              [0.4, 0.2, 0.4],\n              [0.3, 0.3, 0.4]])\n\np = np.array([1.,0.,0.])\nnp.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n\nfor iter in range(11):\n  print(f\"k={iter}, p={p}\")\n  p = p@M\n\nk=0, p=[ 1.000  0.000  0.000]\nk=1, p=[ 0.100  0.600  0.300]\nk=2, p=[ 0.340  0.270  0.390]\nk=3, p=[ 0.259  0.375  0.366]\nk=4, p=[ 0.286  0.340  0.374]\nk=5, p=[ 0.277  0.352  0.371]\nk=6, p=[ 0.280  0.348  0.372]\nk=7, p=[ 0.279  0.349  0.372]\nk=8, p=[ 0.279  0.349  0.372]\nk=9, p=[ 0.279  0.349  0.372]\nk=10, p=[ 0.279  0.349  0.372]\n\n\nIn the code we started from the state \\(S_0\\), therefore \\(\\boldsymbol{p}(0) = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix}\\). The probabilities associated with each state in the first 10 steps are shown.\nThus in Markov Chain simulation, we propagate the probabilities associated with states since the system can be in any state with the given probability. In this case, we see that the system reaches a steady-state probability of \\(\\boldsymbol{p}(\\infty)=\\begin{bmatrix} 0.279 & 0.349 & 0.372 \\end{bmatrix}\\).\nThe steady-state probabilities can be computed from the eigen-values of the transition matrix. This is shown in the next Python code.\n\nimport numpy as np\n\n# Define the transition matrix\ntransition_matrix = np.array([[0.1, 0.6, 0.3],\n                              [0.4, 0.2, 0.4],\n                              [0.3, 0.3, 0.4]])\n\n# Compute left eigenvectors and eigenvalues\neigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)\n\n# Find the left eigenvector corresponding to the eigenvalue 1 (steady state)\nsteady_state_index = np.argmin(np.abs(eigenvalues - 1))\nsteady_state_vector = left_eigenvectors[:, steady_state_index].real\n\n# Normalize the steady state vector\nsteady_state_vector /= steady_state_vector.sum()\n\nprint(f\"The steady-state probabilities are: {steady_state_vector}\")\n\nThe steady-state probabilities are: [ 0.279  0.349  0.372]\n\n\nThe steady state vector for the given Markov Chain, calculated using the left eigenvectors and eigenvalues of the transition matrix, is approximately: \\(\\begin{bmatrix} 0.279 & 0.349& 0.372\\end{bmatrix}\\), which matches the simulated output. Therefore, we don’t need to simulate a Markov chain to determine the steady-state probabilities.\nThe steady-state probability vector indicates that, in the long run, the system will spend approximately 27.9% of the time in state \\(S_0\\), 34.9% in state \\(S_1\\), and 37.2% in state \\(S_2\\). This steady state is reached regardless of the initial state of the system, given the chain is ergodic (see below for a discussion on ergodic processes)."
  },
  {
    "objectID": "random_processes.html#continuous-time-processes",
    "href": "random_processes.html#continuous-time-processes",
    "title": "5  Random Processes and Sequences",
    "section": "5.2 Continuous-Time Processes",
    "text": "5.2 Continuous-Time Processes\nA continuous time random process is a collection of random variables ordered in time, where time is considered as a continuous variable. This type of process is used to model systems or phenomena that evolve or change state in a continuous manner over time.\nKey characteristics of a continuous time random process are:\n\nThe process is defined for every instant in a continuous time interval. For instance, \\(X(t)\\) for \\(t\\) in the interval \\([0, \\infty)\\) or \\([a, b]\\).\nEach \\(X(t)\\) is a random variable representing the state of the process at time \\(t\\).\nThe state space, which is the set of possible values of \\(X(t)\\), can be discrete or continuous.\nFor each \\(t\\), \\(X(t)\\) has a probability distribution, and the joint distribution of \\(X(t_1), ..., X(t_n)\\) for any \\(t_1, ..., t_n\\) is defined.\nThe mean function \\(m(t) = \\mathbb{E}\\left[X(t)\\right]\\) and covariance function \\[R(s, t) = \\mathbb{E}\\left[(X(s) - m(s))(X(t) - m(t))\\right],\\] describe the first and second moments of the process.\n\nMany continuous time processes are analyzed and solved using differential equations.\nContinuous time random processes are essential in modeling and analyzing systems that exhibit random behavior in a continuous temporal framework. They provide a foundation for understanding and predicting the behavior of a wide range of real-world phenomena. Some examples include:\n\nTime Series Analysis: Modeling and predicting stock prices, weather patterns, and other phenomena that change continuously over time.\nSignal Processing: Analyzing and filtering continuous signals in communications and audio processing.\nRegression Models: Gaussian processes are used for non-parametric regression, providing uncertainty measurements along with predictions.\n\nWe next describe some important types of continuous time random processes.\n\n5.2.1 Brownian Motion (Wiener Process)\nBrownian Motion, also known as the Wiener Process, is a fundamental continuous-time stochastic process in mathematics, physics, and finance. It models random motion, often used to represent the unpredictable movement of particles in fluid or the erratic fluctuations in financial markets.\nKey characteristics are:\n\nContinuous Path: Unlike discrete processes, Brownian motion has a continuous path with continuous time parameter.\nNormal Increments: The increments of the process are normally distributed. For any two times \\(t\\) and \\(s\\), the increment \\(W(t) - W(s)\\) is normally distributed with mean 0 and variance \\(|t-s|\\).\nIndependent Increments: Increments over non-overlapping intervals are independent.\nStarts at Zero: The process typically starts at 0, i.e., \\(W(0) = 0\\).\n\nSome applications include:\n\nPhysics: Modeling the random motion of particles suspended in a fluid (a phenomenon observed by botanist Robert Brown).\nFinance: Used in the Black-Scholes model for option pricing and to model stock prices in the Efficient Market Hypothesis.\nMathematics: Fundamental in the study of stochastic processes and calculus.\n\nHere’s a simple Python script to simulate Brownian Motion using normal increments:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_brownian_motion(steps, delta_t):\n    \"\"\"\n    Simulate Brownian Motion (Wiener Process).\n\n    Parameters:\n        steps (int): Number of steps in the simulation.\n        delta_t (float): Time interval between steps.\n\n    Returns:\n        numpy.ndarray: Simulated path of Brownian Motion.\n    \"\"\"\n    # Normal increments with mean 0 and variance delta_t\n    increments = np.random.normal(0, np.sqrt(delta_t), steps)\n    # Cumulative sum to simulate the path\n    return np.cumsum(increments)\n\n# Parameters for the simulation\nnumber_of_steps = 1000\ntime_interval = 0.01\n\n# Simulate Brownian Motion\npath = simulate_brownian_motion(number_of_steps, time_interval)\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.plot(path)\nplt.title('Brownian Motion Simulation')\nplt.xlabel('Time Steps')\nplt.ylabel('Position')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nSimulation of a Brownian motion.\n\n\n\n\n\n\nIn this script, Brownian motion is simulated as a cumulative sum of normal increments. Each increment is normally distributed with mean 0 and variance proportional to the time interval delta_t. The simulate_brownian_motion function generates the path of Brownian motion over a specified number of steps and time interval. The resulting path is plotted to visualize the typical “random walk” pattern of Brownian motion.\n\n\n5.2.2 Poisson Process\nA Poisson Process is a fundamental stochastic process used extensively in various fields, including mathematics, physics, finance, and queueing theory. It models the occurrence of random events over time and is particularly useful for situations where events happen independently of each other.\nKey characteristics are:\n\nThe process models random events (like phone calls to a call center, decay of radioactive particles, or arrival of customers at a store) happening over time.\nEvents occur independently of each other. The occurrence of one event does not affect the probability of another event occurring.\nThe probability of an event occurring in a fixed interval of time is the same for all corresponding intervals of the same length.\nIf the rate (average number of events per time unit) is constant the the Poisson process is ordinary or homogeneous. Else, the process is non-homogeneous.\nIn a time interval of length \\(t\\), the number of events \\(N(t)\\) follows a Poisson distribution with parameter \\(\\lambda t\\), where \\(\\lambda\\) is the rate of the process.\nThe probability of observing \\(k\\) events in time \\(t\\) is given by: \\[ P(N(t) = k) = \\frac{e^{-\\lambda t} (\\lambda t)^k}{k!}.\\]\nThe time between consecutive events follows an exponential distribution with rate \\(\\lambda\\).\nPoisson processes have the memoryless property, meaning that the probability of an event occurring in the future is independent of how much time has already elapsed.\nIt is often used to model rare events, where the actual number of occurrences in a fixed time interval is low relative to the potential number of occurrences.\n\nSome common applications of Poisson process include:\n\nQueueing Systems: Modeling arrival of customers, calls, or requests in a system.\nTelecommunications: Describing the arrival of packets or messages in a network.\nFinance: Modeling the occurrence of certain types of financial transactions or market events.\nBiology and Medicine: Modeling random events like mutation occurrences or spread of diseases.\nPhysics: Used in the study of decay processes of unstable particles.\n\nIn summary, the Poisson Process is a powerful tool for modeling and understanding phenomena where events occur randomly and independently over time. It serves as a foundation for more complex stochastic models and is integral to the field of stochastic processes.\nHere is a Python code to simulate a Poisson process.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_poisson_process(rate, duration):\n    \"\"\"\n    Simulate a Poisson process.\n\n    Parameters:\n        rate (float): The average rate (lambda) of events per unit time.\n        duration (float): The total time duration for the simulation.\n\n    Returns:\n        list: Times at which events occur.\n    \"\"\"\n    # The time between events follows an exponential distribution\n    inter_event_times = np.random.exponential(1/rate, int(rate * duration))\n    # The actual event times are the cumulative sum of the inter-event times\n    event_times = np.cumsum(inter_event_times)\n    \n    # Filter out times beyond the duration\n    event_times = event_times[event_times &lt;= duration]\n    \n    return event_times\n\n# Parameters\nrate = 2  # Average rate of 2 events per unit time\nduration = 10  # Total duration of 10 time units\n\n# Simulate the Poisson process\nevent_times = simulate_poisson_process(rate, duration)\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.plot(event_times, np.arange(len(event_times)), drawstyle='steps-post', marker='o')\nplt.title('Simulation of a Poisson Process')\nplt.xlabel('Time')\nplt.ylabel('Number of Events')\nplt.grid(True)\nplt.show()\n\n\n\n\nThe plot shows a Poisson process. The y-axis represents the cumulative number of events that have occurred. Each step in the plot (marked with an ‘o’) indicates the occurrence of an event.\n\n\n\n\nThe Python script simulates a Poisson process. In this simulation:\n\nThe Poisson process is characterized by an average rate (lambda) of 2 events per unit time.\nThe process is simulated over a duration of 10 time units.\nThe times at which events occur are generated based on the exponential distribution of inter-event times, which is a characteristic feature of the Poisson process.\n\n\n\n5.2.3 Gaussian Process\nA Gaussian Process (GP) is a powerful, flexible probabilistic model used in machine learning, particularly for tasks like regression, classification, and optimization. It is essentially a collection of random variables, any finite number of which have a joint Gaussian distribution.\nKey concepts involved are:\n\nUnlike a Gaussian distribution defined over a discrete set of points, a GP is defined over a continuous domain, like time or space.\nA GP can be thought of as a distribution over functions. It provides a way to specify prior beliefs about the function being modeled (e.g., smoothness, periodicity).\nA GP is fully specified by its mean function \\(m(x)\\) and covariance function \\(k(x, x')\\), also known as the kernel. The mean function represents the average value of the function at point \\(x\\), and the covariance function defines the similarity between the function values at different points \\(x\\) and \\(x'\\).\n\nMathematically, a GP is defined as:\n\\[ f(x) \\sim \\mathcal{GP}(m(x), k(x, x')),\\]\nwhere:\n\n\\(f(x)\\) is a random function.\n\\(m(x)\\) is the mean function, often assumed to be zero (or another constant) for simplicity.\n\\(k(x, x')\\) is the covariance function or kernel, which encapsulates our assumptions about the function (like smoothness, periodicity, etc.).\n\nGP is widely used in machine learning, such as\n\nRegression (Gaussian Process Regression): Used for non-linear regression tasks. GPs are particularly useful because they provide not only predictions but also a measure of uncertainty in these predictions.\nClassification: GPs can be extended to classification tasks.\nHyperparameter Tuning and Optimization: In Bayesian optimization, GPs are used to model the function to be optimized, particularly useful for optimizing expensive-to-evaluate functions.\nSpatial Data Modeling: Useful in geostatistics and environmental modeling for interpolating and predicting spatial data.\nTime Series Analysis: Applied in modeling and forecasting in time series data.\n\nSome of the key advantages of using GP in machine learning include:\n\nFlexibility: The choice of kernel allows for a wide range of behaviors.\nUncertainty Quantification: Provides a probabilistic measure of uncertainty in predictions.\nNon-Parametric: GPs are non-parametric, meaning they can model complex datasets without having to assume a specific functional form.\n\nHowever, there are some shortcomings, such as:\n\nComputational Complexity: Involves operations on covariance matrices which can be computationally expensive, particularly for large datasets.\nChoice of Kernel: Selecting and tuning the right kernel is crucial and can be non-trivial.\n\nIn general, Gaussian Processes offer a principled approach to learning in uncertain environments, providing both predictions and assessments of uncertainty. They are a cornerstone in probabilistic modeling and Bayesian approaches in machine learning.\nHere is a Python code that generates several sample paths from a given specification of a Gaussian process:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\ndef plot_gaussian_process_samples(kernel, n_samples=5, n_points=100):\n    \"\"\"\n    Plot samples from a Gaussian Process with a specified kernel.\n\n    Parameters:\n        kernel: Kernel function for the Gaussian Process.\n        n_samples (int): Number of sample paths to generate.\n        n_points (int): Number of points in each sample path.\n    \"\"\"\n    # Create a Gaussian Process model with the specified kernel\n    gp = GaussianProcessRegressor(kernel=kernel)\n\n    # Generate points at which to sample\n    x = np.linspace(0, 10, n_points).reshape(-1, 1)\n\n    # Generate sample paths\n    y_samples = gp.sample_y(x, n_samples)\n\n    # Plotting\n    plt.figure(figsize=(10, 6))\n    for i in range(n_samples):\n        plt.plot(x, y_samples[:, i], lw=2, label=f'Sample {i+1}')\n    plt.title(f'{n_samples} Sample functions from a Gaussian Process')\n    plt.xlabel('x')\n    plt.ylabel('y')\n    plt.legend()\n    plt.show()\n\n# Define the kernel (RBF in this case)\nkernel = RBF(length_scale=1.0)\n\n# Plot sample paths from the Gaussian Process\nplot_gaussian_process_samples(kernel, n_samples=5)\n\n\n\n\n\n\nIn the plot each line represents a sample path from the Gaussian Process. The variability in the paths illustrates the range of functions consistent with the Gaussian Process prior and the chosen RBF kernel.\n\n\n\n\n\n\nThe Python script uses the sklearn.gaussian_process module to simulate and plot sample paths from a Gaussian Process with a Radial Basis Function (RBF) kernel.\n\n\n5.2.4 Continuous Time Random Walks\nA Continuous Time Random Walk (CTRW) is a stochastic model that extends the concept of a traditional random walk to a continuous time setting. Unlike discrete random walks where steps occur at regular intervals, CTRW allows for irregular timing of steps, making it a versatile tool for modeling a wide range of real-world phenomena. In a CTRW, the steps themselves and the waiting times between these steps are treated as random variables. The step sizes can follow any chosen distribution, allowing the model to adapt to the specific characteristics of the system being studied, such as the erratic movement of stock prices or the random motion of particles in fluid dynamics.\nThe waiting time distribution between steps, often modeled by exponential or heavy-tailed distributions, introduces a degree of randomness in the timing of events, which is crucial for systems where events occur sporadically. This randomness in both step sizes and timing distinguishes CTRW from models like Brownian motion, which assumes infinitesimally small, Gaussian-distributed steps at continuous intervals.\nCTRWs are widely used in various fields including physics, for modeling diffusion in complex media; finance, for stock price movement; ecology, for animal movement tracking; and network theory, for traffic and communication patterns. Their flexibility and adaptability in modeling both the magnitude and timing of changes make them particularly valuable in studying complex systems where traditional models may fall short. The analysis of CTRWs often involves Monte Carlo simulations and statistical methods to understand and predict the behavior of the underlying process."
  },
  {
    "objectID": "random_processes.html#some-important-definitions",
    "href": "random_processes.html#some-important-definitions",
    "title": "5  Random Processes and Sequences",
    "section": "5.3 Some Important Definitions",
    "text": "5.3 Some Important Definitions\n\n5.3.1 Ergodic Process\nAn ergodic process is a type of stochastic process that, over a long period, exhibits the same behavior averaged over time as it does averaged over the space of all its possible states. In other words, time averages and ensemble averages are equivalent for ergodic processes. This concept is vital in statistics, thermodynamics, and information theory, as it ensures that long-term observations are representative of the whole process.\nErgodic Process in Discrete Time In discrete-time stochastic processes (like Markov chains), an ergodic process must satisfy two conditions:\n\nIrreducibility: From any state, there is a non-zero probability of reaching any other state. This ensures that the process doesn’t get stuck in a subset of states but can explore the entire state space over time.\nAperiodicity: The process should not be locked into a cyclic pattern. In other words, the system should not return to the same state only at multiples of some fixed number of steps.\n\nWhen a discrete-time stochastic process, such as a Markov chain, is ergodic, it means that it will eventually reach a steady state distribution that does not depend on the initial state. This steady state distribution is used for ensemble averages, and over a long time, the time averages of the process will converge to the same values.\nErgodic Process in Continuous Time\nIn continuous-time stochastic processes (like Brownian motion or certain differential equations), ergodicity implies that the statistical properties (like mean and variance) can be deduced from a single, sufficiently long, random sample path of the process.\nIn continuous time, an ergodic process must satisfy two conditions:\n\nThe process must be stationary: its statistical properties should not change over time.\nSimilar to the discrete case, the process should not be restricted to a subset of its state space; it should be able to explore its entire state space given enough time.\n\nErgodicity plays a significant role in machine learning, particularly in ensuring the reliability and robustness of various algorithms and models. Some examples are:\n\nMarkov Chain Monte Carlo (MCMC) Methods: In MCMC, ergodicity is crucial to ensure that the Markov chain explores the entire state space adequately, thereby guaranteeing that samples generated from the chain are representative of the target distribution. This is essential in Bayesian inference, where MCMC methods are used to approximate posterior distributions when analytical solutions are infeasible.\nTime Series Analysis: In time series analysis, the assumption of ergodicity allows for the use of time averages as substitutes for ensemble averages. This is particularly important when dealing with real-world data where obtaining multiple sample paths is impractical. Ergodic time series models ensure that inferences drawn from a single observed sequence over time are representative of the process’s overall behavior.\nReinforcement Learning (RL): In RL, many algorithms rely on the ergodicity of Markov Decision Processes (MDPs) to ensure that the state-action space is sufficiently explored. This exploration guarantees that the learned policy will perform well across the entire state space, not just in frequently visited regions.\nStationary Processes in Machine Learning Models: Ergodicity is often assumed in stationary processes, where statistical properties like mean and variance are constant over time. This assumption simplifies the modeling and prediction tasks in various machine learning applications, from natural language processing to financial modeling.\nRobustness of Algorithms: Ergodicity can contribute to the robustness of machine learning algorithms by ensuring that the conclusions drawn from the training process (such as parameter estimates or feature importance) are representative of the entire data distribution and not biased by specific characteristics of the training set.\nStatistical Learning Theory: In the broader context of statistical learning, ergodicity helps in understanding and proving convergence properties of learning algorithms, ensuring that they perform well not just on the training data but also on unseen data.\n\nIn summary, ergodicity is leveraged in machine learning to ensure that models and algorithms are representative, robust, and reliable, particularly in scenarios involving stochastic processes, Bayesian inference, time series analysis, and reinforcement learning. It forms a foundational aspect of ensuring that long-term behavior or averaged behavior of models aligns with the true underlying patterns of the data.\n\n\n5.3.2 Markov Processes\nA Markov Process, fundamentally characterized by the Markov property, asserts that the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. In discrete time, this process is exemplified by a Markov Chain, where transitions between a finite or countably infinite set of states are governed by a transition matrix \\(P\\). Each element \\(P_{ij}\\) in this matrix represents the probability of moving from state \\(i\\) to state \\(j\\) in one time step, thus encapsulating the process’s dynamics. The transition matrix is a (right) stochastic matrix, i.e., the rows sums are equal to one. The Markov Chain’s analysis often involves studying its stationary distribution, if it exists, where the state probabilities stabilize over time.\nIn contrast, continuous-time Markov Processes, which are more complex, often manifest as Markovian jump processes or are defined by Stochastic Differential Equations (SDEs). The state transitions occur at random intervals, dictated by exponential distributions, and are characterized by a rate matrix or generator matrix, \\(Q\\), where each element \\(Q_{ij}\\) (for \\(i \\neq j\\)) indicates the rate of transitioning from state \\(i\\) to state \\(j\\). The diagonal elements \\(Q_{ii}\\) are defined such that each row sums to zero, ensuring the conservation of probability. These processes are key in modeling systems where changes are continuous and event timings are stochastic, such as in financial modeling for interest rates or particle dynamics in physics.\nBoth discrete and continuous-time Markov Processes share the principle of memorylessness — the future evolution is independent of the past, given the present. This property simplifies the analysis and modeling of complex stochastic systems, making Markov Processes a fundamental tool in fields ranging from statistical mechanics to quantitative finance. The study of these processes involves a range of mathematical techniques, from linear algebra and probability theory in discrete time to differential equations and stochastic calculus in continuous time.\nIn machine learning, Markov Processes, encompassing both discrete-time Markov chains and continuous-time processes, are instrumental in various algorithms and models, particularly those involving sequential data or decision-making under uncertainty. Some examples are:\n\nHidden Markov Models (HMMs): These are widely used in sequence modeling tasks such as speech recognition, natural language processing, and bioinformatics. In HMMs, the observed data are considered to be a probabilistic function of underlying hidden states that follow a Markov process. The model aims to learn the hidden state sequence that most likely explains the observed data.\nReinforcement Learning (RL): Markov Decision Processes (MDPs), an extension of Markov processes, are foundational in RL. They provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. RL algorithms learn optimal policies to maximize cumulative rewards in MDPs, applicable in robotics, game playing, and autonomous systems.\nTime Series Analysis: Markov models are used for forecasting in financial modeling, weather prediction, and other areas where future states of a series depend on current states. They provide a way to model temporal dynamics and make predictions.\nGraphical Models: In probabilistic graphical models like Bayesian Networks, the Markov property is used to simplify the joint probability distribution over a set of random variables. This approach is useful in various tasks like causal inference, diagnosis, and prediction in complex systems.\nNatural Language Processing (NLP): Beyond HMMs, other Markov process-based models are used for tasks like text generation, where the next word in a sequence depends on the previous ones.\nClustering and Segmentation: Markov models can be used to identify clusters or segments in sequential data, such as segmenting customer transaction sequences into meaningful phases.\nQueueing Theory in Systems Optimization: In areas like network traffic management or supply chain optimization, Markov processes model the behavior of queues, aiding in the design of more efficient systems.\n\nIn machine learning, the application of Markov Processes is rooted in their ability to model stochastic behavior in sequential data and decision-making environments. Their versatility makes them a key component in the toolkit for addressing a wide range of problems where understanding and predicting temporal dynamics are crucial.\n\n\n5.3.3 Stationary & Non Stationary Processes\nA stationary process is a type of stochastic process whose statistical properties, such as mean, variance, and autocorrelation, do not change over time. This concept is crucial in time series analysis and signal processing, as it implies that the process behaves consistently over time, making it easier to analyze and predict.\nTypes of Stationarity 1. Strict (Strong) Stationarity: A process is strictly stationary if the joint probability distribution of any set of variables \\(X_{t_1}, X_{t_2}, ..., X_{t_n}\\) is the same as the distribution of $X_{t_1+h}, X_{t_2+h}, …, X_{t_n+h} $ for all \\(t_1, t_2, ..., t_n\\) and for any time shift \\(h\\). This means that not just the mean and variance, but all moments and joint distributions, are invariant over time.\n\nWeak (Covariance) Stationarity: A process is weakly stationary if the mean is constant over time, the variance is finite and time-invariant, and the covariance between two time points depends only on the time lag between them and not on the actual time points themselves.\n\nExamples of stationary processes are:\n\nWhite Noise: A classic example of a stationary process is white noise. In white noise, each random variable has the same distribution, usually a normal distribution with constant mean and variance, and each variable is independent of the others. This process is often used as a baseline or simple error model in time series analysis.\nRolling Dice: The process of rolling a fair dice repeatedly over time can be considered a stationary process. The probability distribution (each face having a 1/6 chance of appearing) does not change over time, satisfying the criteria for stationarity.\nDaily Temperature Variations: Assuming a stable climate, the daily temperature variations in a specific location could be modeled as a stationary process if the average temperature and variation remain consistent year after year.\n\nIn contrast, a non-stationary process is one whose statistical properties change over time. Such processes are more complex to analyze and model because their behavior varies, and the techniques used for stationary processes are often not applicable.\nSome characteristics of non-stationary process are:\n\nChanging Mean or Variance: The mean or variance (or both) of a non-stationary process may change over time, which can be due to trends, cyclic patterns, or other structural changes in the data.\nTime-Dependent Covariance: The covariance between observations may depend on the actual time at which the observations are made, not just the lag between them.\n\nExamples of non-stationary processes are:\n\nEconomic Growth: The Gross Domestic Product (GDP) of a country is a typical example of a non-stationary process. It often shows a trend over time, with the average GDP and its variance changing, reflecting economic growth or recession phases.\nStock Prices: The prices of stocks in financial markets are non-stationary. They often exhibit trends, sudden jumps, and changes in volatility (variance), making them challenging to predict over long periods.\nSeasonal Sales Data: Retail sales data often show non-stationarity due to seasonal effects. For example, sales might increase during the holiday season each year, showing a recurring but time-dependent pattern.\nClimate Change Data: Long-term climate data, such as global temperatures, can be non-stationary, especially in the context of global warming, where there is a clear upward trend in average temperatures over the years.\n\nIn the aforementioned examples, the distinguishing factor is the evolution of the process’s statistical characteristics over time. Stationary processes maintain consistent statistical properties, simplifying their analysis and modeling. Conversely, non-stationary processes exhibit changing statistical features, such as mean and variance, requiring more intricate models to accommodate these changes. Understanding the process’s nature is essential for selecting suitable statistical or machine learning methods for analysis and prediction.\nAnalytical approaches often presuppose stationarity, particularly weak stationarity. For data that is non-stationary, techniques like differencing, detrending, or seasonal adjustments are typically employed to render the data stationary prior to analysis. The selection of predictive or analytical models is greatly influenced by the process’s stationarity. For example, ARIMA models are frequently used for stationary data, whereas variations of ARIMA that include differencing or trend components are applied to non-stationary data. Therefore, discerning whether a process is stationary or non-stationary is fundamental in choosing the appropriate analytical tools and methods for a range of fields such as economics, finance, meteorology, and signal processing."
  },
  {
    "objectID": "bayesian_inference.html",
    "href": "bayesian_inference.html",
    "title": "6  Bayesian Inference",
    "section": "",
    "text": "Bayesian vs. Frequentist Approach: Philosophical differences and their implications in machine learning.\nBayesian Inference and Prior Distributions: How to update beliefs with evidence, essential in many machine learning models.\nPython Code: Implement simple Bayesian inference examples."
  },
  {
    "objectID": "monte_carlo_methods.html",
    "href": "monte_carlo_methods.html",
    "title": "7  Monte Carlo Methods*",
    "section": "",
    "text": "Monte Carlo Simulation: Estimating integrals and expectations, important in approximation algorithms.\nMarkov Chain Monte Carlo (MCMC): Algorithms for sampling from complex distributions. Propagation of distributions through functions.\nPython Code: Simple Monte Carlo integration and examples of MCMC."
  },
  {
    "objectID": "optimization.html#convex-sets",
    "href": "optimization.html#convex-sets",
    "title": "8  Optimization*",
    "section": "8.1 Convex Sets",
    "text": "8.1 Convex Sets"
  },
  {
    "objectID": "optimization.html#convex-functions",
    "href": "optimization.html#convex-functions",
    "title": "8  Optimization*",
    "section": "8.2 Convex Functions",
    "text": "8.2 Convex Functions"
  },
  {
    "objectID": "optimization.html#some-useful-inequalities",
    "href": "optimization.html#some-useful-inequalities",
    "title": "8  Optimization*",
    "section": "8.3 Some Useful Inequalities",
    "text": "8.3 Some Useful Inequalities"
  },
  {
    "objectID": "function_approximation.html#linear-spaces-with-basis-elements",
    "href": "function_approximation.html#linear-spaces-with-basis-elements",
    "title": "9  Function Approximation",
    "section": "9.1 Linear Spaces with Basis Elements",
    "text": "9.1 Linear Spaces with Basis Elements\nThis chapter emphasizes function approximation through a linear combination of known functions, creating a linear approximation space. In this realm, the notions of “basis” and “basis functions” are essential. A basis in a linear approximation space consists of a set of basis elements (vectors or functions) that are linearly independent and encompass the entire space. This implies that any vector or function in this space can be represented as a linear combination of these basis elements.\nThese basis vectors or functions need to be linearly independent, ensuring that none of them can be expressed as a linear combination of the others. This guarantees the minimality of the basis, meaning that there are no superfluous elements. Additionally, this implies that the basis elements are orthogonal.\nThe collection of basis vectors or functions must cover the entire space, signifying that any element within the space can be precisely depicted using a linear combination of the basis components.\nSome important linear approximation spaces are discussed next."
  },
  {
    "objectID": "function_approximation.html#eulidean-space",
    "href": "function_approximation.html#eulidean-space",
    "title": "9  Function Approximation",
    "section": "9.2 Eulidean Space",
    "text": "9.2 Eulidean Space\nA Euclidean space, often denoted as \\(\\mathcal{R}^n\\), is a mathematical space where each point is represented by a vector \\(\\boldsymbol{x}:=\\begin{bmatrix} x_1 & \\cdots & x_n \\end{bmatrix}^T\\). Each element \\(x_i\\) represents the coordinate of the point along a specific axis or dimension.\nA basis in a Euclidean space consists of a set of linearly independent vectors that span the entire space. In other words, any vector in the Euclidean space can be uniquely expressed as a linear combination of the basis vectors.\n\n9.2.1 Minimal Basis\nIn Euclidean spaces, the most commonly used basis is the standard or canonical basis. In \\(\\mathcal{R}^n\\), the standard basis consists of n unit vectors, each having a single component equal to 1 and all other components equal to 0. For example, in 3D (\\(\\mathcal{R}^3\\)), the standard basis vectors are \\[\n\\boldsymbol{e}_1 = \\begin{bmatrix}1\\\\ 0\\\\ 0\\end{bmatrix}, \\boldsymbol{e}_2 = \\begin{bmatrix}0\\\\ 1\\\\ 0\\end{bmatrix}, \\boldsymbol{e}_3 = \\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix}.\n\\]\nEuclidean space can be represented as a linear combination of these basis vectors. For example, in \\(\\mathcal{R}^3\\), we can express any vector \\(\\boldsymbol{x}:=\\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix}^T\\) as \\[\n\\boldsymbol{x}=  x_1\\begin{bmatrix}1\\\\ 0\\\\ 0\\end{bmatrix} + x_2 \\begin{bmatrix}0\\\\ 1\\\\ 0\\end{bmatrix} + x_3 \\begin{bmatrix}0\\\\ 0\\\\ 1\\end{bmatrix},\n\\] or more compactly as \\(\\boldsymbol{x}= x_1\\boldsymbol{e}_1 + x_2\\boldsymbol{e}_2 + x_3\\boldsymbol{e}_3.\\)\nEuclidean spaces are equipped with an inner product, denoted as \\(\\langle \\cdot, \\cdot \\rangle\\), which defines the dot product or scalar product between two vectors. Mathematically, it is defined as \\[\\langle \\boldsymbol{x}, \\boldsymbol{y}\\rangle = \\sum_{i=1}^n x_i y_i.\\]\nThe basis \\(\\boldsymbol{e}_1, \\boldsymbol{e}_2\\), and \\(\\boldsymbol{e}_3\\) are orthogonal, i.e. \\[\\begin{align*}\n\\left\\langle \\boldsymbol{e}_i,\\boldsymbol{e}_j \\right\\rangle &= 0, \\text{ for } i \\neq j; \\; i,j = {1,2,3},\\\\\n\\left\\langle \\boldsymbol{e}_i,\\boldsymbol{e}_i \\right\\rangle &= 1, \\text{ for } i = {1,2,3}.\n\\end{align*}\\] This orthogonality condition generalizes to \\(\\mathcal{R}^n\\).\nIn general, the basis need not be canonical, but a set of any \\(n\\) orthogonal vectors will also span \\(\\mathcal{R}^n\\). Think of them as different coordinate system in \\(\\mathcal{R}^n\\). That is, we can express \\(\\boldsymbol{x}\\) a linear combination of two sets of basis: \\[\n\\boldsymbol{x}= \\sum_{i=1}^n a_i \\boldsymbol{a}_i = \\sum_{i=1}^n b_i \\boldsymbol{b}_i,\n\\] where \\(\\boldsymbol{a}_i\\) and \\(\\boldsymbol{a}_j\\) are orthogonal, and \\(\\boldsymbol{b}_i\\) and \\(\\boldsymbol{b}_j\\) are orthogonal. The corresponding components are \\(a_i\\) and \\(b_i\\) respectively. This is illustrated in the figure below:\n\n\n\n\n\nDifferent sets of orthogonal basis to represent the same 2D Euclidean space.\n\n\n\n\nNote: Therefore, a Euclidean space can be represented by a non-unique set of orthogonal basis.\nNote: In \\(\\mathcal{R}^n\\), we need a minimum \\(n\\) orthogonal basis to span it.\nWe can recover the components of the vector \\(\\boldsymbol{x}\\) by projecting \\(\\boldsymbol{x}\\) on each of the basis \\(\\boldsymbol{e}_i\\). The projection is defined as the inner-product between \\(\\boldsymbol{x}\\) and \\(\\boldsymbol{e}_i\\). Therefore, the component \\(x_i\\) is the projection of \\(\\boldsymbol{x}\\) on \\(\\boldsymbol{e}_i\\), i.e., \\[\nx_i = \\left\\langle \\boldsymbol{x},\\boldsymbol{e}_i \\right\\rangle.\n\\]\n\n\n9.2.2 Nonminimal Basis\nThe basis in \\(\\mathcal{R}^n\\) need not be orthogonal. If they are then we have a minimal set.\nIn general it is possible to exactly represent a vector \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) with basis \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) where \\(\\boldsymbol{e}_i \\in \\mathcal{R}^n\\) and \\(m &gt; n\\). However, the basis must have \\(n\\) linear independent components, otherwise \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) doesn’t span \\(\\mathcal{R}^n\\) and we will not be able to represent \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) exactly using linear combinations of \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\).\nIf we define a matrix \\(\\boldsymbol{E} \\in \\mathcal{R}^{n\\times m}\\) with \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\), i.e., \\[\n\\boldsymbol{E} = \\begin{bmatrix}\\boldsymbol{e}_1 & \\boldsymbol{e}_2 & \\cdots & \\boldsymbol{e}_m \\end{bmatrix},\n\\] then the vector \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) can be expressed as \\[\n\\boldsymbol{x}\\approx  \\sum_{i=1}^m {y_i\\boldsymbol{e}_i} = \\boldsymbol{E}\\boldsymbol{y},\\] where \\(\\boldsymbol{y}= \\begin{bmatrix}y_1 & y_2 & \\cdots & y_m\\end{bmatrix}^T\\) are the components of \\(\\boldsymbol{x}\\) in \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\). We can think of \\(\\boldsymbol{E}\\) as a matrix that linearly maps vectors from \\(\\mathcal{R}^m\\) to \\(\\mathcal{R}^n\\).\nWe use \\(\\approx\\) in the above equation because \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) may not span \\(\\mathcal{R}^n\\). In that, case our approximation space will be a lower dimensional space, even when we have \\(m&gt;n\\).\nThe dimension of the space spanned by \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) is given by the rank of the matrix \\(\\boldsymbol{E}\\). Recall that a matrix is said to have full rank if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be rank-deficient if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. Therefore, in this case the rank of \\(\\boldsymbol{E}\\) is always less than or equal to \\(n\\).\nWe solve for \\(\\boldsymbol{y}\\) in an optimization framework. We first define the residual or error \\[\\boldsymbol{r} = \\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y}\\] and the objective is to minimize \\(\\boldsymbol{r}\\) in some sense. For this problem, \\(\\boldsymbol{r}\\) is a vector and we want to minimize the length of \\(\\boldsymbol{r}\\). For simplicity, we minimize the square of the length of \\(\\boldsymbol{r}\\), which is simply \\(\\boldsymbol{r}^T\\boldsymbol{r}\\). Therefore, the best representation of \\(\\boldsymbol{x}\\) in the space spanned by \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) is the one that minimizes the residual.\nMathematically, it can be written as \\[\n\\min_{\\boldsymbol{y}} \\boldsymbol{r}^T\\boldsymbol{r},\n\\] which defines the least-squares problem in Euclidean space.\nThe cost function \\(\\boldsymbol{r}^T\\boldsymbol{r}\\) can be written as \\[\\begin{align*}\n\\boldsymbol{r}^T\\boldsymbol{r} &= (\\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y})^T(\\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y}),\\\\\n&= \\boldsymbol{y}^T\\boldsymbol{E}^T\\boldsymbol{E}\\boldsymbol{y}- \\boldsymbol{y}^T\\boldsymbol{E}^T\\boldsymbol{x}- \\boldsymbol{x}^T\\boldsymbol{E}\\boldsymbol{y}+ \\boldsymbol{x}^T\\boldsymbol{x},\\\\\n&= \\boldsymbol{y}^T\\boldsymbol{E}^T\\boldsymbol{E}\\boldsymbol{y}-2\\boldsymbol{y}^T\\boldsymbol{E}^T\\boldsymbol{x}+ \\boldsymbol{x}^T\\boldsymbol{x}.\n\\end{align*}\\]\nThis is a quadratic equation in the unknown \\(\\boldsymbol{y}\\). To solve for \\(\\boldsymbol{y}\\) we first apply the first order condition of optimality \\[\\begin{align*}\n& \\frac{\\partial \\left(\\boldsymbol{y}^T(\\boldsymbol{E}^T\\boldsymbol{E})\\boldsymbol{y}-2\\boldsymbol{y}^T\\boldsymbol{E}^T\\boldsymbol{x}+ \\boldsymbol{x}^T\\boldsymbol{x}\\right)}{\\partial \\boldsymbol{y}} = 0 \\\\\n\\implies & \\boldsymbol{y}^T(\\boldsymbol{E}^T\\boldsymbol{E}) - 2\\boldsymbol{E}^T\\boldsymbol{x}= 0,\\\\\n\\implies & 2(\\boldsymbol{E}^T\\boldsymbol{E})\\boldsymbol{y}= 2\\boldsymbol{E}^T\\boldsymbol{x},\\\\\n\\implies & (\\boldsymbol{E}^T\\boldsymbol{E})\\boldsymbol{y}= \\boldsymbol{E}^T\\boldsymbol{x},\\\\\n\\text{or } & \\boldsymbol{y}=  (\\boldsymbol{E}^T\\boldsymbol{E})^\\dagger\\boldsymbol{E}^T\\boldsymbol{x}.\n\\end{align*}\\]\nTherefore, the optimal representation of \\(\\boldsymbol{x}\\) in the space spanned by \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) is given by the components \\[\n\\boldsymbol{y}^\\ast = (\\boldsymbol{E}^T\\boldsymbol{E})^\\dagger\\boldsymbol{E}^T\\boldsymbol{x}.\n  \\tag{9.1}\\]\nWe put \\(\\ast\\) to indicate optimality.\nNote that:\n\nIf \\(\\boldsymbol{E}\\) is full rank, i.e. \\(\\textbf{rank}\\left(\\boldsymbol{E}\\right) = n\\), then the optimal residual \\(\\boldsymbol{r}^\\ast := \\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y}^\\ast = \\boldsymbol{0}\\), i.e., we are able to exactly represent \\(\\boldsymbol{x}\\) with linear combinations \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\).\nIf \\(\\boldsymbol{E}\\) is rank-deficient rank, i.e. \\(\\textbf{rank}\\left(\\boldsymbol{E}\\right) &lt; n\\), then the optimal residual \\(\\boldsymbol{r}^\\ast := \\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y}^\\ast \\neq \\boldsymbol{0}\\), i.e., we are not able to exactly represent \\(\\boldsymbol{x}\\) with linear combinations \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\). We can only obtain the best approximation of \\(\\boldsymbol{x}\\).\n\n\n\n9.2.3 Examples\nHere are some examples demonstrating approximation in Euclidean space.\n\n9.2.3.1 \\(\\boldsymbol{E}\\) Has Full Rank\nConsider \\(\\mathcal{R}^3\\) with basis \\(\\boldsymbol{e}_i := \\begin{bmatrix}\\cos(\\theta_i) & \\sin(\\theta_i) & 1\\end{bmatrix}^T\\), for \\(\\theta_i \\in \\{0^\\circ, 30^\\circ, 45^\\circ, 60^\\circ, 90^\\circ\\}\\).\n\nimport numpy as np \nd2r = np.pi/180;\nTH = np.array([0,30,45,60,90])\ne = lambda th: np.array([np.cos(th*d2r), np.sin(th*d2r), 1])\nE = np.array([e(th) for th in TH]).T\nprint(E)\n\n[[1.00000000e+00 8.66025404e-01 7.07106781e-01 5.00000000e-01\n  6.12323400e-17]\n [0.00000000e+00 5.00000000e-01 7.07106781e-01 8.66025404e-01\n  1.00000000e+00]\n [1.00000000e+00 1.00000000e+00 1.00000000e+00 1.00000000e+00\n  1.00000000e+00]]\n\n\nThis results in \\[\n  \\boldsymbol{E} = \\begin{bmatrix}\n  1.0 & 0.866 & 0.707  &0.5&       0.0\\\\\n0.0  &0.5       &0.707  &0.866 & 1.0\\\\\n1.0  &1.0       &1.0       &1.0      & 1.0\\\\\n  \\end{bmatrix}.\n\\] We can verify that \\(\\textbf{rank}\\left(\\boldsymbol{E}\\right) = 3\\), therefore \\(\\boldsymbol{E}\\) has full-rank.\n\nprint(np.linalg.matrix_rank(E))\n\n3\n\n\nLet us now represent a vector \\(\\boldsymbol{x}:=\\begin{bmatrix} 1 & 1 & 1\\end{bmatrix}^T\\) with the basis provided. Using (Equation 9.1), the optimal components of \\(\\boldsymbol{x}\\) in \\(\\{\\boldsymbol{e}_i\\}_{i=1}^5\\) is given by:\n\nx = np.array([1,1,1])\nyopt = np.linalg.pinv(E.T@E)@E.T@x\nprint(\"Optimal components:\", yopt)\n\nOptimal components: [-0.79976421  0.79646048  1.00660745  0.79646048 -0.79976421]\n\n\nSince \\(\\boldsymbol{E}\\) is full rank, the optimal residual \\(\\boldsymbol{r}^\\ast\\) should be (numerically) zero.\n\nprint(\"Optimal residual:\", x - E@yopt)\n\nOptimal residual: [ 9.32587341e-15 -1.11022302e-14  2.22044605e-15]\n\n\n\n\n9.2.3.2 Obtain Minimal Spanning Set From a Given Basis\nGiven \\(\\boldsymbol{E}\\) for the previous example, we are interested in determining the minimal basis from it. Since we are representing vectors in \\(\\mathcal{R}^3\\), the minimal set should include three orthogonal basis. We can obtain them using singular value decomposition of \\(\\boldsymbol{E}\\). This is shown in the following Python code.\n\nU, singular_values, Vt = np.linalg.svd(E) # Get svd of E\nprint(\"The reduced basis are given by the columns of U:\\n\", U)\n\nThe reduced basis are given by the columns of U:\n [[-4.66539471e-01  7.07106781e-01 -5.31357622e-01]\n [-4.66539471e-01 -7.07106781e-01 -5.31357622e-01]\n [-7.51453155e-01  1.14761992e-16  6.59786447e-01]]\n\n\nThese columns are orthogonal.\n\nprint(\"dot(U[:,0],U[:,1]):\", np.dot(U[:,0],U[:,1]))\n\ndot(U[:,0],U[:,1]): -2.7460636708693843e-16\n\nprint(\"dot(U[:,0],U[:,2]):\", np.dot(U[:,0],U[:,2]))\n\ndot(U[:,0],U[:,2]): 1.1337185675573172e-16\n\nprint(\"dot(U[:,1],U[:,2]):\", np.dot(U[:,1],U[:,2]))\n\ndot(U[:,1],U[:,2]): -1.4394465687159935e-16\n\n\nThe components of \\(\\boldsymbol{x}\\) in the new basis are given by:\n\nyred = np.linalg.pinv(U.T@U)@(U.T)@x\nprint(\"The components in reduced basis:\\n\", yred);\n\nThe components in reduced basis:\n [-1.68453210e+00 -5.81373296e-17 -4.02928796e-01]\n\n\nThe residual should also be zero, since \\(\\textbf{rank}\\left(\\boldsymbol{U}\\right) = 3\\).\n\nprint(\"Rank(U):\", np.linalg.matrix_rank(U))\n\nRank(U): 3\n\nprint(\"Optimal residue with reduced basis:\\n\", x-U@yred)\n\nOptimal residue with reduced basis:\n [-2.22044605e-16  0.00000000e+00  0.00000000e+00]\n\n\n\n\n9.2.3.3 \\(\\boldsymbol{E}\\) Spans Lower Dimensional Space\nNow consider basis \\(\\boldsymbol{e}_i := \\begin{bmatrix}\\cos(\\theta_i) & \\sin(\\theta_i) & 0 \\end{bmatrix}^T\\), for \\(\\theta_i \\in \\{0^\\circ, 30^\\circ, 45^\\circ, 60^\\circ, 90^\\circ\\}\\). The corresponding \\(\\boldsymbol{E}\\) matrix is given by \\[\n  \\boldsymbol{E} = \\begin{bmatrix}\n  1.0  & 0.866     & 0.707  & 0.5   &  0.0\\\\\n  0.0  & 0.5       & 0.707  & 0.866 & 1.0\\\\\n  0.0  & 0.0       & 0.0    & 0.0   & 0.0\\\\\n  \\end{bmatrix}.\n\\]\n\nimport numpy as np \nd2r = np.pi/180;\nTH = np.array([0,30,45,60,90])\ne = lambda th: np.array([np.cos(th*d2r), np.sin(th*d2r), 0])\nE = np.array([e(th) for th in TH]).T\nprint(E)\n\n[[1.00000000e+00 8.66025404e-01 7.07106781e-01 5.00000000e-01\n  6.12323400e-17]\n [0.00000000e+00 5.00000000e-01 7.07106781e-01 8.66025404e-01\n  1.00000000e+00]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n  0.00000000e+00]]\n\nprint(\"Matrix rank: \", np.linalg.matrix_rank(E))\n\nMatrix rank:  2\n\n\nSince the rank of this matrix is 2, the associated basis doesn’t span \\(\\mathcal{R}^3\\). Therefore, we will not be able to exactly represent vectors in \\(\\mathcal{R}^3\\). We can get the best approximation in \\(\\mathcal{R}^2\\).\nFor example, \\(\\boldsymbol{x}:=\\begin{bmatrix} 1 & 1 & 1\\end{bmatrix}^T\\) cannot be exactly represented, i.e., we will have a non-zero residual. This is demonstrated in the following Python code.\n\ny = np.linalg.pinv(E.T@E)@(E.T)@x\nprint(\"Optimal residue with given basis:\\n\", x-E@y)\n\nOptimal residue with given basis:\n [ 4.4408921e-16 -4.4408921e-16  1.0000000e+00]\n\n\nWe see that there is zero residual in the first two coordinates, which is in the space spanned by the given basis. The residual vector is thus in the space orthogonal to the space spanned by the given basis.\nIn general, the residual will be orthogonal to the space spanned by the basis. This will always be the case whenever we try to approximate objects in higher dimensional space in lower dimensional space.\n\n\n9.2.3.4 A Projection Perspective\nWe next show that the best approximation in a lower dimensional space can be eqiuvalently achieved by projecting the residual on each of the basis function. We assume we have \\(m\\) basis in \\(\\mathcal{R}^n\\), i.e. \\(\\{\\boldsymbol{e}_i\\}_{i=1}^m\\) for \\(\\boldsymbol{e}_i\\in\\mathcal{R}^n\\).\nThe optimal representation of \\(\\boldsymbol{x}\\in\\mathcal{R}\\) in that case satisfies \\[\n\\left\\langle \\boldsymbol{r},\\boldsymbol{e}_i \\right\\rangle = 0, \\text{ for } i = 1,\\cdots,m.\n\\tag{9.2}\\]\nRecalling that \\(\\boldsymbol{r}:=\\boldsymbol{x}- \\boldsymbol{E}\\boldsymbol{y}\\) and \\(\\left\\langle \\boldsymbol{r},\\boldsymbol{e}_i \\right\\rangle\\) for vectors is simply \\(\\boldsymbol{r}^T \\boldsymbol{e}_i = \\boldsymbol{e}_i^T\\boldsymbol{r}\\), Equation 9.2 can be written as \\[\\begin{align*}\n& \\boldsymbol{e}_i^T\\boldsymbol{x}- \\boldsymbol{e}_i^T\\boldsymbol{E}\\boldsymbol{y}= 0,\\\\\n\\text{ or }& \\boldsymbol{e}_i^T\\boldsymbol{E}\\boldsymbol{y}= \\boldsymbol{e}_i^T\\boldsymbol{x}, \\text{ for } i = 1,\\cdots,m,\n\\end{align*}\\] or more compactly as \\[\n\\boldsymbol{E}^T\\boldsymbol{E}\\boldsymbol{y}= \\boldsymbol{E}^T\\boldsymbol{x}.\n\\]\nTherefore, the coefficients \\(\\boldsymbol{y}\\) can be solved as \\[\n\\boldsymbol{y}= (\\boldsymbol{E}^T\\boldsymbol{E})^\\dagger\\boldsymbol{E}^T\\boldsymbol{x},\n\\] which is the same as Equation 9.1. Therefore, projecting the residual on each basis results in the optimal approximation.\n\n\n\n9.2.4 Matrix Spaces\nWe next look at linear matrix spaces. Let us consider a matrix in \\(\\mathcal{R}^{2\\times 2}\\) given by \\[\n\\boldsymbol{A}= \\begin{bmatrix}a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix}.\n\\] We can represent it as a linear combination of basis matrices in the following manner: \\[\n\\boldsymbol{A}= a_{11}\\begin{bmatrix}1 & 0\\\\0 & 0\\end{bmatrix} +  a_{12}\\begin{bmatrix}0 & 1\\\\0 & 0\\end{bmatrix} + a_{21}\\begin{bmatrix}0 & 0\\\\1 & 0\\end{bmatrix} + a_{22}\\begin{bmatrix}0 & 0\\\\0 & 1\\end{bmatrix},\n\\] where \\[\n\\begin{bmatrix}1 & 0\\\\0 & 0\\end{bmatrix}, \\begin{bmatrix}0 & 1\\\\0 & 0\\end{bmatrix}, \\begin{bmatrix}0 & 0\\\\1 & 0\\end{bmatrix}, \\text{ and }\\begin{bmatrix}0 & 0\\\\0 & 1\\end{bmatrix},\n\\] are basis matrices that span \\(\\mathcal{R}^{2\\times 2}\\).\nSimilarly, space of symmetric matrices in \\(\\mathcal{R}^{2\\times 2}\\) can be written as \\[\n\\boldsymbol{A}= a_{11}\\begin{bmatrix}1 & 0\\\\0 & 0\\end{bmatrix} +  a_{12}\\begin{bmatrix}0 & 1\\\\1 & 0\\end{bmatrix} + a_{22}\\begin{bmatrix}0 & 0\\\\0 & 1\\end{bmatrix} =  \\begin{bmatrix}a_{11} & a_{12} \\\\ a_{12} & a_{22} \\end{bmatrix}.\n\\]\nRecall that inner product of two matrices \\(\\boldsymbol{A},\\boldsymbol{B}\\) is defined as: \\[\n\\left\\langle \\boldsymbol{A},\\boldsymbol{B} \\right\\rangle := \\textbf{tr}\\left[\\boldsymbol{A}^T\\boldsymbol{B}\\right],\n\\] which can be applied to the basis matrices shown above to confirm that they are orthogonal."
  },
  {
    "objectID": "function_approximation.html#function-space",
    "href": "function_approximation.html#function-space",
    "title": "9  Function Approximation",
    "section": "9.3 Function Space",
    "text": "9.3 Function Space\nLet us consider an example of function approximation in the \\(\\mathcal{L}^2\\) Hilbert space, which consists of all square-integrable functions. Recall that a function \\(f(x)\\) is in \\(\\mathcal{L}^2\\) if \\(\\int |f(x)|^2 dx &lt; \\infty\\). In this space, the inner product of two functions \\(f(x)\\) and \\(g(x)\\) which maps \\(\\mathcal{R}\\) to \\(\\mathcal{R}\\) is defined as \\(\\langle f(x), g(x) \\rangle = \\int f(x)g(x) dx\\). If the functions are vectors or matrices with vector arguments, then we have the following definition of inner products: \\[\\begin{align*}\n\\text{For vector functions: } & \\langle\\boldsymbol{f}(\\boldsymbol{x}),\\boldsymbol{g}(\\boldsymbol{x})\\rangle := \\int_\\mathcal{D} \\boldsymbol{f}^T(x)\\boldsymbol{g}(\\boldsymbol{x}) d\\boldsymbol{x}, \\\\\n\\text{For matrix functions: } & \\langle\\boldsymbol{F}(\\boldsymbol{x}),\\boldsymbol{G}(\\boldsymbol{x})\\rangle := \\int_\\mathcal{D} \\textbf{tr}\\left[\\boldsymbol{F}^T(x)\\boldsymbol{G}(\\boldsymbol{x})\\right] d\\boldsymbol{x},\n\\end{align*}\\] where \\(\\mathcal{D}\\) is the set over which the functions are defined.\n\n9.3.1 Basis Functions\nA Hilbert space is defined by a set of basis functions (or vectors for \\(l_2\\) spaces), which are denoted by \\(\\phi_i(\\boldsymbol{x})\\) for \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\). The set of basis functions are denoted \\[\n\\boldsymbol{\\Phi}(\\boldsymbol{x}) := \\begin{bmatrix}\\phi_0(\\boldsymbol{x})\\\\\\phi_1(\\boldsymbol{x}) \\\\ \\vdots \\\\\\phi_N(\\boldsymbol{x}) \\end{bmatrix}.\n\\tag{9.3}\\]\nScalar functions in this space can be represented as linear combination of these basis functions as, \\[\n\\hat{f}(\\boldsymbol{x}) = \\sum_{i=0}^N \\phi_i(\\boldsymbol{x})\\alpha_i = \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}= \\boldsymbol{\\alpha}^T\\boldsymbol{\\Phi}(\\boldsymbol{x}),\n\\] where \\(\\boldsymbol{\\alpha}:= \\begin{bmatrix}\\alpha_0 & \\alpha_1 & \\cdots &\\alpha_N\\end{bmatrix}^T\\) is the vector of coefficients. Vector functions \\(\\boldsymbol{F}(\\boldsymbol{x}):\\mathcal{R}^n \\mapsto \\mathcal{R}^m\\) can be represented as \\[\n\\hat{\\boldsymbol{f}}(\\boldsymbol{x}) = \\boldsymbol{A}\\boldsymbol{\\Phi}(\\boldsymbol{x}),\n\\] where \\(\\boldsymbol{A}\\in\\mathcal{R}^{m\\times (N+1)}\\) is the coeffient matrix. We put a \\(\\hat{}\\) on the functions to indicate that we will be approximating the true functions in a finite-dimensional Hilbert space.\nWhen \\(\\phi_i(\\boldsymbol{x})\\) are orthogonal, i.e., \\(\\langle\\phi_i(\\boldsymbol{x}),\\phi_j(\\boldsymbol{x})\\rangle = 0\\) for \\(i\\neq j\\), the dimension of the Hilbert space is given by the number of basis functions. However, these basis functions need not be orthogonal, resulting in non-minimal set of basis functions. In that case, the dimension of the Hilbert space is given by the rank of the matrix \\(\\langle \\boldsymbol{\\Phi}^T(\\boldsymbol{x}),\\boldsymbol{\\Phi}^T(\\boldsymbol{x}) \\rangle\\).\nLet us next consider approximation of a function \\(f(\\boldsymbol{x}):\\mathcal{R}^n \\mapsto \\mathcal{R}\\) in the space defined by \\(\\boldsymbol{\\Phi}(\\boldsymbol{x})\\). Similar to approximation in the Euclidean space, we will define the error (or residual) as \\[\ne(\\boldsymbol{x}) := f(\\boldsymbol{x}) - \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}.\n\\] The objective is to determine the coefficient vector \\(\\boldsymbol{\\alpha}\\) which minimizes \\(e(\\boldsymbol{x})\\) in some sense. We will minimize \\(\\|e(\\boldsymbol{x})\\|^2_2\\), which is defined as \\[\\begin{align*}\n\\|e(\\boldsymbol{x})\\|^2_2 &:= \\langle e(\\boldsymbol{x}),e(\\boldsymbol{x}))\\rangle^2, \\\\\n&=  \\langle f(\\boldsymbol{x}) - \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha},  f(\\boldsymbol{x}) - \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}\\rangle,\\\\\n&= \\int_\\mathcal{D} (f(\\boldsymbol{x}) - \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha})^T (f(\\boldsymbol{x}) - \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}) d\\boldsymbol{x},\\\\\n& = \\int_\\mathcal{D} \\left(f^2(\\boldsymbol{x}) -  2f(\\boldsymbol{x})\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}+ \\boldsymbol{\\alpha}^T\\boldsymbol{\\Phi}(\\boldsymbol{x})\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}\\right)d\\boldsymbol{x},\\\\\n& = \\underbrace{\\left(\\int_\\mathcal{D} f^2(\\boldsymbol{x})d\\boldsymbol{x}\\right)}_{=: s} - 2\\underbrace{\\left(\\int_\\mathcal{D} f(\\boldsymbol{x})\\boldsymbol{\\Phi}^T(\\boldsymbol{x}) \\right)}_{=:\\boldsymbol{r}^T}\\boldsymbol{\\alpha}+ \\boldsymbol{\\alpha}^T \\underbrace{\\left(\\int_\\mathcal{D} \\boldsymbol{\\Phi}(\\boldsymbol{x})\\boldsymbol{\\Phi}^T(\\boldsymbol{x}) d\\boldsymbol{x}\\right)}_{=:\\boldsymbol{Q \\ge \\boldsymbol{0}}}\\boldsymbol{\\alpha}, \\\\\n&= \\boldsymbol{\\alpha}\\top\\boldsymbol{Q}\\boldsymbol{\\alpha}-  2\\boldsymbol{r}^T\\boldsymbol{\\alpha}+ s,\n\\end{align*}\\] which is quadratic in \\(\\boldsymbol{\\alpha}\\). The minimum value of a quadratic cost function is obtained by first setting the gradient w.r.t \\(\\boldsymbol{\\alpha}\\) to zero and then checking the Hessian for positive (semi-)definiteness. Therefore, \\[\\begin{align*}\n& \\frac{\\partial}{\\partial \\boldsymbol{\\alpha}}\\left(\\boldsymbol{\\alpha}\\top\\boldsymbol{Q}\\boldsymbol{\\alpha}-  2\\boldsymbol{r}^T\\boldsymbol{\\alpha}+ s\\right) = 2\\boldsymbol{Q}\\boldsymbol{\\alpha}- 2\\boldsymbol{r} = 0.\\\\\n\\end{align*}\\] Therefore, the solution is given by \\[\n\\implies  \\boldsymbol{\\alpha}^\\ast := \\boldsymbol{Q}^\\dagger\\boldsymbol{r}.\n\\tag{9.4}\\]\nThe Hessian of the cost function is \\(\\boldsymbol{Q}\\) which is positive (semi-) definite. Therefore, the candidate solution(s) obtained by setting the gradient to zero is (are) the minima of the function.\nIf \\(\\boldsymbol{Q}\\) is positive definite then it is invertible and \\(\\boldsymbol{\\alpha}^\\ast\\) is unique. If \\(\\boldsymbol{Q}\\) is positive semi-definite then \\(\\boldsymbol{Q}\\) is not invertible and there are many solutions of \\(\\boldsymbol{\\alpha}\\). The pseudo-inverse gives the \\(\\boldsymbol{\\alpha}\\) which minimizes \\(\\|\\boldsymbol{\\alpha}\\|_2\\).\nThe Hessian of the cost function is \\(\\boldsymbol{Q}\\), which is positive (semi-)definite. Consequently, the potential solution(s) identified by equating the gradient to zero represent the minima of the function. When \\(\\boldsymbol{Q}\\) is positive definite, it also becomes invertible, leading to a unique solution denoted as \\(\\boldsymbol{\\alpha}^\\ast\\). In contrast, if \\(\\boldsymbol{Q}\\) is positive semi-definite, it lacks invertibility, resulting in multiple solutions for \\(\\boldsymbol{\\alpha}\\). In such cases, the pseudo-inverse method is employed to determine the value of \\(\\boldsymbol{\\alpha}\\) that minimizes the \\(\\ell_2\\) norm, \\(\\|\\boldsymbol{\\alpha}\\|_2\\).\nNote: If the function \\(f(\\boldsymbol{x})\\) is in the space spanned by \\(\\boldsymbol{\\Phi}(x)\\), then the optimal \\(\\|e(\\boldsymbol{x})\\|^2_2\\) will be zero. Otherwise, the optimal residual will be non-zero.\n\n\n9.3.2 A Projection Perspective\nWe can show that projecting the error \\(e(\\boldsymbol{x})\\) on the basis function also results in the optimal solution. We can write \\(\\langle e(\\boldsymbol{x}),\\phi_i(\\boldsymbol{x}) \\rangle = 0\\) for \\(i = 0,\\cdots,N\\) compactly as \\[\n\\begin{bmatrix}\\langle e(\\boldsymbol{x}),\\phi_0(\\boldsymbol{x}) \\rangle\\\\\n\\langle e(\\boldsymbol{x}),\\phi_1(\\boldsymbol{x}) \\rangle \\\\\n\\vdots \\\\\n\\langle e(\\boldsymbol{x}),\\phi_N(\\boldsymbol{x}) \\rangle\n\\end{bmatrix} = \\boldsymbol{0} \\\\\n\\implies\n\\underbrace{\\begin{bmatrix}\n\\langle\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\phi_0(\\boldsymbol{x})\\rangle\\\\\n\\langle\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\phi_1(\\boldsymbol{x})\\rangle\\\\\n\\vdots \\\\\n\\langle\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\phi_N(\\boldsymbol{x})\\rangle\n\\end{bmatrix}}_{\\text{This is equal to } \\langle \\boldsymbol{\\Phi}(\\boldsymbol{x})\\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\rangle = \\boldsymbol{Q}.} \\boldsymbol{\\alpha}= \\underbrace{\\begin{bmatrix}\\langle f(\\boldsymbol{x})\\phi_0(\\boldsymbol{x})\\rangle\\\\\n\\langle f(\\boldsymbol{x})\\phi_1(\\boldsymbol{x})\\rangle\\\\\n\\vdots \\\\\n\\langle f(\\boldsymbol{x})\\phi_N(\\boldsymbol{x})\\rangle\n\\end{bmatrix}}_{\\text{This is equal to } \\langle \\boldsymbol{\\Phi}(\\boldsymbol{x})f(\\boldsymbol{x})\\rangle = \\boldsymbol{r}.}.\n\\] Therefore, the system of linear equations has a solution \\(\\boldsymbol{\\alpha}^\\ast = \\boldsymbol{Q}^\\dagger\\boldsymbol{r}\\), which is the same solution as Equation 9.4.\n\n9.3.2.1 Example: Exact Polynomial Representation\nHere we consider approximation of \\(f(x) = 1+x^2 - x^3\\) with monomial basis \\(\\boldsymbol{\\Phi}(x) = \\begin{bmatrix}1 & x & x^2 & x^3 & x^4 \\end{bmatrix}^T\\). This is a simple problem, where \\(f(x)\\) is in the space spanned by \\(\\boldsymbol{\\Phi}(x)\\) and we should expect zero residual error.\nTo approximate the function \\(f(x) = 1 + 2x^2 - x^3\\) using a polynomial basis, we can use the SymPy library in Python. We define the inner product in this context as \\(\\langle p(x), q(x) \\rangle = \\int_{-1}^{1} p(x)q(x) dx\\) for polynomials \\(p(x)\\) and \\(q(x)\\).\nHere’s the Python code to perform this approximation:\n\nimport numpy as np\nimport sympy as sp\n\nx = sp.symbols('x')\n\nbasis = [1,x,x**2,x**3,x**4]\nnbasis = len(basis)\nf = 1 + 2*x**2 - x**3\n\n# Compute projections using Sympy.\nQ = [];\nr = [];\nfor p in basis:\n   a = sp.integrate(p*f,(x,-1,1))\n   r.append(a)\n   for q in basis:\n      b = sp.integrate(p*q,(x,-1,1))\n      Q.append(b)\n\n# Solve for the coefficients\nQQ = np.reshape(Q, (nbasis,nbasis)).astype(float)\nrr = np.array(r).astype(float)\nalp = np.round(np.linalg.pinv(QQ)@rr,2)\n\nfhat = sum(basis*alp)\nprint(' f(x):',f,'\\n','fhat(x):', fhat)\n\n f(x): -x**3 + 2*x**2 + 1 \n fhat(x): -1.0*x**3 + 2.0*x**2 + 1.0\n\n\nWe see that the basis functions are able to exactly recover the true function. This is expected since \\(f(x)\\) lies in the space of functions spanned by \\(\\boldsymbol{\\Phi}(\\boldsymbol{x})\\).\n\n\n9.3.2.2 Example: Approximation of \\(e^x\\) with Monomials\nNow consider \\(f(x) = e^x\\). From Taylor series expansion, we know that we need infinite terms to represent \\(e^x\\), i.e., \\[\ne^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\frac{x^4}{4!} + \\frac{x^5}{5!} + \\cdots\n\\]\nTherefore, if we treat each monomial as a basis function, we will need infinite basis to exactly represent \\(e^x\\). Consequently, we can only approximate \\(e^x\\) with a finite number of monomial basis functions. However, if the approximation interval is small, we may get satisfactorily high accuracy with a few basis functions, as demonstrated next. The following Python code approximates \\(e^x\\) with increasing number of basis functions.\n\nimport numpy as np\nimport sympy as sp\nimport matplotlib.pyplot as plt\n\nx = sp.symbols('x')\n\nN = 10\nii = range(N)\nbasis = [x**i for i in ii]\n\nnbasis = len(basis)\nf = sp.exp(x)\n\n# Compute projections using Sympy.\nQ = [];\nr = [];\nfor p in basis:\n   a = sp.integrate(p*f,(x,-1,1))\n   r.append(a)\n   for q in basis:\n      b = sp.integrate(p*q,(x,-1,1))\n      Q.append(b)\n\n# Solve for the coefficients\nQQ = np.reshape(Q, (nbasis,nbasis)).astype(float)\nrr = np.array(r).astype(float)\nalp = np.round(np.linalg.pinv(QQ)@rr,2)\n\nfhat = sum(basis*alp)\nplt.plot(ii,alp,'o-'); \nplt.xlabel('Coefficient Index');\nplt.ylabel('alpha');\nplt.title('Coefficients of various basis functions.')\nplt.grid()\nplt.show()\n\n\n\n\n\n\nApproximation of \\(e^x\\) with monomial basis functions in the interval \\([-1,1]\\). The plot shows coefficients associated with each basis function.\n\n\n\n\n\n\nWe see that the coefficient for basis number six on onwards are zero, implying we are able accurately represent \\(e^x\\) in the interval \\([-1,1]\\) with the first six basis functions. This is supported by the next set of plots where we plot approximations of \\(e^x\\) with increasing number of basis functions.\n\nk1=3; fhat1 = sp.lambdify(x,sum(basis[:k1]*alp[:k1]))\nk2=4; fhat2 = sp.lambdify(x,sum(basis[:k2]*alp[:k2]))\nk3=5; fhat3 = sp.lambdify(x,sum(basis[:k3]*alp[:k3]))\n\nxx = np.linspace(-1,1,100)\n\ndef plot_approximations(k,fhat,xx):\n   line1, = plt.plot(xx,fhat(xx), label='Approximation')\n   line2, = plt.plot(xx,np.exp(xx),label='exp(x)'); \n   plt.title(f'With {k+1} basis functions'); \n   plt.xlabel('x')\n   plt.legend(handles=[line1,line2])\n\nplt.figure(1);\nplt.subplot(1,3,1);  plot_approximations(k1,fhat1,xx);\nplt.subplot(1,3,2);  plot_approximations(k2,fhat2,xx);\nplt.subplot(1,3,3);  plot_approximations(k3,fhat3,xx);\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nComparison of \\(e^x\\) with its approximation with increasing number of basis functions in the interval \\([-1,1]\\).\n\n\n\n\n\n\nNote: Orthogonal polynomials, such as Legendre, Chebyshev, and Hermite, are often favored over monomials for function approximation due to their distinct advantages in numerical stability and efficiency. These polynomials are mutually orthogonal with respect to a specific inner product, ensuring independent coefficients in the polynomial approximation and reducing common numerical problems like ill-conditioning. The approximation process involves projecting the function into a space defined by these polynomials, with coefficients computed via inner products, leading to stable and precise results, especially with higher-degree polynomials. Unique properties of certain orthogonal polynomials, like the minimax property of Chebyshev polynomials and the consistent behavior of Legendre polynomials, make them ideal for function approximation.\n\n\n9.3.2.3 Example: Approximation with Trigonometric Basis Functions\nTrigonometric basis functions are useful in approximating periodic functions. Let us consider basis functions \\[\n\\boldsymbol{\\Phi}(x):= \\begin{bmatrix} 1, & \\cos(\\frac{\\pi}{L}x), & \\sin(\\frac{\\pi}{L}x), &\\cdots, & \\cos(\\frac{N\\pi}{L}x), & \\sin(\\frac{N\\pi}{L}x) \\end{bmatrix}^T,\n\\] over the interval \\([-L,L]\\). These basis functions are orthogonal.\nThe following Python code approximates \\(f(x) = \\sin(\\frac{3\\pi}{2}x)^3(1-x^2)\\) using trigonometric basis functions over the interval \\([-1,1]\\)\n\nimport numpy as np\nimport sympy as sp\nimport matplotlib.pyplot as plt\n\nx = sp.symbols('x')\nf = sp.cos(3*(sp.pi/2)*x)**3*(1-x**2)\n\nbasis = [1]\nN = 7\nii = range(N)\nfor i in ii:\n   basis.append(sp.cos(i*sp.pi*x))\n   basis.append(sp.sin(i*sp.pi*x))\n\nnBasis = len(basis)\n\n# Compute projections using Sympy.\nQ = [];\nr = [];\nfor p in basis:\n   a = sp.integrate(p*f,(x,-1,1)) # Going to be slow\n   r.append(a)\n   for q in basis:\n      b = sp.integrate(p*q,(x,-1,1)) # Going to be slow\n      Q.append(b)\n\n# Solve for the coefficients\nQQ = np.reshape(Q, (nBasis,nBasis)).astype(float)\nrr = np.array(r).astype(float)\nalp = np.round(np.linalg.pinv(QQ)@rr,2)\n\nxx = np.linspace(-1,1,100)\nF = sp.lambdify(x,f)\nfhat = sum(alp*basis)\nFhat = sp.lambdify(x,fhat)\n\nplt.figure(1)\nplt.subplot(1,2,1)\nplt.plot(xx,F(xx),xx,Fhat(xx),'--')\nplt.legend(('True','Approximate'))\nplt.xlabel('x')\nplt.title('Approximation Using \\n Trigonometric Basis Functions.')\n\nplt.subplot(1,2,2)\nplt.plot(xx,F(xx)-Fhat(xx))\nplt.xlabel('x')\nplt.title('Approximation Error.')\n\nplt.tight_layout()\n\n\n\n\n\n\nFigure 9.1: Approximation of a periodic function in the interval \\([-1,1]\\) using trigonometric basis functions.\n\n\n\n\n\n\nFigure 9.1 shows that we are able to approximate \\(f(x) = \\sin(\\frac{3\\pi}{2}x)^3(1-x^2)\\) with \\(15\\) basis functions with small errors. We can reduce the error further by increasing the number of basis functions.\n\nplt.stem(range(nBasis),alp)\nplt.xlabel('Basis Index')\nplt.title('Basis Coefficient')\n\n\n\n\n\n\nFigure 9.2: Coefficients of basis functions.\n\n\n\n\n\n\nFigure 9.2 show the coefficients, which are projections of \\(f(x) = \\sin(\\frac{3\\pi}{2}x)^3(1-x^2)\\) on each of the \\(15\\) basis functions. We see that most of the coefficients are zero indicating the function lies in a much smaller dimensional space than assumed.\nThe basis functions are essentially Fourier modes. Therefore, the approximation is nothing but Fourier series expansion of the function and the coefficients of the basis functions gives rise to the discrete Fourier transform of the signal. If chose wavelets as basis, then we would get wavelet transform of the function. Therefore, we can think of all these transforms as projections of a function on different basis functions.\n\n\n\n9.3.3 Function Approximation Over Discrete Data\nSo far we have considered approximation of a function from its analytical form. It involves inner products which require integral of function products. The functions we have considered so far have analytical integrals. However, in more complicated scenarios, analytical integrals may not exist and we have to resort to numerical integrals. In more realistic scenarios, we may have to approximate a function from some data points. In both cases, we have to determine the optimal coefficients for the basis functions from discrete data points.\n\n9.3.3.1 Quadrature Methods\nQuadrature, in the context of numerical analysis, refers to the process of estimating the definite integral of a function. It is a fundamental technique used when an integral is too complex to be solved analytically, or when dealing with integrals of data points rather than known functions. The goal of quadrature is to approximate the area under a curve defined by a function over a specific interval.\nThe process typically involves approximating the function by a simpler one, often a polynomial, and then dividing the integration interval into smaller subintervals. The integral over each subinterval is estimated using this approximation, and the results are summed to approximate the total integral.\nCommon quadrature methods include:\n\nTrapezoidal Rule: This method approximates the area under the curve as a series of trapezoids and sums their areas. It is simple but less accurate for functions that are not approximately linear over the subintervals.\nSimpson’s Rule: Simpson’s Rule improves accuracy by approximating the function with second-degree (quadratic) polynomials. It’s more accurate than the trapezoidal rule for smooth functions.\nGaussian Quadrature: This technique improves accuracy by strategically choosing the points within each interval where the function is evaluated. It can achieve higher accuracy with fewer evaluations than equally-spaced samples. We discuss it in more detail next.\n\nGaussian quadrature is a sophisticated numerical method for approximating the integral of a function, particularly effective with polynomial and smooth functions. It is based on transforming the integral into a weighted sum of function values at strategically chosen points or abscissas.\nThese abscissas are the roots of orthogonal polynomials defined over the integration interval. For a standard Gaussian quadrature over the interval \\([-1, 1]\\), Legendre polynomials are utilized. These polynomials are orthogonal with respect to the weight function, typically set to \\(1\\), over the given interval. Mathematically, the orthogonality condition for Legendre polynomials \\(P_n(x)\\) is expressed as \\[\\int_{-1}^{1} P_m(x) P_n(x) \\, dx = 0, \\text{ for } m\\neq n.\\]\nThe abscissas are the roots of the Legendre polynomial of degree \\(n\\), providing \\(n\\) distinct points within the interval. The integration process then hinges on these points, with each accompanied by a specific weight. These weights are derived from the properties of orthogonal polynomials and are calculated by integrating the corresponding Lagrange polynomial, which is constructed to be \\(1\\) at the given abscissa and \\(0\\) at others, over the interval. For Legendre polynomials, the weight formula simplifies to \\[w_i = \\frac{2}{(1 - x_i^2) [P_n'(x_i)]^2},\\] where \\(x_i\\) are the roots of \\(P_n(x)\\) and \\(P_n'(x_i)\\) is the derivative of \\(P_n(x)\\) evaluated at \\(x_i\\).\nThe Gaussian quadrature approximates the integral by summing the products of function values at each abscissa and the corresponding weights: \\[\nI \\approx \\sum_{i=1}^{n} \\frac{2 f(x_i)}{(1 - x_i^2) [P_n'(x_i)]^2}.\\]\nThis method is particularly efficient and accurate for polynomial functions, ensuring exact results for polynomials of degree up to \\(2n-1\\) with just \\(n\\) function evaluations.\nWe next use Gaussian quadrature to integrate polynomial functions and compare it with the analytical solution. Let us consider polynomial \\(f(x) = x^3 - 3x^2 + 2x\\). We will integrate this function over an interval \\([0,5]\\), both analytically and numerically using Gaussian quadrature, and then compare the results. The following Python code demonstrates it.\n\nimport numpy as np\nfrom scipy.integrate import quad, quadrature\n\n# Define the polynomial function\ndef poly_function(x):\n    return x**3 - 3*x**2 + 2*x\n\n# Analytical integration of the polynomial\ndef analytical_integration(a, b):\n    # Integral of the polynomial is x^4/4 - x^3 + x^2\n    return (b**4)/4 - (b**3) + (b**2) - ((a**4)/4 - (a**3) + (a**2))\n\n# Integration limits\na, b = 0, 5\n\n# Perform the integration using Gaussian quadrature\nresult_gauss, _ = quadrature(poly_function, a, b)\n\n# Calculate the analytical result\nresult_analytical = analytical_integration(a, b)\n\n# Print the results\nprint(f\"Gaussian Quadrature Result: {result_gauss}\")\nprint(f\"Analytical Result: {result_analytical}\")\nprint(f\"Error:{result_analytical - result_gauss}, which is machine precision.\")\n\n\n\nGaussian Quadrature Result: 56.25000000000001\n\n\n\n\nAnalytical Result: 56.25\n\n\n\n\nError:-7.105427357601002e-15, which is machine precision.\n\n\n\n\nThis script uses the quadrature function from SciPy, which implements Gaussian quadrature for numerical integration. It also defines an analytical_integration function that computes the integral of the given polynomial analytically. We see that the numerical integration matches the analytical solution upto machine precision.\nWe next show the performance of Gaussian quadrature for integrating non-polynomial functions. We consider integration of \\(f(x) = e^{-x^2}\\) over \\([0,1]\\), which analytically is \\(\\frac{\\sqrt{\\pi}}{2}\\text{erf}(1)\\), where \\(\\text{erf}(\\cdot)\\) is the error function.\n\nimport numpy as np\nimport math\nfrom scipy.integrate import quad, quadrature\n\n# Define the non-polynomial function\ndef non_poly_function(x):\n    return np.exp(-x**2)\n\n# Integration limits\na, b = 0, 1\n\n# Perform the integration using Gaussian quadrature\nresult_gauss, _ = quadrature(non_poly_function, a, b)\n\n# Calculate the \"analytical\" result using quad (numerical approximation)\nresult_analytical = math.sqrt(math.pi)*math.erf(1)/2\n\n# Print the results\nprint(f\"Gaussian Quadrature Result: {result_gauss}\")\nprint(f\"Analytical Result: {result_analytical}\")\nprint(f\"Error:{result_analytical - result_gauss}\")\n\n\n\nGaussian Quadrature Result: 0.7468241328901553\n\n\n\n\nAnalytical Result: 0.7468241328124269\n\n\n\n\nError:-7.772837928854415e-11\n\n\n\n\nThe error is quite small, but not quite machine precision.\n\n\n9.3.3.2 Least-Square Approximation\nIn this case, we assume we do not have analytical form of the function \\(f(\\boldsymbol{x}):\\mathcal{R}^n\\mapsto\\mathcal{R}\\), but only pairs of values \\((\\boldsymbol{x}_k,f(\\boldsymbol{x}_k))\\) for \\(k=1,\\cdots, d\\). Given a set of basis functions \\(\\boldsymbol{\\Phi}(\\boldsymbol{x})\\), we want to determine \\(\\hat{f}(\\boldsymbol{x}) := \\boldsymbol{\\Phi}^T(\\boldsymbol{x})\\boldsymbol{\\alpha}\\) such that the errors \\(f(\\boldsymbol{x}_k) - \\hat{f}(\\boldsymbol{x}_k)\\) for \\(k=1,\\cdots, d\\); is minimized in some sense. If the error is zero, we say that \\(\\hat{f}(\\boldsymbol{x})\\) interpolates the data, else it approximates the data.\nWe define the error vector \\(\\boldsymbol{e}\\) as \\[\n\\boldsymbol{e} := \\begin{bmatrix}e_1 \\\\ \\vdots \\\\ e_d \\end{bmatrix} =  \\begin{bmatrix} f(\\boldsymbol{x}_1) - \\hat{f}(\\boldsymbol{x}_1) \\\\ \\vdots \\\\ f(\\boldsymbol{x}_d) - \\hat{f}(\\boldsymbol{x}_d) \\end{bmatrix} =\\underbrace{\\begin{bmatrix} f(\\boldsymbol{x}_1)\\\\ \\vdots \\\\ f(\\boldsymbol{x}_d) \\end{bmatrix}}_{=: \\boldsymbol{b}} - \\underbrace{\\begin{bmatrix}\\boldsymbol{\\Phi}^T(\\boldsymbol{x}_1) \\\\ \\vdots \\\\ \\boldsymbol{\\Phi}^T(\\boldsymbol{x}_d)\\end{bmatrix}}_{=:\\boldsymbol{A}}\\boldsymbol{\\alpha}.\n\\]\nThe two norm of the error vector is given by, \\[\\begin{align*}\n\\|\\boldsymbol{e}\\|_2^2 &= \\|\\boldsymbol{A}\\boldsymbol{\\alpha}- \\boldsymbol{b} \\|_2^2, \\\\\n&= (\\boldsymbol{A}\\boldsymbol{\\alpha}- \\boldsymbol{b})^T(\\boldsymbol{A}\\boldsymbol{\\alpha}- \\boldsymbol{b}),\\\\\n&= \\boldsymbol{\\alpha}^T(\\boldsymbol{A}^T\\boldsymbol{A})\\boldsymbol{\\alpha}-2\\boldsymbol{\\alpha}^T\\boldsymbol{A}^T\\boldsymbol{b} + \\boldsymbol{b}^T\\boldsymbol{b},\n\\end{align*}\\] which is quadratic in \\(\\boldsymbol{\\alpha}\\) and the optimal solution is given by \\[\n\\boldsymbol{\\alpha}^\\ast := (\\boldsymbol{A}^T\\boldsymbol{A})^\\dagger \\boldsymbol{A}^T\\boldsymbol{b}.\n\\tag{9.5}\\]\nNote: If \\(\\boldsymbol{A}\\) is full rank, then \\(\\|\\boldsymbol{e}\\|_2^2=0\\) and we interpolate the data, i.e., the \\(\\hat{f}(\\boldsymbol{x})\\) passes through \\((\\boldsymbol{x}_k,f(\\boldsymbol{x}_k))\\).\n\n\n9.3.3.3 Least-Squares Approximation Using Legendre Polynomials.\nThe following Python code illustrates a method for approximating a function using Legendre polynomials on scattered 1D data. Initially, a set of scattered data points is generated. These data points (x) are randomly selected and associated with values (y) derived from a noisy sine wave. The core of this approach lies in constructing a Vandermonde-like matrix for Legendre polynomials. Each column of this matrix, denoted as \\(\\boldsymbol{A}\\), corresponds to a Legendre polynomial of a specific degree evaluated at the data points. To approximate the function, a least squares problem is formulated and solved using np.linalg.lstsq, which essentially returns the optimal solution given by Equation 9.5.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some scattered data\nnp.random.seed(0)\nx = np.sort(np.random.rand(100))\ny = np.sin(2 * np.pi * x) + np.random.normal(0, 0.1, x.size)\n\n# Function to approximate the data using Legendre polynomials with linear algebra\ndef legendre_approximation(x, y, degree):\n    # Construct the Vandermonde matrix for Legendre polynomials\n    A = np.zeros((len(x), degree + 1))\n    for i in range(degree + 1):\n        A[:, i] = np.polynomial.legendre.legval(x, [0]*i + [1])\n    \n    # Solve the least squares problem\n    coeffs, _, _, _ = np.linalg.lstsq(A, y, rcond=None)\n    return np.polynomial.legendre.Legendre(coeffs)\n\n# Approximating the data\ndegree = 5  # Degree of the Legendre polynomial\np = legendre_approximation(x, y, degree)\n\n# Plotting the original data and the approximation\nplt.figure(figsize=(12, 6))\nplt.scatter(x, y, label='Original data')\nplt.plot(x, p(x), label=f'Approximation with Legendre basis', color='red')\nplt.legend()\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(f'Function Approximation with Legendre Polynomials Up to Degree {degree}')\nplt.show()\n\n\n\n\n\n\nFunction approximation using Legendre polynomials.\n\n\n\n\n\n\n\n\n9.3.3.4 Runge Phenomenon\nThe Runge phenomenon is a notable issue in function approximation using polynomial interpolation. It occurs when high-degree polynomial interpolants are used to approximate certain types of functions, leading to counterintuitive and problematic results.\nThis behavior is most prominently seen when using equidistant interpolation points. As the degree of the interpolating polynomial increases, the approximation becomes excellent near the center of the interval but worsens towards the ends. The polynomial starts to exhibit large oscillations, particularly near the endpoints, which severely affects the accuracy of the interpolation. This counterintuitive result is contrary to the expectation that higher-degree polynomials should provide better approximations.\nThe Runge phenomenon highlights a fundamental limitation of polynomial interpolation, particularly with equidistant points. It can be mitigated by choosing points that are not evenly spaced, like Chebyshev nodes. Another solution is to use methods like spline interpolation, which involve breaking the range into smaller pieces and using lower-degree polynomials over each piece. This approach avoids the big swings and inaccuracies seen with high-degree polynomials.\nThe Runge phenomenon is demonstrated with the approximation of the functon \\(f(x) = \\frac{1}{1 + 25x^2}\\) using high-degree polynomials with both equidistant points (which leads to the Runge phenomenon) and Chebyshev nodes (to mitigate the phenomenon). It is demonstrated in the following Python code.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy.polynomial.polynomial import Polynomial\n\n# Define Runge's function\ndef runge_function(x):\n    return 1 / (1 + 25 * x**2)\n\n# Create equidistant and Chebyshev nodes\ndef create_nodes(n, type='equidistant'):\n    if type == 'equidistant':\n        return np.linspace(-1, 1, n)\n    elif type == 'chebyshev':\n        return np.cos((2 * np.arange(1, n + 1) - 1) / (2 * n) * np.pi)\n\n# Interpolate the function\ndef interpolate_runge(n, node_type):\n    x_nodes = create_nodes(n, node_type)\n    y_nodes = runge_function(x_nodes)\n    p = Polynomial.fit(x_nodes, y_nodes, deg=n-1)\n    return p\n\n# Number of nodes\nn = 15\n\n# Interpolate using equidistant nodes and Chebyshev nodes\np_equidistant = interpolate_runge(n, 'equidistant')\np_chebyshev = interpolate_runge(n, 'chebyshev')\n\n# Plotting\nx = np.linspace(-1, 1, 1000)\nplt.figure(figsize=(12, 6))\nplt.plot(x, runge_function(x), label='Runge Function', color='black')\nplt.plot(x, p_equidistant(x), label='Equidistant Nodes', linestyle='--')\nplt.plot(x, p_chebyshev(x), label='Chebyshev Nodes', linestyle='-.')\nplt.scatter(create_nodes(n, 'equidistant'), runge_function(create_nodes(n, 'equidistant')), color='blue')\nplt.scatter(create_nodes(n, 'chebyshev'), runge_function(create_nodes(n, 'chebyshev')), color='red')\nplt.legend()\nplt.title('Demonstration of Runge Phenomenon and its Mitigation')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\nFigure 9.3: Demonstration of Runge phenomenon and its mitigation.\n\n\n\n\n\n\n\n\n\n9.3.4 Local vs Global Basis Functions\nIn general, the basis functions could be globally or locally supported. Globally supported basis functions, such as polynomials and trigonometric functions, are defined over the entire domain of interest. They offer smoothness and continuity, making them ideal for approximating smooth, globally defined functions with high accuracy. However, they are sensitive to local changes and can be computationally intensive due to dense matrix systems, limiting their practicality for large-scale problems.\nIn contrast, local basis functions, like piecewise polynomials in spline interpolation and shape functions in finite element methods, are defined over small subdomains. They provide efficient computation and local control, effectively handling functions with local irregularities or discontinuities. Despite these advantages, ensuring continuity between elements can be challenging, and their piecewise nature adds complexity to their formulation. The choice between these two types of basis functions depends on the problem’s requirements: global basis functions are preferred for smooth, entire-domain applications, while local basis functions are favored for large-scale, locally varying problems.\nLocally supported functions are a class of functions in mathematics that are non-zero only over a specific, limited range and zero everywhere else. This property makes them highly valuable in various applications like finite element analysis, wavelet transforms, and spline interpolation. Below are some notable examples of locally supported functions, along with their mathematical details:\n\n9.3.4.1 Splines\nA spline is a piecewise polynomial function used in interpolation and approximation. In simple terms, a spline is a series of polynomial segments strung together, with each polynomial defined over a small subinterval.\nFor a spline of degree \\(n\\), each piece is a polynomial of degree \\(n\\) defined on a subinterval \\([x_i, x_{i+1}]\\). For example, a cubic spline (\\(n = 3\\)) is a piecewise-defined function where each piece is a cubic polynomial.\nSplines have continuity properties at the points where the polynomial pieces meet (called knots). For instance, cubic splines are often constructed to have continuous first and second derivatives across these knots.\nHere is a Python code that demonstrates cubic spline interpolation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n\n# Sample data points\nx = np.array([0, 1, 2, 3, 4, 5])\ny = np.array([0, 0.8, 0.9, 0.1, -0.8, -1.0])\n\n# Create a cubic spline interpolation of the data\ncubic_spline = interp1d(x, y, kind='cubic')\n\n# Generate more points to evaluate the spline\nx_fine = np.linspace(0, 5, 500)\ny_fine = cubic_spline(x_fine)\n\n# Plot the original data points and the interpolated values\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, 'o', label='Data points')\nplt.plot(x_fine, y_fine, '-', label='Cubic spline interpolation')\nplt.title('Spline Interpolation Example')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3.4.2 B-Splines\nB-splines or Basis splines are a generalization of the spline concept. They provide a basis for the spline space and are defined by a degree and a set of knot points. A B-spline of degree \\(n\\) is defined piecewise by polynomial segments of degree \\(n\\). Its shape is influenced by a set of control points, and it is non-zero only in a range defined by \\(n+1\\) consecutive knots. B-splines have local control (adjusting one control point affects the spline shape only in a local region) and are widely used in computer graphics and data fitting due to their stability and flexibility.\nThe following code shows the B-spline basis functions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import BSpline\n\n# Define the degree of the B-spline\ndegree = 3\n\n# Define a set of knots\n# Note: For partition of unity, we need to add degree+1 equal knots at both ends\nknots = np.array([0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4])\n\n# Generate B-spline basis functions\nx = np.linspace(knots[degree], knots[-degree-1], 100)\nbasis_functions = []\nfor i in range(len(knots) - degree - 1):\n    coeffs = np.zeros(len(knots) - degree - 1)\n    coeffs[i] = 1\n    basis_function = BSpline(knots, coeffs, degree)\n    basis_functions.append(basis_function(x))\n\n# Plot the B-spline basis functions\nplt.figure(figsize=(10, 6))\nfor i, basis_function in enumerate(basis_functions):\n    plt.plot(x, basis_function, label=f'B-spline basis {i+1}')\n# Plot the sum of all basis functions to demonstrate partition of unity\nplt.plot(x, np.sum(basis_functions, axis=0), 'k--', label='Sum of all basis functions');\n\nplt.title('B-spline Basis Functions and Partition of Unity')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nB-Spline basis functions satisfying partition of unity, i.e., the sum of all the basis function is equal to one at every point. Note that each basis function is locally supported, i.e., it goes to zero far away from the location of its peak value.\n\n\n\n\n\n\nWe next show how to interpolate data using B-Splines.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import splrep, splev\n\n# Sample data points\nx = np.linspace(0, 10, 10)\ny = np.sin(x)\n\n# Create a B-Spline representation of the data\ntck = splrep(x, y, s=0)\n\n# Generate more points to evaluate the spline\nx_fine = np.linspace(0, 10, 200)\ny_fine = splev(x_fine, tck)\n\n# Plot the original data points and the B-Spline interpolation\nplt.figure(figsize=(10, 6));\nplt.plot(x, y, 'o', label='Data points');\nplt.plot(x_fine, y_fine, '-', label='B-Spline interpolation');\nplt.title('B-Spline Interpolation Example');\nplt.xlabel('x');\nplt.ylabel('y');\nplt.legend();\nplt.show()\n\n\n\n\n\n\n\n\n\n\n9.3.4.3 Wavelets\nWavelets are functions used to divide a given function or continuous-time signal into different scale components. They have localized support in both time and frequency domains. A wavelet function \\(\\psi(t)\\) is typically defined over a finite interval and is used to generate a family of functions through scaling and translation: \\(\\psi_{a,b}(t) = \\frac{1}{\\sqrt{a}} \\psi\\left(\\frac{t - b}{a}\\right)\\), where \\(a\\) and \\(b\\) are the scaling and translation parameters. Wavelets are useful in signal processing and image compression, as they can represent data at different levels of resolution and are particularly effective in analyzing transient or high-frequency features.\nHere is a Python code that applies Daubechies wavelets to approximate a function. We first show the Daubechies wavelets followed by approximation of a sine function with it.\n\nimport pywt\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Function to plot a wavelet\ndef plot_wavelet(wavelet, ax, title):\n    wavelet_function, scaling_function, x_values = wavelet\n    ax.plot(x_values, wavelet_function, label=\"Wavelet Function\")\n    ax.plot(x_values, scaling_function, label=\"Scaling Function\")\n    ax.set_title(title)\n    ax.legend()\n\n# Create a figure with subplots\nfig, axs = plt.subplots(2, 2, figsize=(12, 8))\n\n# List of Daubechies wavelets to plot\nwavelets = ['db1', 'db2', 'db3', 'db4']\n\n# Plot each wavelet\nfor i, wavelet_name in enumerate(wavelets):\n    wavelet = pywt.Wavelet(wavelet_name)\n    phi, psi, x = pywt.Wavelet(wavelet_name).wavefun(level=5)\n    plot_wavelet((psi, phi, x), axs[i // 2, i % 2], f'Daubechies {wavelet_name.upper()}')\n\n# Display the plot\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nDaubechies wavelets with increasing complexity.\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pywt\n\n# Define the function to approximate\ndef f(x):\n    return np.sin(x)\n\n# Generate sample data\nx = np.linspace(0, 4 * np.pi, 400)\ny = f(x)\n\n# Choose a wavelet type and level of decomposition\nwavelet_type = 'db1'  # Daubechies wavelet\nlevel = 3\n\n# Compute the wavelet coefficients\ncoeffs = pywt.wavedec(y, wavelet_type, level=level)\n\n# Reconstruct the signal from the coefficients\ny_approx = pywt.waverec(coeffs, wavelet_type)\n\n# Plot the original and approximated function\nplt.figure(figsize=(10, 6))\nplt.plot(x, y, label='Original Function')\nplt.plot(x, y_approx, label='Wavelet Approximation', linestyle='--')\nplt.title('Function Approximation Using Wavelets')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nApproximation of \\(\\sin(x)\\) with Daubechies wavelets.\n\n\n\n\n\n\n\n\n9.3.4.4 Radial Basis Functions\nRadial Basis Functions (RBFs) are a class of functions with radial symmetry. The value of the function depends only on the distance from a central point, termed the center of the RBF. This distance is typically measured using the Euclidean norm, making the function spherically symmetric in multidimensional space.\nMathematically, an RBF \\(\\phi\\) is a function such that \\(\\phi(\\boldsymbol{x}) = \\phi(\\|\\boldsymbol{x} - \\boldsymbol{c}\\|)\\), where \\(\\boldsymbol{x}\\) is a point in space, \\(\\boldsymbol{c}\\) is the center of the RBF, and \\(\\|\\boldsymbol{x} - \\boldsymbol{c}\\|\\) is the distance from \\(\\boldsymbol{x}\\) to \\(\\boldsymbol{c}\\).\nCommon Types of Radial Basis Functions:\n\nGaussian: \\(\\phi(r) = e^{-(\\varepsilon r)^2}\\), where \\(\\varepsilon\\) is a scaling parameter. Gaussian RBFs are widely used due to their smooth and rapidly decaying nature.\nMultiquadric: \\(\\phi(r) = \\sqrt{1 + (\\varepsilon r)^2}\\). These RBFs are smooth and have been used effectively in various interpolation tasks.\nInverse Multiquadric: \\(\\phi(r) = \\frac{1}{\\sqrt{1 + (\\varepsilon r)^2}}\\). They have a similar form to multiquadric RBFs but behave differently as the distance increases.\nPolyharmonic Splines: \\(\\phi(r) = r^k\\), where \\(k\\) is typically an integer. These functions are particularly useful for their smoothness properties in higher dimensions.\n\nRadial Basis Functions stand out due to their flexibility and effectiveness in handling multidimensional data, their ability to create smooth interpolants, and their applicability in a wide range of scientific and engineering disciplines. Their unique radial property ensures that the influence of a point diminishes with distance, making them suitable for localized approximations in high-dimensional spaces.\nThe following code uses multiquadric radial basis functions to approximate \\(\\sin(x)\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import Rbf\n\n# Define the function to approximate\ndef f(x):\n    return np.sin(x)\n\n# Generate sample data (you can change this to any other function or data points)\nx_sample = np.linspace(0, 10, 10)\ny_sample = f(x_sample)\n\n# Create the Radial Basis Function interpolator\nrbf_interpolator = Rbf(x_sample, y_sample, function='multiquadric')\n\n# Generate more points to evaluate the RBF interpolator\nx_fine = np.linspace(0, 10, 100)\ny_fine = rbf_interpolator(x_fine)\n\n# Plot the original function, sample points, and RBF approximation\nplt.figure(figsize=(10, 6))\nplt.plot(x_sample, y_sample, 'o', label='Sample points')\nplt.plot(x_fine, f(x_fine), label='Original function')\nplt.plot(x_fine, y_fine, label='RBF approximation', linestyle='--')\nplt.title('Function Approximation using Radial Basis Functions')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nRBF interpolation is especially useful for scattered data in multiple dimensions, but this example shows its application in a simple 1D case for simplicity.\nWe next show how multiquadric RBFs can be used to approximate \\(\\sin(x)\\cos(y)\\) from scattered data.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import Rbf\n\n# Define the 2D function to approximate\ndef f(x, y):\n    return np.sin(x) * np.cos(y)\n\n# Create scattered data points\nnSamples = 200;\nx_sample = np.random.uniform(0, 5, nSamples)\ny_sample = np.random.uniform(0, 5, nSamples)\nz_sample = f(x_sample, y_sample)\n\n# Create the Radial Basis Function interpolator\nrbf_interpolator = Rbf(x_sample, y_sample, z_sample, function='multiquadric')\n\n# Create a grid to evaluate the interpolator\nx_grid, y_grid = np.meshgrid(np.linspace(0, 5, 100), np.linspace(0, 5, 100))\nz_grid = rbf_interpolator(x_grid, y_grid)\n\n# Plot the results\nfig = plt.figure(figsize=(12, 6))\n\n# Original function\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(x_grid, y_grid, f(x_grid, y_grid), cmap='viridis')\nax1.set_title('Original Function')\nax1.set_xlabel('X')\nax1.set_ylabel('Y')\nax1.set_zlabel('f(X, Y)')\n# RBF Interpolation\nax2 = fig.add_subplot(122, projection='3d')\nax2.plot_surface(x_grid, y_grid, z_grid, cmap='viridis')\nax2.scatter(x_sample, y_sample, 0.1+z_sample, color='red')  # Scattered data points. Added 0.1 to make the dots visible.\nax2.set_title('RBF Interpolation')\nax2.set_xlabel('X')\nax2.set_ylabel('Y')\nax2.set_zlabel('f(X, Y)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFunction approximation in 2D with radial basis functions."
  },
  {
    "objectID": "data_pre_processing.html#data-cleaning",
    "href": "data_pre_processing.html#data-cleaning",
    "title": "10  Data Pre-Processing",
    "section": "10.1 Data Cleaning",
    "text": "10.1 Data Cleaning\nData cleaning is a crucial step in the data preprocessing phase, involving the correction or removal of incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. We next describe some of various data cleaning techniques, with real-world examples and Python code for each.\n\n10.1.1 Handling Missing Values\nHandling missing values is a fundamental aspect of data cleaning in machine learning and data analysis. Missing data can arise due to various reasons like errors during data collection, processing, or transmission. Proper handling of these missing values is crucial as they can lead to biased or incorrect results. Some of the common techniques are summarized next.\nNote that the choice of method depends on the nature of the data, the extent of missingness, and the analysis or modeling task. It is important to consider the potential biases introduced by missing data and the chosen method to handle it. In some cases, it might be informative to create an additional binary feature indicating whether data was missing.\n\n10.1.1.1 Mean/Median/Mode Imputation\nReplacing missing values with the mean, median, or mode of the column. This method is simple and works well with numerical data. The mean is typically used for normal distributions, while the median is better for skewed distributions. The mode is used for categorical data.\nLet us consider an engineering application involving sensor data from a manufacturing process. In such settings, sensors might record various parameters like temperature, pressure, and operational speed of machinery. It is common to have missing values due to sensor malfunctions or transmission errors. We’ll create a Python example where we handle missing values in such a dataset using mean and median imputation:\n\nimport pandas as pd\nimport numpy as np\n\n# Example DataFrame representing sensor data in a manufacturing process\n# Columns: Temperature (°C), Pressure (kPa), Speed (RPM)\ndata = {\n    'Temperature': [200, 205, np.nan, 210, 208, np.nan, 207],\n    'Pressure': [30, 35, 34, np.nan, 36, 37, np.nan],\n    'Speed': [1500, 1495, 1500, 1502, np.nan, 1498, 1501]\n}\n\ndf = pd.DataFrame(data)\n\n# Mean Imputation for Temperature\n# Assuming temperature readings are relatively stable and normally distributed\ndf['Temperature'].fillna(df['Temperature'].mean(), inplace=True)\n\n# Median Imputation for Pressure\n# Pressure might have occasional spikes; median is more robust to outliers\ndf['Pressure'].fillna(df['Pressure'].median(), inplace=True)\n\n# Speed is a crucial operational parameter, so we might decide not to impute and keep it as is\n\nprint(df)\n\n   Temperature  Pressure   Speed\n0        200.0      30.0  1500.0\n1        205.0      35.0  1495.0\n2        206.0      34.0  1500.0\n3        210.0      35.0  1502.0\n4        208.0      36.0     NaN\n5        206.0      37.0  1498.0\n6        207.0      35.0  1501.0\n\n\nIn this example, we have a DataFrame df representing sensor data. We use:\n\nMean Imputation for the ‘Temperature’ column. We assume temperature readings are normally distributed and don’t vary wildly in short periods, which is often the case in controlled industrial environments with Gaussian sensor noise.\nMedian Imputation for the ‘Pressure’ column, as pressure can have occasional spikes due to sudden changes in the manufacturing process. Median imputation helps to mitigate the impact of such outliers.\nWe choose not to impute values for the ‘Speed’ column, considering that missing values in this crucial operational parameter might need special attention or indicate critical issues that imputation might mask.\n\nThis example illustrates how different imputation methods can be applied in an engineering context, considering the nature and criticality of the data being handled.\n\n\n10.1.1.2 Custom Value\nCustom value imputation is a technique where missing data in a dataset is filled with a predefined or custom value. This approach is particularly useful in scenarios where it is important to distinctly identify or separate the missing data from the naturally occurring data. In engineering contexts, this method can be used to maintain consistency, signal a specific condition, or ensure that the processing algorithms function correctly without introducing biases.\nConsider a dataset from an industrial monitoring system that tracks the operational status of various machines in a factory. The dataset contains readings such as temperature, pressure, and a status code indicating the machine’s condition (e.g., 0 for normal, 1 for maintenance required, etc.). Sometimes, the status code might be missing due to communication issues or sensor errors.\nIn such a case, we might choose to fill the missing status codes with a custom value that does not overlap with existing status codes. For example, we could use -1 to indicate missing or unknown status. This approach makes it clear that the data was missing and not a part of the normal operational readings.\n\nimport pandas as pd\nimport numpy as np\n\n# Sample DataFrame representing machine operational data\ndata = {\n    'Temperature': [200, 205, 210, 208, 207],\n    'Pressure': [30, 35, 34, 36, 37],\n    'StatusCode': [0, 1, np.nan, 0, np.nan]  # 0: normal, 1: maintenance required\n}\n\ndf = pd.DataFrame(data)\n\n# Custom Value Imputation for StatusCode\n# -1 will indicate missing or unknown status\ndf['StatusCode'].fillna(-1, inplace=True)\n\nprint(df)\n\n   Temperature  Pressure  StatusCode\n0          200        30         0.0\n1          205        35         1.0\n2          210        34        -1.0\n3          208        36         0.0\n4          207        37        -1.0\n\n\nIn this code we create a DataFrame df with ‘Temperature’, ‘Pressure’, and ‘StatusCode’ columns. Missing values in the ‘StatusCode’ column are replaced with -1, a custom value indicating unknown status. By using a custom value for imputation, engineers and analysts can easily distinguish between normal data and data that was missing or not recorded, which is essential for accurate monitoring and decision-making in industrial settings.\n\n\n10.1.1.3 Predictive Imputation\nPredictive imputation is a sophisticated method of handling missing data in a dataset. Unlike simpler methods like mean or median imputation, predictive imputation uses the relationships found in the non-missing parts of the data to predict and fill in the missing values. This approach often leads to more accurate and realistic data imputation, especially when the missing data is not random and depends on other variables in the dataset.\nThe process begins with the development of a model using the portion of the dataset that does not contain any missing values. This model is tailored to predict the missing variable using other related variables in the dataset as inputs. Essentially, it learns the relationships and patterns present in the complete data to estimate the missing values.\nOnce the model is established and trained, it is then employed to predict the missing values. For each instance in the dataset where the target variable is missing, the model uses the available, non-missing data to make a prediction. It applies the relationships it has learned to estimate the missing value as accurately as possible.\nThe final step in predictive imputation is the substitution of these predicted values back into the dataset. The missing values are replaced with these newly predicted values, thus filling the gaps in the dataset. This method ensures a more informed and potentially accurate way of dealing with missing data, as it takes into account the underlying patterns and correlations in the data, unlike simpler methods such as mean or median imputation. However, the success of predictive imputation largely depends on the accuracy and appropriateness of the predictive model used, making it crucial to choose and tune the model carefully based on the specific characteristics of the data. If physics-based models are available, it can be also be used predict missing data.\nPredictive imputation stands out for its ability to yield more accurate results compared to simpler imputation methods, largely because it takes into account the correlations and relationships between different variables in the dataset. This makes it especially advantageous in situations where the missingness of data is not random but instead is influenced by other variables, allowing for a more context-aware approach to filling in missing values. However, this technique comes with its own set of challenges. The process of building and validating an appropriate predictive model can be complex and time-consuming, requiring careful consideration and expertise. There’s also a risk of overfitting, particularly if the model is too complex, which can result in imputations that are less reliable as they may overly conform to the specifics of the available data rather than general patterns. Furthermore, the effectiveness of predictive imputation hinges significantly on the assumptions underpinning the chosen model and its alignment with the data’s characteristics. Any misalignment or incorrect assumptions can adversely affect the accuracy of the imputed values.\nLet’s consider an engineering scenario where we have a dataset from a wind turbine farm. This dataset includes variables like wind speed, turbine rotation speed, and power output. Sometimes, due to sensor errors or data transmission issues, we might have missing values in the power output, which we want to predict based on the other variables.\nWe’ll use a simple linear regression model for predictive imputation. This example assumes that a linear relationship exists between the wind speed, turbine rotation speed, and the power output of the turbines. Here’s the Python code demonstrating this:\n\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\n# Sample DataFrame representing wind turbine data\n# Columns: WindSpeed (m/s), TurbineSpeed (RPM), PowerOutput (kW)\ndata = {\n    'WindSpeed': [5.2, 7.4, 6.5, np.nan, 7.0, 5.5, 6.8],\n    'TurbineSpeed': [1200, 1400, 1300, 1250, 1350, 1280, np.nan],\n    'PowerOutput': [300, 400, 350, np.nan, 390, 320, 360]\n}\n\ndf = pd.DataFrame(data)\n\n# Imputing missing values in WindSpeed and TurbineSpeed with their means\nimputer = SimpleImputer(strategy='mean')\ndf['WindSpeed'] = imputer.fit_transform(df[['WindSpeed']])\ndf['TurbineSpeed'] = imputer.fit_transform(df[['TurbineSpeed']])\n\n# Separating the dataset into two - one where PowerOutput is missing and one where It is available\ndf_missing_power = df[df['PowerOutput'].isna()]\ndf_complete = df.dropna(subset=['PowerOutput'])\n\n# Training a linear regression model to predict PowerOutput\nmodel = LinearRegression()\nmodel.fit(df_complete[['WindSpeed', 'TurbineSpeed']], df_complete['PowerOutput'])\n\n# Predicting the missing PowerOutput values\npredicted_power = model.predict(df_missing_power[['WindSpeed', 'TurbineSpeed']])\ndf_missing_power.loc[:,'PowerOutput'] = predicted_power\n\n# Combining the data back together\ndf_imputed = pd.concat([df_complete, df_missing_power])\n\nprint(df_imputed)\n\n   WindSpeed  TurbineSpeed  PowerOutput\n0        5.2   1200.000000   300.000000\n1        7.4   1400.000000   400.000000\n2        6.5   1300.000000   350.000000\n4        7.0   1350.000000   390.000000\n5        5.5   1280.000000   320.000000\n6        6.8   1296.666667   360.000000\n3        6.4   1250.000000   341.876212\n\n\nIn this code, we create a DataFrame df with simulated data for wind speed, turbine speed, and power output. Missing values in ‘WindSpeed’ and ‘TurbineSpeed’ are imputed using the mean of their respective columns. Next, we train a linear regression model on the part of the dataset where ‘PowerOutput’ is not missing, using ‘WindSpeed’ and ‘TurbineSpeed’ as predictors. The model is then used to predict missing ‘PowerOutput’ values. Finally, we combine the imputed data back into a single DataFrame.\nNote: This approach assumes a linear relationship between the variables, which might be an oversimplification in real-world scenarios. In practice, more complex models and validation methods would likely be necessary to accurately predict missing values in an engineering context.\n\n\n10.1.1.4 Dropping Rows or Columns\nIn engineering and similar data-intensive fields, dropping rows and columns from a dataset is a common data cleaning practice, particularly essential when dealing with issues such as missing data, irrelevant information, or erroneous entries.\nThe decision to drop rows is often made when they contain a high proportion of missing values or outliers that could skew analysis results. This is particularly relevant in engineering scenarios like sensor data analysis, where missing or aberrant readings can significantly distort the interpretation.\nDropping columns is typically considered when the data they contain is irrelevant, highly correlated with other columns, or so predominantly missing as to render them uninformative. For instance, in an engineering dataset, a feature that remains constant across all data points (such as a specific component in a machine that doesn’t vary) may be dropped, as it doesn’t contribute to the variability or insights sought in the analysis.\nWhile dropping rows and columns can simplify and improve the quality of the data, it must be done judiciously to avoid losing valuable information or introducing bias. The process should be well-documented and justified, considering the specific goals and context of the analysis. Python’s pandas library simplifies this task with functions like dropna() for rows and drop() for columns, facilitating efficient data cleaning in engineering data processing workflows.\n\n\n\n10.1.2 Smoothing Noisy Data\nSmoothing noisy data is an important step in data preprocessing, especially in fields like signal processing, finance, and engineering where data quality can significantly impact analysis and decision-making. Several techniques are commonly used to smooth out noise, each with its own advantages and disadvantages.\n\n10.1.2.1 Binning\nBinning is a data smoothing technique commonly used to reduce the effects of minor observation errors. The main idea is to transform continuous numeric variables into discrete categories, or “bins”. Each original data value is replaced by a value representative of its bin, often the bin’s mean or median.\nBinning encompasses three main steps. First, the range of continuous data is segmented into a series of intervals, commonly referred to as bins. This segmentation is a crucial step as it determines the granularity of the analysis. In the next step, each individual data point from the dataset is allocated to one of these bins, based on where it falls within the range. The final step involves replacing the data points in each bin with a representative value of that bin, which is often the mean or median. This representative value stands in for all the data points in the bin, effectively smoothing out the data by reducing the impact of minor variations or outliers within each bin.\nSuppose we have a dataset of temperature readings that we wish to bin. We can use Python’s pandas library to accomplish this, as shown next.\n\nimport pandas as pd\n\n# Sample data: temperature readings\ntemperature_data = [68, 71, 74, 69, 70, 73, 65, 67, 72, 70, 75]\n\n# Convert to DataFrame\ndf = pd.DataFrame(temperature_data, columns=['Temperature'])\n\n# Define the number of bins\nnum_bins = 3\n\n# Create bins\ndf['Temp_Binned'] = pd.cut(df['Temperature'], bins=num_bins)\n\n# Replace each value by the mean of its bin\ndf['Temp_Binned_Mean'] = df.groupby('Temp_Binned',observed=False)['Temperature'].transform('mean')\n\nprint(df)\n\n    Temperature       Temp_Binned  Temp_Binned_Mean\n0            68   (64.99, 68.333]         66.666667\n1            71  (68.333, 71.667]         70.000000\n2            74    (71.667, 75.0]         73.500000\n3            69  (68.333, 71.667]         70.000000\n4            70  (68.333, 71.667]         70.000000\n5            73    (71.667, 75.0]         73.500000\n6            65   (64.99, 68.333]         66.666667\n7            67   (64.99, 68.333]         66.666667\n8            72    (71.667, 75.0]         73.500000\n9            70  (68.333, 71.667]         70.000000\n10           75    (71.667, 75.0]         73.500000\n\n\nIn the above example, a series of temperature readings is initially organized into a pandas DataFrame. To smooth these data, the readings are categorized into three distinct bins by employing the pd.cut() function. Following this, each temperature reading is replaced with the average value of its respective bin. This is achieved by segmenting the DataFrame based on the binned temperature categories and then computing the mean temperature for each category.\nBinning offers notable advantages. It is straightforward to implement and effectively smooths the data, thereby minimizing the impact of minor variances. However, there are also drawbacks to consider. The size of the bins chosen can greatly influence the outcome, potentially leading to significant alterations in the results. Additionally, this method might result in a loss of detailed information and the original data’s variability, as it essentially generalizes the data points within each bin.\nThis method is particularly useful in situations where the data contains minor inaccuracies or when preparing data for certain types of categorical analysis. It helps to mitigate the impact of minor errors or variances in the measurements.\n\n\n10.1.2.2 Regression\nThe regression-based technique for smoothing noisy data involves constructing a statistical model that best fits the observed data. Essentially, this method fits a regression model (such as linear, polynomial, or logistic regression, depending on the data nature) to the dataset. Once this model is established, it is used to predict the expected values for each data point. These predicted values, which are based on the underlying pattern identified by the regression model, are generally smoother and less noisy compared to the original data. There are many advantages to this approach. It is capable of handling complex relationships within the data, making it particularly valuable in scenarios where the underlying data patterns are intricate and not immediately apparent. Additionally, it is most effective when the noise present in the data is random, as the regression model focuses on capturing and representing the fundamental trend or pattern in the data.\nHowever, there are some disadvantages to this technique. One significant risk is overfitting, especially if the chosen model is overly complex. Overfitting occurs when the model becomes too tailored to the specific idiosyncrasies of the training data, reducing its ability to generalize and perform well on new, unseen data. This issue is particularly pertinent when dealing with large datasets or datasets with many features, where the model may inadvertently learn noise as if it were a valid signal. Furthermore, regression models, especially complex ones, can demand substantial computational resources, making them less practical for very large datasets or scenarios where computational efficiency is a priority. These drawbacks necessitate a careful balance in model selection and complexity, ensuring that the model is sophisticated enough to capture the essential patterns in the data, but not so complex that it overfits or becomes computationally unmanageable.\n\n\n10.1.2.3 Moving Average\nThe moving average technique is a widely used method for smoothing noisy data, particularly in time series analysis. It involves calculating the average of a specified number of data points within a moving window that progresses through the dataset. For example, in a simple moving average, each data point is replaced with the average of itself and the surrounding data points within the window. This method effectively dampens short-term fluctuations and highlights longer-term trends or cycles in the data.\nAdvantage: One of the main advantages of the moving average technique is its simplicity and ease of implementation. It requires minimal computational resources, making it suitable for large datasets and real-time data analysis. Additionally, it’s intuitive and straightforward to interpret, which is particularly beneficial for quick insights and initial data exploration.\nTo demonstrate the advantage of moving average smoothing, let’s create a Python example where we apply this technique to a time series data set with some random noise. The goal is to see how the moving average can smooth out short-term fluctuations and reveal underlying trends in the data. We’ll start by generating a simple time series dataset with added random noise and then apply a moving average smoothing technique to it:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample time series data\nnp.random.seed(0)\ntime = np.arange(100)\ndata = np.sin(time / 10) + np.random.normal(scale=0.5, size=time.size)\n\n# Convert to DataFrame for convenience\ndf = pd.DataFrame({'Time': time, 'Data': data})\n\n# Apply moving average with a window of 10\nwindow_size = 10\ndf['Moving_Average'] = df['Data'].rolling(window=window_size).mean()\n\n# Plotting the original data and the smoothed data\nplt.figure(figsize=(12, 6))\nplt.plot(df['Time'], df['Data'], label='Original Data')\nplt.plot(df['Time'], df['Moving_Average'], label='Moving Average', color='red')\nplt.title('Effect of Moving Average Smoothing')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nFigure 10.1: Demonstration of moving average as a technique for data de-noising.\n\n\n\n\n\n\nIn this code, we create a time series dataset (data) that represents some signal (in this case, a sine wave) with added Gaussian noise. The data is put into a pandas DataFrame for ease of manipulation. We then apply a moving average smoothing with a window size of 10. This means each point in the ‘Moving_Average’ column is the average of the current and the previous 9 points in the ‘Data’ column. Finally, we plot both the original and the smoothed data. Figure 10.1 shows how the moving average smooths out the fluctuations and reveals the underlying sine wave pattern more clearly.\nDisadvantage: However, the moving average technique also has its limitations. A significant disadvantage is its inability to handle rapid changes or non-linear trends in the data effectively. The method tends to “lag” behind the actual data, especially with larger window sizes, meaning that it might not be responsive enough to recent changes in the data. Furthermore, the choice of the window size is crucial; a window that is too small may not smooth the data sufficiently, while a window that is too large can oversmooth the data, potentially obscuring important details and patterns. Additionally, the moving average does not differentiate between older and more recent data points within the window, treating all points with equal importance, which might not always be appropriate depending on the data’s nature and the analysis’s goals.\nThe following Python code demonstrate a disadvantage of using the moving average for data denoising, especially in the context of rapidly changing or non-linear trends. The code generates a time series dataset that includes a sudden change in trend, highlighting a key disadvantage of the moving average smoothing method. Initially, random data is generated to simulate a scenario where the data abruptly shifts from one trend to another. Specifically, for the first half of the time series, the data follows a normal distribution with a mean of 0, and then it suddenly shifts to a different normal distribution with a mean of 5, representing a distinct change in trend.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample time series data with a sudden change\nnp.random.seed(0)\ntime = np.arange(100)\ndata = np.where(time &lt; 50, np.random.normal(0, 0.5, time.size), np.random.normal(5, 0.5, time.size))\n\n# Convert to DataFrame\ndf = pd.DataFrame({'Time': time, 'Data': data})\n\n# Apply moving average with a window of 10\nwindow_size = 10\ndf['Moving_Average'] = df['Data'].rolling(window=window_size).mean()\n\n# Plotting the original data and the smoothed data\nplt.figure(figsize=(12, 6))\nplt.plot(df['Time'], df['Data'], label='Original Data')\nplt.plot(df['Time'], df['Moving_Average'], label='Moving Average', color='red')\nplt.title('Disadvantage of Moving Average in Rapid Trend Changes')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nFigure 10.2: Limitations of moving average data denoising.\n\n\n\n\n\n\nThis dataset is converted into a pandas DataFrame for ease of manipulation. A moving average is then applied to this data with a window size of 10. This window size means that each point in the ‘Moving_Average’ column of the DataFrame represents the average of the current and the previous nine points in the ‘Data’ column.\nThe key point of interest is observed in Figure 10.2, where both the original and smoothed data are plotted. The plot clearly illustrates the disadvantage of the moving average in this scenario. Due to the moving average’s inherent nature, it lags in responding to the sudden change in trend. This lag is particularly noticeable at the point where the data shifts from one distribution to another. This lag can lead to misinterpretation or delay in recognizing significant changes in the data, which is a critical drawback in applications where timely and accurate response to data trends is essential. ​\n\n\n10.1.2.4 Exponential Smoothing\nExponential smoothing is a popular data smoothing technique, particularly effective in time series forecasting. It operates by applying decreasing weights to past observations, with the most recent observations given more significance. The fundamental principle is that more recent data is a better reflector of the future, hence the exponential decrease in weight for older data. This weighting is achieved through a smoothing factor, typically denoted as \\(\\alpha\\), which determines how rapidly the weights decrease; a higher alpha places more emphasis on recent observations.\nAdvantages: One of the key advantages of exponential smoothing is its adaptability to changes in data trends and patterns. Unlike simple moving averages that treat all points in the window equally, exponential smoothing can quickly adjust to recent changes, making it more responsive to shifts in the underlying data pattern. This feature makes it particularly useful for forecasting in scenarios where data trends are dynamic and evolving.\nHere is a Python code example demonstrating the use of exponential smoothing on a time series dataset. We use the same data as shown in Figure 10.2, where the simple moving average was lagging.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate sample time series data with a linear trend\nnp.random.seed(0)\ntime = np.arange(100)\ndata = np.where(time &lt; 50, np.random.normal(0, 0.5, time.size), np.random.normal(5, 0.5, time.size))\n\n# Convert to DataFrame\ndf = pd.DataFrame({'Time': time, 'Data': data})\n\n# Apply exponential smoothing\nalpha = 0.3  # Smoothing factor\ndf['Exponential_Smoothing'] = df['Data'].ewm(alpha=alpha).mean()\n\n# Plotting the original data and the smoothed data\nplt.figure(figsize=(12, 6))\nplt.plot(df['Time'], df['Data'], label='Original Data')\nplt.plot(df['Time'], df['Exponential_Smoothing'], label='Exponential Smoothing', color='red')\nplt.title('Exponential Smoothing Demonstration')\nplt.xlabel('Time')\nplt.ylabel('Value')\nplt.legend()\nplt.show()\n\n\n\n\n\n\nFigure 10.3: Exponential smoothing of a noisy data with sudden jumps in trends.\n\n\n\n\n\n\nFigure 10.3 shows both the original noisy data and the smoothed data. It illustrates how exponential smoothing effectively smooths out the noise, and is more responsive to changes in trends in comparison with simple moving average shown in Figure 10.2.\nLimitations: However, the method also has its drawbacks. Choosing the right smoothing factor \\(\\alpha\\) can be challenging and may require trial and error or optimization techniques. If \\(\\alpha\\) is set too high, the smoothed data might become too sensitive to recent changes, capturing random fluctuations rather than the underlying trend. Conversely, if \\(\\alpha\\) is too low, the method becomes similar to a moving average, potentially lagging behind recent trends. Another limitation is its simplicity; exponential smoothing is best suited for univariate time series without seasonality or trend components. For more complex data structures, more sophisticated methods like Holt-Winters exponential smoothing, which extends the basic idea to capture seasonality and trends, are often required. We can change \\(\\alpha\\) in the above code to \\(0.1\\) to see increased lag in the prediction, which is worse than the simple moving average example.\nWhile exponential smoothing is a valuable tool in time series analysis, especially for its quick adaptation to recent data changes and its ease of implementation. However, its effectiveness is dependent on the appropriate setting of the smoothing factor and the complexity of the time series data. It works best for simpler, non-seasonal series and might not be suitable for more complex datasets with multiple influencing factors.\n\n\n10.1.2.5 Filtering (e.g., Low-pass filters)\nLow-pass filtering is a technique used in both signal processing and machine learning to reduce noise in data. It works by allowing signals with a frequency lower than a certain cutoff frequency to pass through and attenuating frequencies higher than this cutoff. The rationale and application in machine learning can be understood through the following points:\n\nNature of Noise: In many real-world datasets, noise is often high-frequency. This means that the unwanted variations or fluctuations in the data occur at short intervals. These could be due to various factors like measurement errors, anomalies, or irrelevant variations.\nSmoothing Data: Low-pass filters help in smoothing the data by removing these high-frequency fluctuations. This can be particularly useful in time-series data or any data with a temporal or spatial dimension, where smoothness often corresponds to more meaningful trends and patterns.\nPreserving Relevant Information: By choosing an appropriate cutoff frequency, a low-pass filter can preserve the essential, low-frequency components of the data. These low-frequency components often represent the underlying trends or the ‘signal’ in the data that are of interest for analysis and model training.\nImproving Model Performance: In machine learning, models trained on noisy data can overfit, meaning they learn the noise as part of the signal, which reduces their ability to generalize to new, unseen data. By denoising the data, low-pass filtering can improve the generalization ability of these models.\nImplementation: Low-pass filtering can be implemented in several ways, such as using Fourier transforms, moving averages, or specific digital filter designs like Butterworth or Chebyshev filters. The choice of method depends on the nature of the data and the specific requirements of the task.\n\nAdvantages: Low-pass filtering is a valuable preprocessing step at reducing high-frequency noise, which is a common characteristic of many types of noise in data, especially in signal processing and time-series analysis. These filters can preserve the main signal or trend in the data if the cutoff frequency is chosen correctly. This makes them particularly useful for applications where the signal of interest is of lower frequency. Low-pass filters are generally simple to implement and computationally efficient. They don’t require extensive computational resources, making them suitable for real-time applications. By removing noise, they can improve the performance of machine learning models, reducing the chance of overfitting and enhancing the model’s ability to generalize from training to unseen data. There are various types of low-pass filters (e.g., Butterworth, Chebyshev), offering flexibility to choose one that best fits the specific characteristics of the data. Here is a Python code demonstrating advantages of using low-pass filtering.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, lfilter\n\n# Function to create a Butterworth low-pass filter\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\n# Function to apply the low-pass filter\ndef lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n# Sample rate and desired cutoff frequency of the filter\nfs = 1000  # Sample rate, Hz\ncutoff = 3  # Desired cutoff frequency of the filter, Hz\n\n# Generate a clean sinusoidal signal\nT = 5.0     # seconds\nn = int(T * fs)  # total number of samples\nt = np.linspace(0, T, n, endpoint=False)\nclean_signal = np.sin(1.2 * 2 * np.pi * t)\n\n# Add noise to the signal\nnoise = np.random.normal(0, 0.5, clean_signal.shape)\nnoisy_signal = clean_signal + noise\n\n# Filter the noisy signal\nfiltered_signal = lowpass_filter(noisy_signal, cutoff, fs)\n\n# Plotting\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 3, 1)\nplt.plot(t, clean_signal)\nplt.title('Clean Signal')\nplt.grid()\n\nplt.subplot(1, 3, 2)\nplt.plot(t, noisy_signal)\nplt.title('Noisy Signal')\nplt.grid()\n\nplt.subplot(1, 3, 3)\nplt.plot(t, filtered_signal,label=\"Filtered Signal\")\nplt.plot(t, clean_signal,'--',label=\"Clean Signal\")\nplt.title('Filtered Signal (and Clean Signal)')\nplt.legend()\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 10.4: Low-pass filtering to denoise data.\n\n\n\n\n\n\nFigure 10.4 shows three plots: the original clean sinusoidal signal, the same signal with added noise, and the noisy signal after applying a low-pass filter. We see that the filtered signal is devoid of any noise, but has a magnitude and phase distortion when compared to the clean signal.\nLimitations: Low-pass filtering, while effective for reducing high-frequency noise, presents several challenges and limitations. A significant concern is the potential loss of valuable high-frequency information, which can be detrimental if the signal of interest includes crucial high-frequency components. Additionally, selecting the correct cutoff frequency is a critical yet often challenging task; an inappropriate cutoff may either inadequately filter noise or inadvertently eliminate important signal elements. Low-pass filters can also introduce phase shifts or distortions, which are problematic in scenarios where signal phase is key. There’s also a risk of oversmoothing with excessive use of these filters, leading to a loss of important data variations and details. Moreover, these filters are specifically tailored for high-frequency noise and may not be suitable for low-frequency or broadly distributed noise types. Finally, in the context of time-series or spatial data, low-pass filtering can cause edge effects at data boundaries, where the performance of the filter may be compromised due to insufficient surrounding data points. These limitations highlight the need for careful consideration and application of low-pass filtering in data processing and analysis.\nThe following Python code demonstrates the disadvantages of using low-pass filtering in denoising data. The example focuses on two primary concerns: the loss of significant high-frequency signal components and the possibility of phase shift. We first create a composite signal with both low and high-frequency elements, mimicking a real-world scenario where a signal carries multiple frequencies. Then we add a random noise to it, simulating typical real-life interference. The noisy data is then smoothened using a low-pass filter.\nWhile the filter’s primary objective is to remove noise, it is important to observe its impact on the high-frequency components of the signal. Low-pass filtering, by design, attenuates high-frequency elements, and this effect becomes evident in this example. Finally, the example involves a comparison among the original signal, the noisy version, and the filtered output. This comparison is crucial as it highlights the loss of high-frequency details due to filtering, and it also reveals phase shift introduced by the filter. Such phase shifts can misalign signal components in time, which can be a critical issue in applications where the timing of signal events is important – for example detecting faults in safety critical systems.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import butter, lfilter\n\n# Function to create a Butterworth low-pass filter\ndef butter_lowpass(cutoff, fs, order=5):\n    nyq = 0.5 * fs\n    normal_cutoff = cutoff / nyq\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    return b, a\n\n# Function to apply the low-pass filter\ndef lowpass_filter(data, cutoff, fs, order=5):\n    b, a = butter_lowpass(cutoff, fs, order=order)\n    y = lfilter(b, a, data)\n    return y\n\n# Sample rate and desired cutoff frequency of the filter\nfs = 1000  # Sample rate, Hz\ncutoff = 10  # Desired cutoff frequency of the filter, Hz\n\n# Generate a composite signal (low + high frequency)\nT = 5.0     # seconds\nn = int(T * fs)  # total number of samples\nt = np.linspace(0, T, n, endpoint=False)\nlow_freq_signal = np.sin(2 * np.pi * 1.5 * t)\nhigh_freq_signal = np.sin(2 * np.pi * 30 * t)\ncomposite_signal = low_freq_signal + high_freq_signal\n\n# Add noise to the signal\nnoise = np.random.normal(0, 0.3, composite_signal.shape)\nnoisy_signal = composite_signal + noise\n\n# Filter the noisy signal\nfiltered_signal = lowpass_filter(noisy_signal, cutoff, fs)\n\n# Plotting\nplt.figure(figsize=(15, 7))\nplt.subplot(2, 2, 1)\nplt.plot(t, composite_signal)\nplt.title('Original Composite Signal')\nplt.grid()\n\nplt.subplot(2, 2, 2)\nplt.plot(t, noisy_signal)\nplt.title('Noisy Signal')\nplt.grid()\n\nplt.subplot(2, 2, 3)\nplt.plot(t, filtered_signal,label=\"Filtered signal\")\nplt.plot(t, low_freq_signal,'--', label=\"Original low frequency signal\")\nplt.title('Filtered Signal')\nplt.legend()\nplt.grid()\n\nplt.subplot(2, 2, 4)\nplt.plot(t, high_freq_signal)\nplt.title('High Frequency Component')\nplt.grid()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 10.5: Loss of high frequency features, and phase and magnitude distortions, in low-pass filtering.\n\n\n\n\n\n\nThe code generates four plots (see Figure 10.5): the original composite signal with both low and high-frequency components, the same signal with added noise, the filtered signal, and the high-frequency component of the original signal. After applying the low-pass filter, we notice that the high-frequency component is lost in the filtered signal, illustrating the disadvantage of losing important high-frequency details. Additionally, there are magnitude and phase distortions in the predicted (or reconstructed) low frequency signal. This example underscores the need for careful consideration when applying low-pass filters, as they can potentially alter the desired characteristics of the original signal.\n\n\n10.1.2.6 Wavelet Transformation\nWavelet transformation is a powerful tool in denoising data, particularly in the field of machine learning. It involves decomposing a signal into a set of wavelets, which are small waves that vary in frequency and duration. This technique is advantageous because it provides a multi-resolution analysis of the data, allowing for the observation and modification of the signal at various scales. The process typically involves transforming the noisy data into the wavelet domain, where noise and signal components can be distinctly identified and separated. Denoising is then achieved by applying a thresholding technique to remove the noise components while retaining the essential features of the original signal.\nWavelet Transform (WT) is a mathematical technique used for signal processing, which involves decomposing a signal into components at various scales using wavelets. Wavelets are functions that are localized in both time and frequency, unlike the sinusoids used in Fourier Transform, which are localized only in frequency. The mathematical rigor of wavelet transform can be understood through its two main types: Continuous Wavelet Transform (CWT) and Discrete Wavelet Transform (DWT).\nCWT of a continuous signal \\(x(t)\\) is defined by the integral: \\[ W_x(a, b) = \\frac{1}{\\sqrt{|a|}} \\int_{-\\infty}^{\\infty} x(t) \\psi^*\\left(\\frac{t-b}{a}\\right) dt,\\] where \\(W_x(a, b)\\) is the wavelet coefficient at scale \\(a\\) and position \\(b\\), \\(\\psi(t)\\) is the mother wavelet – a function localized in time, \\(\\psi^*(t)\\) is the complex conjugate of \\(\\psi(t)\\), \\(a\\) is the scale parameter, and \\(b\\) is the translation parameter.\nThe signal is convolved with a family of wavelets, which are scaled and translated versions of the mother wavelet. The scale parameter \\(a\\) compresses or stretches the wavelet, allowing analysis at different frequency bands, while the translation parameter \\(b\\) moves the wavelet along the time axis, enabling time localization.\nDWT provides a discrete and computationally efficient version of the wavelet transform. It is defined using dyadic scales and positions: \\[DWT_{jk} = \\sum_{n} x[n] \\psi_{jk}[n],\\] where \\(\\psi_{jk}[n] = 2^{-j/2} \\psi(2^{-j}n - k)\\) are the discrete wavelets, \\(j\\) and \\(k\\) are integers that control the scale and translation, respectively, and \\(x[n]\\) is the discrete signal.\nDWT is typically implemented using a filter bank consisting of a high-pass and a low-pass filter, corresponding to the decomposition of the signal into approximation and detail coefficients at each level. The process is iterative, where the approximation coefficients at each level are further decomposed in subsequent levels.\nAdvantages: One of the main advantages of wavelet transformation is its ability to handle non-stationary signals, where the statistical properties of the signal change over time. This makes it particularly useful in real-world scenarios where data often exhibits such characteristics. The wavelet transform is also efficient in localizing both time and frequency features, making it superior to traditional Fourier methods for many applications. This localization allows for more precise denoising, as it can differentiate between noise and signal characteristics at different scales. The following Python code demonstrates denoising data (or signals) with wavelet transformation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pywt\n\n# Create a sample signal with noise\nt = np.linspace(0, 1, 1000, endpoint=False)\noriginal_signal = np.sin(2 * np.pi * 5 * t)  # 5 Hz sine wave\nnoise = np.random.normal(0, 0.5, 1000)\nnoisy_signal = original_signal + noise\n\n# Wavelet denoising\ndef wavelet_denoise(data, wavelet, level):\n    coeff = pywt.wavedec(data, wavelet, mode=\"per\")\n    threshold = np.sqrt(2 * np.log(len(data))) * np.median(np.abs(coeff[-level]) / 0.6745)\n    coeff[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeff[1:])\n    reconstructed_signal = pywt.waverec(coeff, wavelet, mode=\"per\")\n    return reconstructed_signal\n\ndenoised_signal = wavelet_denoise(noisy_signal, 'db10', 1)\n\n# Plotting the results\nplt.figure(figsize=(12, 8))\nplt.subplot(311)\nplt.plot(t, original_signal)\nplt.title(\"Original Signal\")\nplt.subplot(312)\nplt.plot(t, noisy_signal)\nplt.title(\"Noisy Signal\")\nplt.subplot(313)\nplt.plot(t, denoised_signal,label=\"denoised\")\nplt.plot(t, original_signal,'--',label=\"original\")\nplt.title(\"Denoised Signal\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nFigure 10.6: Denoising data using wavelet transformations.\n\n\n\n\n\n\nIn the above Python code, we demonstrate the use of wavelet transformation for effective denoising of a signal.We frist generate a 5 Hz sine wave, with random Gaussian noise added to create a noisy signal. The key to the denoising process is the wavelet_denoise function, which leverages the Discrete Wavelet Transform (DWT). This function decomposes the noisy signal into wavelet coefficients, then applies a noise-reduction technique by thresholding these coefficients. The thresholding method employed is the universal threshold with soft thresholding, a common approach in denoising tasks. This method calculates the threshold value based on the median of the coefficients at the highest decomposition level, adjusted by a factor related to the length of the data. This calculation is critical in determining an optimal threshold for effectively removing noise.\nFor simplicity and demonstration purposes, the Daubechies wavelet (‘db10’) is used, though the choice of wavelet can vary depending on the specific characteristics of the signal being processed. Figure 10.6 shows the original signal, the noisy version, and the denoised signal. We observe that the wavelet transform is an effective techniques to denoise signals. However, it introduces distortions in the reconstructed signal, but are less prone to introducing phase distortions compared to some other signal processing techniques.\nLimitations: However, there are also disadvantages to using wavelet transformation. The selection of an appropriate wavelet function and the determination of the correct level of decomposition are not straightforward and often require expert knowledge and experimentation. This can make the technique less accessible to non-experts. Additionally, the effectiveness of wavelet denoising can be highly dependent on the characteristics of the noise and signal in the data. In cases where noise and signal characteristics overlap significantly in the wavelet domain, denoising becomes more challenging and can lead to the loss of important signal information.\n\n\n\n10.1.3 Identifying and Removing Outliers\nDetecting and removing outliers is an important step in preparing data for machine learning, as outliers can significantly skew the results of from models. There are several methods for detecting and dealing with outliers. We briefly explain some of the commonly used methods.\n\n10.1.3.1 The Z-score\nThe Z-score is a statistical measurement that describes a data point’s relation to the mean of a group of values, measured in terms of standard deviations from the mean. It’s used in outlier detection to identify data points that are unusually far from the mean.\nThe formula for calculating the Z-score of a data point is: \\[ Z = \\frac{(X - \\mu)}{\\sigma},\\] where \\(X\\) is the data point, \\(\\mu\\) is the mean of the data \\(\\sigma\\) is the standard deviation of the data.\nExample: Let’s say we have a dataset of test scores: \\[100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150.\\]\nIn this dataset, most scores are around \\(70-100\\), but there’s a score of \\(150\\), which seems unusually high. We suspect it might be an outlier. We next compute the Z-score from this data, by first computing the mean and the standard deviation. The Z-score is then computed for each score using the above formula. Outliers are often determined by considering data points with Z-scores beyond +/- 2.5 or 3 as outliers, as they are significantly far from the mean.\nA step-by-step process to compute Z-scores is presented next:\n\nFind the Mean: Mean \\(\\mu = \\frac{\\sum X}{n}\\)\nFind the Standard Deviation: Standard Deviation \\(\\sigma = \\sqrt{\\frac{\\sum (X - \\mu)^2}{n}}\\)\nCalculate Z-Scores for Each Data Point: \\(Z = \\frac{(X - \\mu)}{\\sigma}\\)\nIdentify Outliers: Any score with \\(|Z| &gt; 3\\) (or another chosen threshold) is considered an outlier.\n\nLet’s perform these calculations for our example dataset. Based on our calculations, here are the test scores with their corresponding Z-scores:\n\n\n\nTest Score\nZ-Score\n\n\n\n\n100\n0.638\n\n\n95\n0.437\n\n\n90\n0.237\n\n\n85\n0.036\n\n\n80\n-0.164\n\n\n75\n-0.365\n\n\n70\n-0.565\n\n\n65\n-0.766\n\n\n60\n-0.966\n\n\n55\n-1.167\n\n\n150\n2.643\n\n\n\nFrom this table, we can see that most scores have Z-scores within the range of -1 to 1, indicating that they are close to the mean. However, the test score of 150 has a Z-score of approximately 2.643, which is significantly higher than our outlier threshold of 2.5 or 3.\nTherefore, according to this method, the test score of 150 can be considered an outlier. To remove it, we would simply exclude this data point from our dataset before proceeding with further analysis or machine learning modeling. This helps in ensuring that our model is not unduly influenced by this extreme value.\nPython implementation is provided next:\n\nimport numpy as np\n\n# Dataset of test scores\ndata = np.array([100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150])\n\n# Calculate the mean and standard deviation\nmean = np.mean(data)\nstd_dev = np.std(data)\n\n# Calculate Z-scores\nz_scores = (data - mean) / std_dev\n\n# Combine data, Z-scores for easier interpretation\ndata_with_z_scores = np.column_stack((data, z_scores))\n\n# Display the data with their corresponding Z-scores\ndata_with_z_scores\n\narray([[ 1.00000000e+02,  6.37947290e-01],\n       [ 9.50000000e+01,  4.37449570e-01],\n       [ 9.00000000e+01,  2.36951850e-01],\n       [ 8.50000000e+01,  3.64541308e-02],\n       [ 8.00000000e+01, -1.64043589e-01],\n       [ 7.50000000e+01, -3.64541308e-01],\n       [ 7.00000000e+01, -5.65039028e-01],\n       [ 6.50000000e+01, -7.65536747e-01],\n       [ 6.00000000e+01, -9.66034467e-01],\n       [ 5.50000000e+01, -1.16653219e+00],\n       [ 1.50000000e+02,  2.64292449e+00]])\n\n\n\n\n10.1.3.2 Interquartile Range (IQR)\nThe Interquartile Range (IQR) is another statistical method used to detect and remove outliers. It is particularly useful because it is less affected by extremes than methods relying on the mean and standard deviation, like the Z-score. IQR is the range between the first quartile (Q1, the 25th percentile) and the third quartile (Q3, the 75th percentile) in a dataset. It represents the middle 50% of the data. The IQR is calculated as: \\[ \\text{IQR} = Q3 - Q1.\\]\nOutliers are typically defined as observations that fall below \\(Q1 - 1.5 \\times \\text{IQR}\\) or above \\(Q3 + 1.5 \\times \\text{IQR}\\). This rule of thumb is widely used but can be adjusted based on specific data characteristics or domain knowledge.\nA step-by-step process to detect and remove outliers using IQR:\n\nCalculate Q1 and Q3: These are the 25th and 75th percentiles of the data, respectively.\nCalculate IQR: Subtract Q1 from Q3.\nDetermine Outlier Thresholds:\n\nLower Bound = \\(Q1 - 1.5 \\times \\text{IQR}\\)\nUpper Bound = \\(Q3 + 1.5 \\times \\text{IQR}\\)\n\nIdentify Outliers: Data points that fall outside of these bounds are considered outliers.\nRemove or Adjust Outliers: Once identified, outliers can be removed, capped, or adjusted based on the analytical requirements.\n\nExample: Let us use the same dataset of test scores: \\[100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150.\\]\nWe will calculate the IQR and use it to determine the outliers in this dataset. Based on the IQR method, here are the results for our test score dataset:\n\nLower Bound for Outliers: 30.0\nUpper Bound for Outliers: 130.0\nIdentified Outliers: [150]\nData Without Outliers: [100, 95, 90, 85, 80, 75, 70, 65, 60, 55]\n\nIn this case, the score of 150 is identified as an outlier, as it is above the upper bound of 130.0. According to the IQR method, this data point can be considered for removal or adjustment.\nThe process of removing the outlier involves excluding the score of 150 from the dataset. This leaves us with the scores [100, 95, 90, 85, 80, 75, 70, 65, 60, 55], which are more representative of the typical range of scores in this dataset. By doing this, we reduce the potential impact of extreme values on further analysis or machine learning modeling.\nThe following Python code shows the computations involved.\n\nimport numpy as np\n\n# Re-defining the dataset of test scores\ndata = np.array([100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150])\n\n# Calculate Q1 and Q3\nQ1 = np.percentile(data, 25)\nQ3 = np.percentile(data, 75)\n\n# Calculate IQR\nIQR = Q3 - Q1\n\n# Determine outlier thresholds\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identifying outliers\noutliers = data[(data &lt; lower_bound) | (data &gt; upper_bound)]\n\n# Data without outliers\ndata_without_outliers = data[(data &gt;= lower_bound) & (data &lt;= upper_bound)]\n\nlower_bound, upper_bound, outliers, data_without_outliers\n\n(30.0,\n 130.0,\n array([150]),\n array([100,  95,  90,  85,  80,  75,  70,  65,  60,  55]))\n\n\n\n\n10.1.3.3 Isolation Forest Algorithm\nThe Isolation Forest algorithm is an effective method for detecting outliers, especially in large datasets. It’s particularly useful in unsupervised learning scenarios where you don’t have labeled data to train on.\nThe key concept behind Isolation Forest is that outliers are few and different, and hence, they are ‘easier’ to isolate compared to non-outlier points. In other words, it requires fewer random partitions to isolate an outlier than to isolate a regular data point.\nIt involves the following steps:\n\nRandom Sampling: The algorithm randomly samples a subset of the data. This makes the method efficient for large datasets.\nBuilding Isolation Trees (iTrees): For each sample, it randomly selects a feature and then randomly selects a split value between the maximum and minimum values of the selected feature. This process partitions the dataset, and it’s repeated recursively, leading to the formation of an ‘isolation tree’. Each node in the tree isolates data points from the rest of the sample.\nPath Lengths: The number of splits required to isolate a sample is recorded as the ‘path length’. Since outliers are ‘easier’ to isolate, they tend to have shorter path lengths.\nEnsemble of iTrees: Multiple isolation trees are created to form a random forest. The path lengths across these trees are then averaged to get a final score.\nScoring Anomalies: The average path length is used to calculate an anomaly score for each data point. A shorter path length results in a higher anomaly score, indicating a higher likelihood of the point being an outlier.\n\nHere is a Python code demonstrating detection of outliers using the algorithm.\n\nfrom sklearn.ensemble import IsolationForest\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generating a synthetic dataset\nnp.random.seed(42)\nX = 0.3 * np.random.randn(100, 2)\nX_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\nX = np.r_[X + 2, X - 2, X_outliers]\n\n# Fitting the model\nclf = IsolationForest(max_samples=100, random_state=42)\nclf.fit(X)\n\n# Predictions\ny_pred = clf.predict(X)\n\n# Visualization\nplt.figure(figsize=(10, 6))\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolor='k', s=50)\nplt.title(\"Isolation Forest Outlier Detection\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\n\n\nFigure 10.7: Detection of outliers using the Isolation Forest algorithm.\n\n\n\n\n\n\nThe above Python code demonstrates the use of the Isolation Forest algorithm for outlier detection in a synthetic dataset. We first create a synthetic dataset by generating random data points using a normal distribution (np.random.randn) and then scaling them by a factor of 0.3. To introduce some variability, the generated points are shifted to create two clusters (X + 2 and X - 2). Additionally, a set of outlier points is created using a uniform distribution over a wider range (np.random.uniform). These outlier points are then combined with the original data to form the complete dataset (X). An Isolation Forest model (IsolationForest) is instantiated with a specified max_samples parameter (100 in this case, which is the number of samples to draw from the dataset to train each base estimator in the forest). The model is then fitted to the dataset using the fit method. After fitting the model, it’s used to predict the nature of each data point in the dataset (y_pred). The prediction results in a label for each point, where 1 indicates a normal point (inlier), and -1 indicates an outlier.\nFigure 10.7 demonstrates how the Isolation Forest algorithm separates outliers from normal observations. The outliers, which are fewer and more scattered, are identified and marked distinctly from the denser clusters of normal data points. This demonstration provides a practical insight into the algorithm’s capability in handling unsupervised outlier detection in a dataset.\nAdvantages: The Isolation Forest algorithm offers several distinct advantages, making it a popular choice for outlier detection, especially in large and complex datasets. One of its primary strengths is efficiency: it can handle large datasets and high-dimensional data effectively due to its sampling strategy and the simplicity of isolation trees. Unlike many traditional statistical methods, the Isolation Forest does not assume a normal distribution of data, making it versatile and robust in handling various types of datasets. This characteristic is particularly beneficial in real-world scenarios where data often deviates from theoretical distributions.\nAnother advantage of the Isolation Forest is its resilience to the swamping and masking effects. Traditional outlier detection methods can sometimes misclassify normal observations as outliers (swamping) or fail to detect actual outliers (masking). The Isolation Forest algorithm, with its unique approach of isolating anomalies, tends to be less prone to these issues, leading to more accurate detection.\nFurthermore, the algorithm’s random forest approach contributes to its effectiveness. By creating multiple isolation trees and averaging their results, the algorithm can achieve a more reliable and stable outlier detection, mitigating the randomness that might arise from any single tree. This ensemble technique enhances the overall accuracy of the model. Lastly, the Isolation Forest is relatively straightforward to implement and does not require extensive parameter tuning, which adds to its practicality in various applications, from fraud detection to anomaly detection in network traffic or sensor data monitoring.\nLimitations: The Isolation Forest algorithm, though effective for outlier detection, has several disadvantages. Firstly, its reliance on random selection for creating isolation trees introduces a degree of unpredictability, potentially leading to inconsistent results across different runs. This randomness can affect the stability and reproducibility of the model, which is a significant concern in applications requiring consistent performance. Secondly, the algorithm’s performance is highly sensitive to its parameters, such as the number of trees in the forest and the sample size. Selecting these parameters appropriately requires a deep understanding of the dataset and can involve a trial-and-error approach, which may not always be feasible.\nAdditionally, while Isolation Forest is generally efficient, its performance can degrade with extremely large datasets, particularly those with a high number of features. This can lead to increased computational demands, making the algorithm less suitable for very large-scale applications. Another limitation is the interpretability of the results; understanding why a specific data point is identified as an outlier is not straightforward with Isolation Forest, which can be a drawback in scenarios where explanation and transparency are crucial.\nMoreover, the algorithm’s effectiveness is predominantly in numerical data contexts and may not perform as well with categorical data or mixed data types without adequate preprocessing. Also, it operates purely on statistical properties, lacking contextual sensitivity, which means it might not align with domain-specific definitions of outliers. In summary, while the Isolation Forest algorithm is a valuable tool for detecting outliers, its application requires careful consideration of its limitations regarding randomness, parameter sensitivity, scalability, interpretability, and data type compatibility.\nNote on Removing Outliers: It’s important to note that Isolation Forest is primarily a method for detecting outliers, not necessarily removing them. The decision to remove outliers identified by the algorithm should be made based on the context and the impact of these outliers on the subsequent analysis or machine learning models.\n\n\n10.1.3.4 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nDBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a widely used clustering algorithm in the realms of machine learning and data analysis. The cornerstone of DBSCAN is the concept of clustering, which involves grouping data points in such a way that points in the same cluster are more similar to each other than to those in other clusters. This method is particularly effective for discovering patterns and groupings within data.\nA key feature distinguishing DBSCAN from other clustering methods like K-Means is its focus on density-based clustering. Rather than grouping points based on distance from a central point, as in K-Means, DBSCAN forms clusters based on the density of data points. This approach enables the identification of clusters with arbitrary shapes and even the detection of nested clusters.\nAt the heart of DBSCAN are three core concepts:\n\nCore Points: These are points that have a minimum number of other points (specified as MinPts) within a certain radius (\\(\\epsilon\\), or epsilon).\nBorder Points: These are points that fall within the radius of a core point but have fewer than MinPts within their own radius.\nNoise Points: Points that do not qualify as either core or border points are categorized as noise or outliers.\n\nThe DBSCAN algorithm operates by starting with an arbitrary point and identifying all points that are density-reachable from this point, based on the \\(\\epsilon\\) and MinPts parameters. If the starting point is a core point, a cluster is formed. If it’s a border point, with no density-reachable points, the algorithm proceeds to the next point. This process continues until each point in the dataset is classified as either a core point, border point, or noise. This method’s ability to handle noise and identify clusters of varying shapes and densities makes it a versatile and powerful tool in data analysis.\nAdvantages: DBSCAN offers several significant advantages as a clustering algorithm. First and foremost, its ability to identify clusters of arbitrary shapes is a major strength, especially when dealing with complex spatial data. This contrasts with algorithms like K-Means, which tend to identify only spherical clusters. Secondly, DBSCAN is particularly adept at handling noise and outliers in the data set. It can effectively differentiate between core points (dense areas of the dataset), border points (edges of clusters), and noise, providing a more accurate clustering result in noisy environments. Another advantage of DBSCAN is that it does not require the number of clusters to be defined a priori, unlike K-Means. This makes it particularly useful in exploratory data analysis where the number of clusters is not known beforehand. Additionally, DBSCAN’s minimal reliance on domain knowledge for setting its two main parameters (radius and minimum number of points) simplifies its application across various domains. Lastly, DBSCAN’s capability to discover clusters within clusters allows for a deeper and more nuanced understanding of the data structure, making it a versatile tool for a wide range of data analysis tasks. These advantages make DBSCAN a popular choice for many real-world applications, including image processing, genetics, and spatial data analysis.\nLimitations: DBSCAN, while versatile, has certain limitations that can affect its performance and applicability. One of the primary challenges lies in determining the appropriate values for its two main parameters: epsilon (\\(\\epsilon\\)), which defines the radius around a point to search for neighboring points, and MinPts, the minimum number of points required to form a dense region. The choice of these parameters can significantly influence the clustering outcome, and finding the right balance often requires domain knowledge or trial-and-error, which can be impractical in many situations. Another limitation is DBSCAN’s sensitivity to varying densities within the same dataset. It may struggle to identify clusters correctly if there are significant differences in the density of clusters, leading to either fragmented clusters or over-merging of clusters. Additionally, DBSCAN’s performance can degrade with high-dimensional data due to the curse of dimensionality, as the notion of density becomes less meaningful in high-dimensional spaces. Finally, the computational complexity of DBSCAN, particularly with large datasets, can be a drawback. It requires calculating the distance between points, which can be computationally intensive for large datasets, although optimized implementations can mitigate this to some extent. Despite these limitations, DBSCAN remains a popular and effective clustering method for datasets where its unique advantages can be fully leveraged.\nBelow is an example of how to use the DBSCAN algorithm for clustering in Python using the scikit-learn library.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import datasets\n\n# Step 1: Generate sample data\nX, _ = datasets.make_moons(n_samples=300, noise=0.05, random_state=0)\n\n# Step 2: Apply DBSCAN\n# epsilon (eps) is the maximum distance between two samples for them to be considered in the same neighborhood\n# min_samples is the number of samples in a neighborhood for a point to be considered a core point\ndbscan = DBSCAN(eps=0.3, min_samples=10)\nclusters = dbscan.fit_predict(X)\n\n# Step 3: Visualize the results\nplt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='Paired', marker='o', s=30, edgecolor='k')\nplt.title('DBSCAN Clustering')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\nFigure 10.8: Clustering of complicated data using DBSCAN.\n\n\n\n\n\n\nIn this script, we first generate a two-dimensional dataset using make_moons from sklearn.datasets. We then apply the DBSCAN algorithm to this data, where eps and min_samples are key parameters. Figure 10.8 shows the results where each cluster is colored differently, and outliers (points not belonging to any cluster) are usually shown in a different color.\nThe next code shows outlier detection using DBSCAN.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\n# Generate sample data\ncenters = [[1, 1], [-1, -1], [1, -1]]\nX, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.4, random_state=0)\n\n# Add some noise/outlier points\nnp.random.seed(42)\noutliers = np.random.uniform(low=-3, high=3, size=(20, 2))\nX = np.concatenate([X, outliers])\n\n# Apply DBSCAN\ndbscan = DBSCAN(eps=0.3, min_samples=10)\nlabels = dbscan.fit_predict(X)\n\n# Identify core and noise points\ncore_samples_mask = np.zeros_like(labels, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\nunique_labels = set(labels)\n\n# Plot the results\ncolors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black is used for noise/outliers\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n\n    # Plot core points\n    xy = X[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)\n\n    # Plot outliers\n    xy = X[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n\nplt.title('DBSCAN: Outlier Detection')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.show()\n\n\n\n\n\n\nFigure 10.9: Outlier detection using DBSCAN.\n\n\n\n\n\n\nIn the above Python code, the process of outlier detection using the DBSCAN algorithm is illustrated through a series of steps. Initially, a sample dataset is created using the make_blobs function, which generates clusters of data points. To simulate a real-world scenario where data might contain anomalies, additional random points are introduced into this dataset, serving as potential outliers. Following the creation of this dataset, the DBSCAN algorithm is applied. DBSCAN operates by identifying clusters based on the density of data points, and it labels points that do not belong to any cluster as -1, effectively categorizing them as outliers. After the application of DBSCAN, the results are shown in Figure 10.9. This plot distinctly shows the core points (the dense central parts of clusters), border points (points on the edges of clusters), and outliers. The outliers, which are points identified as not belonging to any cluster, are typically highlighted in black. This visual representation aids in understanding how DBSCAN segregates outliers from the main clusters in the dataset, showcasing its utility in outlier detection.\n\n\n10.1.3.5 Outliers and Rare Events\nOutlier detection, a critical task in data analysis, faces several challenges, particularly in identifying and correctly interpreting rare events. An outlier is typically a data point that significantly deviates from the majority of data, suggesting a different underlying mechanism. Rare events are similar but are specifically those events that occur infrequently in the dataset. For instance, fraud in financial transactions or the failure of a critical component in a manufacturing process are rare events – they don’t happen often, but when they do, their impact can be substantial.\nChallenges in Outlier Detection: Outlier detection in data analysis encounters several notable challenges, particularly in specific contexts. One such challenge is high dimensionality. In spaces with many dimensions, the task of distinguishing between mere noise and genuine outliers becomes increasingly complex. This complexity arises from the “curse of dimensionality,” a phenomenon where the conventional measure of distance between data points loses its effectiveness as the number of dimensions increases.\nAnother challenge is the presence of varying densities within datasets. In such cases, a data point that is an outlier in a less dense cluster might be considered normal in a denser cluster. This variability complicates the establishment of a universal threshold for identifying outliers, as what qualifies as an outlier can differ significantly from one region of the dataset to another.\nAdditionally, the phenomena of masking and swamping add to the complexity. Masking occurs when the presence of certain outliers obscures or hides other outliers, while swamping refers to the misidentification of normal points as outliers because they are in close proximity to actual outliers.\nLastly, the challenge of evolving data is significant, especially in dynamic datasets where patterns and trends change over time, a concept known as concept drift. In such environments, the definition of what constitutes an outlier is not static but changes as the underlying data patterns evolve. This necessitates a continuous adaptation of the outlier detection models to remain effective in identifying true anomalies over time. These challenges underscore the need for sophisticated and adaptable methods in outlier detection to accurately discern genuine anomalies in complex datasets.\nMissing Rare Events: The challenges in detecting rare events in data analysis and machine learning are multifaceted, primarily due to their inherent characteristics. One of the most prominent challenges is their low frequency of occurrence. By nature, rare events occur infrequently within datasets, leading to a tendency for them to be overlooked or misclassified as mere anomalies. This infrequency poses a significant challenge in accurately identifying and assessing the impact of these events.\nAnother critical issue is the lack of representation in training data for machine learning models. In many scenarios, the training datasets do not adequately capture these rare events, leading to models that are not well-equipped to recognize them when they do occur. The scarcity of examples of rare events in the training data results in a learning process that is heavily biased towards more frequent occurrences, thereby diminishing the model’s ability to identify rare events accurately.\nFurthermore, the challenge of balancing sensitivity and specificity in the context of rare events is a delicate task. Sensitivity refers to the model’s ability to correctly detect most of the rare events, whereas specificity relates to its ability not to mislabel normal events as rare. Striking the right balance is crucial; overly sensitive models might flag too many normal occurrences as rare events, causing false alarms, while models with excessive specificity might miss genuine rare events. This balancing act is particularly crucial in fields where the implications of missing a rare event or falsely identifying one are significant, such as in medical diagnostics, financial fraud detection, or security systems.\nThese challenges highlight the need for specialized approaches in data analysis and model training that can accommodate the unique nature of rare events, ensuring that these critical but infrequent occurrences are neither missed nor wrongly classified.\nExamples of Rare Events: Rare events in various engineering fields, particularly in aerospace, mechanical, and other disciplines, often involve scenarios where the low probability of occurrence belies the potential for severe consequences. Here are some examples, along with explanations of their nature and the catastrophes that ensued from missed rare events:\n\nCivil Engineering - Tacoma Narrows Bridge Collapse (1940):\n\nNature of Rare Event: The bridge collapsed due to aeroelastic flutter caused by wind.\nCatastrophe: The bridge, nicknamed “Galloping Gertie,” was vulnerable to this rare phenomenon, which was not well-understood at the time. The collapse of the bridge was a significant event in engineering, leading to changes in the design and construction of suspension bridges to prevent similar incidents.\n\nChemical Engineering - Bhopal Gas Tragedy (1984):\n\nNature of Rare Event: A leak of methyl isocyanate gas and other chemicals from a pesticide plant.\nCatastrophe: Widely considered the world’s worst industrial disaster, it resulted in thousands of deaths and long-term health effects for many more. This tragedy underscored the catastrophic potential of rare chemical leaks and the need for stringent safety measures.\n\nElectrical Engineering - Northeast Blackout (2003):\n\nNature of Rare Event: A large-scale power outage occurred due to a software bug in an alarm system, coupled with operational errors.\nCatastrophe: This event led to a widespread electricity blackout across parts of the Northeastern and Midwestern United States and Ontario, Canada, affecting around 55 million people. It exposed vulnerabilities in the electrical grid and the cascading effect of small failures.\n\nMechanical Engineering - Fukushima Daiichi Nuclear Disaster (2011):\n\nNature of Rare Event: This disaster was triggered by a massive tsunami following a strong earthquake.\nCatastrophe: The tsunami led to the failure of nuclear reactors’ cooling systems, causing meltdowns, hydrogen-air explosions, and the release of radioactive material. The rarity of such a large-scale natural disaster, coupled with the unpreparedness for such an event, resulted in one of the worst nuclear disasters in history.\n\n\nThese examples from different engineering disciplines demonstrate how rare events, often overlooked due to their low probability, can lead to significant disasters. They underscore the importance of considering even unlikely scenarios in engineering design and risk management to prevent catastrophic outcomes.\nAerospace engineering, with its complex systems and high stakes, has witnessed several rare events that had significant consequences. These events, often considered low-probability, underscore the critical importance of meticulous design, rigorous testing, and comprehensive risk management in the field. Here are some examples from aerospace engineering:\n\nApollo 13 Oxygen Tank Explosion (1970):\n\nNature of Rare Event: An oxygen tank in the service module of Apollo 13 failed due to an electrical fault, causing an explosion.\nCatastrophe: This incident crippled the spacecraft during its mission to the Moon, leading to a critical situation for the astronauts aboard. The mission was aborted, and a safe return to Earth became the primary objective. The event highlighted the need for redundant systems and rigorous pre-flight testing in space missions.\n\nSpace Shuttle Challenger Disaster (1986):\n\nNature of Rare Event: The disaster was primarily caused by the failure of an O-ring seal in its right solid rocket booster.\nCatastrophe: The O-ring failure, exacerbated by unusually cold weather, led to the breakup of the space shuttle shortly after launch, resulting in the death of all seven crew members. This event highlighted the catastrophic consequences of underestimating the likelihood and impact of what was considered a rare component failure.\n\nSoyuz 11 Crew Loss (1971):\n\nNature of Rare Event: The Soyuz 11 spacecraft depressurized during its return to Earth.\nCatastrophe: The three crew members aboard died due to asphyxiation, marking the only human deaths in space (above the Kármán line). The tragedy led to redesigned safety features and protocols in Soviet and later Russian crewed space missions.\n\nMars Climate Orbiter Loss (1999):\n\nNature of Rare Event: The spacecraft was lost due to a navigation error caused by a failure to convert units from English to metric.\nCatastrophe: The orbiter approached Mars at a lower altitude than planned and disintegrated due to atmospheric stresses. This incident underscored the critical importance of thorough cross-checks and consistent units in engineering calculations, especially in international collaborations.\n\nSpace Shuttle Columbia Disaster (2003):\n\nNature of Rare Event: Damage to the thermal protection system of the Space Shuttle Columbia during launch led to its disintegration upon re-entry.\nCatastrophe: All seven crew members were lost. The investigation following the disaster led to a 29-month suspension of the Space Shuttle program and a reevaluation of safety protocols and shuttle integrity inspections.\n\nAir France Flight 447 Crash (2009):\n\nNature of Rare Event: The crash was primarily caused by the aircraft’s speed sensors being obstructed by ice crystals, leading to a series of erroneous readings and pilot errors.\nCatastrophe: The aircraft crashed into the Atlantic Ocean, resulting in the deaths of all passengers and crew. This event brought attention to the importance of pilot training in handling high-altitude stalls and the need for more reliable airspeed sensors.\n\n\nThese incidents in aerospace engineering, though rare, had profound impacts on safety protocols, design considerations, and operational procedures. Each event prompted a reevaluation of existing practices, leading to advancements in technology and an increased emphasis on safety in aerospace engineering.\nIn the realm of autonomy, particularly in autonomous vehicles and systems, recent times have seen several rare events. These events are significant as they highlight the challenges and potential risks associated with the integration of autonomous technology in various sectors. Some of these rare events include:\n\nAutonomous Vehicle Accidents:\n\nEvent Description: There have been instances where autonomous or semi-autonomous vehicles were involved in accidents, some of which were fatal.\nSignificance: These incidents have raised questions about the readiness of autonomous driving technology for widespread use, the reliability of sensors and algorithms in complex, real-world environments, and the need for regulatory frameworks.\n\nDrone Mishaps in Controlled Airspace:\n\nEvent Description: Drones entering controlled or restricted airspace have caused disruptions, including near-miss incidents with manned aircraft and temporary shutdowns of major airports.\nSignificance: These events underscore the challenges in integrating unmanned aerial systems into existing airspace, particularly regarding safety and coordination with manned aircraft operations.\n\nRobotic System Failures in Healthcare:\n\nEvent Description: There have been rare but notable instances where autonomous or robotic systems in healthcare settings malfunctioned, leading to incorrect diagnoses or complications in surgeries.\nSignificance: These cases highlight the importance of stringent testing, validation, and oversight in the deployment of autonomous systems in sensitive and high-stakes fields like healthcare.\n\nSecurity Breaches in Autonomous Systems:\n\nEvent Description: Cybersecurity incidents involving autonomous systems, including hacking or manipulation of autonomous vehicles or industrial automation systems.\nSignificance: These breaches point to the critical need for robust cybersecurity measures in autonomous systems, given their potential impact on safety and privacy.\n\nAI Ethics and Bias Incidents:\n\nEvent Description: Instances where AI systems, including autonomous decision-making algorithms, have demonstrated biases or ethical issues, such as in hiring practices or law enforcement.\nSignificance: These events raise concerns about the ethical implications of autonomous systems, the need for unbiased data, and the importance of incorporating ethical considerations into AI development.\n\nFailures in Autonomous Trading Algorithms:\n\nEvent Description: Rare cases where autonomous trading algorithms caused significant market disruptions or losses due to unexpected behaviors under unusual market conditions.\nSignificance: Such events draw attention to the risks associated with high-frequency, autonomous trading in financial markets and the need for comprehensive risk management strategies.\n\n\nThese rare events in autonomy highlight the complexities and potential risks associated with advancing autonomous technologies. They emphasize the need for ongoing research, development, and regulation to ensure safety, reliability, and ethical considerations are adequately addressed as these technologies become more integrated into daily life.\nIn summary, the primary challenges in outlier detection and the identification of rare events lie in the definition, variability of data, and limitations inherent to data analysis methods. Understanding these challenges is crucial for designing effective detection systems, especially in fields where rare events, while infrequent, can have significant consequences."
  },
  {
    "objectID": "data_pre_processing.html#survivorship-bias",
    "href": "data_pre_processing.html#survivorship-bias",
    "title": "10  Data Pre-Processing",
    "section": "10.2 Survivorship Bias",
    "text": "10.2 Survivorship Bias\nSurvivorship bias is a logical fallacy that occurs when a person focuses on the people or things that “survived” some process and inadvertently overlooks those that did not because of their lack of visibility. This bias can lead to false conclusions because the failures, which are not seen, are not considered. Here are some real-world examples:\n\nWorld War II Aircraft Analysis: During World War II, military analysts examined returning aircraft for bullet holes to determine which areas needed additional armor. However, the analysis initially suffered from survivorship bias. They were only looking at planes that made it back and not considering the ones that were shot down. The areas where the returning planes were hit were actually the strongest parts, as evidenced by their ability to return. The planes that didn’t return were likely hit in different places, which were the areas that actually needed reinforcing.\nStock Market Investment Strategies: In the financial world, survivorship bias can be seen when analyzing investment funds. If one only considers funds that are currently successful or still in existence, they might conclude that investing in funds is generally a profitable venture. This ignores all the funds that failed and closed, which could show a more risky and volatile market.\nSuccess Stories in Entrepreneurship: The media often highlights successful entrepreneurs, leading to a perception that entrepreneurial endeavors are more likely to succeed than they actually are. This ignores the vast majority of startups that fail, and it can lead aspiring entrepreneurs to underestimate the risks involved.\nSelf-Help and Motivational Literature: Books and articles often focus on successful individuals and their routines or habits, implying that emulating these will lead to similar success. This ignores the many people who may have had the same habits but did not achieve the same level of success, often due to factors outside their control.\nEducational Institution Alumni Success: Universities and colleges often showcase their most successful alumni as representations of the potential outcomes of their education. However, this can create a skewed perception, as it doesn’t account for the majority of graduates who may not achieve the same level of fame or success.\nSuccess Stories of College Dropouts: The media and popular culture often highlight the success stories of famous dropouts like Bill Gates, Steve Jobs, and Mark Zuckerberg, who achieved extraordinary success despite not completing their formal education. While these narratives are inspiring, they represent a very small fraction of dropouts and contribute to survivorship bias. The reality is that most people who leave school early do not achieve such high levels of success; instead, they are more likely to face economic and social challenges, including lower incomes and higher unemployment rates. This skewed portrayal can lead to a misconception that formal education is not essential for success, potentially influencing young people to undervalue education and overlook the risks associated with dropping out. It’s important to balance these exceptional stories with the more common and less visible outcomes of leaving school early.\n\nSurvivorship bias leads to a distorted view of reality by emphasizing the winners and ignoring the losers. It’s important in decision-making and analysis to consider the full picture, including those who didn’t “survive” the process being examined.\nSurvivorship bias can significantly impact data-driven machine learning in several ways, primarily by skewing the training data and leading to models that are not representative of the real world. Here’s how this bias can affect machine learning:\n\nIncomplete Training Data: Survivorship bias in machine learning occurs when the training data includes only successful cases or ‘survivors’, while failing to account for failures or ‘non-survivors’. For instance, if a model is trained only on successful retail companies, it might not learn the patterns that lead to the failure of retail businesses.\nMisleading Model Performance: Models trained on data affected by survivorship bias might show deceptively high performance during training and testing but fail in real-world applications. This happens because the model has not learned from the complete range of scenarios, especially those that led to failures or negative outcomes.\nBiased Predictions and Decisions: The bias can lead to models that are biased towards the characteristics of the ‘survivors’. For example, in credit scoring, if the model is trained only on data from borrowers who haven’t defaulted, it might underestimate the risk of default.\nOverlooking Crucial Factors: Survivorship bias can cause important variables and factors leading to failure to be overlooked. In healthcare, for instance, if a model is trained only on patients who have survived a certain disease, it might miss critical symptoms or factors that are common among those who did not survive.\nDifficulty in Generalization: The models become less generalizable to wider, more diverse scenarios. They become overfitted to the successful cases, which could be a small, non-representative sample of the overall population.\nEthical and Fairness Concerns: This bias can also raise ethical concerns, particularly in applications where it’s important to represent and serve a diverse range of individuals fairly, such as in hiring or loan approval processes.\n\nTo mitigate the impact of survivorship bias in machine learning, it’s crucial to use comprehensive and representative datasets that include both successes and failures. Additionally, continuous monitoring and validation of the model’s performance in real-world scenarios are essential to ensure that it remains accurate and fair over time."
  },
  {
    "objectID": "data_pre_processing.html#data-transformation",
    "href": "data_pre_processing.html#data-transformation",
    "title": "10  Data Pre-Processing",
    "section": "10.3 Data Transformation",
    "text": "10.3 Data Transformation\n- **Normalization and Standardization**: Rescaling the data to a specific range (like 0 to 1 for normalization) or changing the distribution to have a mean of 0 and a standard deviation of 1 (standardization).\n- **Scaling to Unit Length**: Rescaling data so that the length of each data point (considered as a vector) is 1.\n- **Power Transforms**: Applying transformations like logarithmic, square root, or exponential to stabilize variance and make the data more 'normal' (Gaussian)."
  },
  {
    "objectID": "data_pre_processing.html#data-reduction",
    "href": "data_pre_processing.html#data-reduction",
    "title": "10  Data Pre-Processing",
    "section": "10.4 Data Reduction",
    "text": "10.4 Data Reduction\n- **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or t-SNE are used to reduce the number of features while retaining most of the information.\n- **Feature Selection**: Selecting the most significant features based on various statistical criteria. Methods include filter, wrapper, and embedded methods.\n- **Binning**: Grouping a set of numerical values into a smaller number of bins (or intervals)."
  },
  {
    "objectID": "data_pre_processing.html#data-encoding",
    "href": "data_pre_processing.html#data-encoding",
    "title": "10  Data Pre-Processing",
    "section": "10.5 Data Encoding",
    "text": "10.5 Data Encoding\n- **Encoding Categorical Data**: Converting categorical data into numerical format using methods like one-hot encoding, label encoding, or binary encoding.\n- **Text Encoding**: Transforming text data into numerical format using techniques like Bag of Words, TF-IDF, or word embeddings (like Word2Vec)."
  },
  {
    "objectID": "data_pre_processing.html#handling-imbalanced-data",
    "href": "data_pre_processing.html#handling-imbalanced-data",
    "title": "10  Data Pre-Processing",
    "section": "10.6 Handling Imbalanced Data",
    "text": "10.6 Handling Imbalanced Data\n- Applying techniques like oversampling the minority class, undersampling the majority class, or using synthetic data generation methods like SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset."
  },
  {
    "objectID": "data_pre_processing.html#feature-engineering",
    "href": "data_pre_processing.html#feature-engineering",
    "title": "10  Data Pre-Processing",
    "section": "10.7 Feature Engineering",
    "text": "10.7 Feature Engineering\n- Creating new features or modifying existing ones to improve model performance. This can involve extracting information from dates, creating interaction features, or aggregating data to create summary statistics."
  },
  {
    "objectID": "data_pre_processing.html#time-series-specific-techniques",
    "href": "data_pre_processing.html#time-series-specific-techniques",
    "title": "10  Data Pre-Processing",
    "section": "10.8 Time Series Specific Techniques",
    "text": "10.8 Time Series Specific Techniques\n- If dealing with time series data, techniques like windowing, lag features, and handling seasonality might be necessary."
  },
  {
    "objectID": "data_pre_processing.html#data-integration",
    "href": "data_pre_processing.html#data-integration",
    "title": "10  Data Pre-Processing",
    "section": "10.9 Data Integration",
    "text": "10.9 Data Integration\n- Combining data from multiple sources, which might involve format alignment, resolving data conflicts, and schema integration.\nEach of these techniques must be chosen and applied based on the specific requirements of the dataset and the problem at hand. The goal is always to produce a clean, reliable dataset that can be effectively used for machine learning modeling."
  },
  {
    "objectID": "supervised_learning.html#k-nearest-neighbors",
    "href": "supervised_learning.html#k-nearest-neighbors",
    "title": "11  Supervised Learning",
    "section": "11.1 \\(k\\)-Nearest Neighbors",
    "text": "11.1 \\(k\\)-Nearest Neighbors\nThe k-nearest neighbors (KNN) algorithm is a simple, yet powerful machine learning method used for both classification and regression tasks. It’s part of a family of algorithms known as instance-based or lazy learning algorithms, where generalization of the training data is delayed until a query is made to the system. Here’s how it works:\n\nBasic Concept: KNN operates on a simple principle: it predicts the label of a new point based on the labels of the ‘k’ nearest points in the training set. In other words, it looks at the ‘k’ closest data points from the dataset to make a prediction.\nChoosing ‘k’: The ‘k’ in KNN represents the number of nearest neighbors to consider. It’s a crucial parameter that influences the accuracy of predictions. A small value of ‘k’ means that noise will have a higher influence on the result, whereas a large ‘k’ makes the algorithm slower and may lead to underfitting.\nDistance Metric: To determine which points are closest, KNN uses a distance metric - typically Euclidean distance, although other metrics like Manhattan or Hamming distance can also be used depending on the type of data.\nClassification vs. Regression: In classification tasks, KNN assigns the most common class among the k-nearest neighbors as the class for the new point. In regression tasks, it assigns the average of the values of the k-nearest neighbors.\nLazy Learning: KNN is considered a lazy learner because it doesn’t learn a discriminative function from the training data but “memorizes” the dataset instead. Therefore, there is no explicit training phase or it is very minimal.\nAdvantages: The algorithm is straightforward and easy to implement, works well with a small number of input variables (features), and is effective if the training data is large.\nDisadvantages: KNN gets significantly slower as the volume of data increases, making it impractical for large datasets. It also suffers from the curse of dimensionality. High-dimensional data can make distance metrics less effective, leading to poor performance of the algorithm. Furthermore, KNN can be sensitive to the scale of the data and irrelevant features, so feature scaling and selection can be crucial steps in using it effectively.\nApplications: KNN is used in a variety of applications such as finance (for credit scoring and market research), healthcare (for classifying patient health risk), and recommendation systems (like suggesting similar products or services).\n\nIn essence, KNN is a versatile algorithm suitable for tackling problems with a smaller dataset and fewer dimensions, where the intuition of “likeness” based on proximity in a feature space is a good indicator of similarity or relatedness.\n\n\n\n\n\n\nFigure 11.1: Five nearest neighbors of a given point in a scattered data set.\n\n\n\n\n\nFigure 11.1 illustrates 5 nearest neighbors of a point in a data set.\n\n11.1.1 Confusion Matrix\nA confusion matrix is a tool often used in machine learning to visualize the performance of a classification algorithm. It is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows us to see how well our classification model is performing, and to understand the types of errors it is making.\nHere’s a breakdown of the terms:\n\nTrue Positives (TP): These are cases in which the model correctly predicts the positive class.\nTrue Negatives (TN): These are cases in which the model correctly predicts the negative class.\nFalse Positives (FP): These are cases in which the model incorrectly predicts the positive class (also known as a “Type I error”).\nFalse Negatives (FN): These are cases in which the model incorrectly predicts the negative class (also known as a “Type II error”).\n\nThe confusion matrix looks like this:\n\n\n\n\nPredicted Positive\nPredicted Negative\n\n\n\n\nActual Positive\nTrue Positive (TP)\nFalse Negative (FN)\n\n\nActual Negative\nFalse Positive (FP)\nTrue Negative (TN)\n\n\n\nFrom the confusion matrix, several performance metrics can be calculated, such as accuracy, precision, recall, and F1 score. These metrics provide deeper insights into the performance of the model, especially in cases where the class distribution is imbalanced.\n\nAccuracy is the proportion of true results (both true positives and true negatives) among the total number of cases examined.\nPrecision is the proportion of true positives among all positive predictions (TP / (TP + FP)).\nRecall (or sensitivity) is the proportion of true positives identified correctly (TP / (TP + FN)).\nF1 Score is the harmonic mean of precision and recall, providing a balance between them.\n\nA confusion matrix is a simple yet powerful tool for understanding the performance of a classifier, especially in cases where mere accuracy is not sufficient to evaluate the model.\n\n\n11.1.2 Example: Classification\nWe next show how KNNs can be used to classify stable and unstable modes of a linear dynamical system based on its eigen-values. This is a contrived example to demonstrate how proximity can be used to solve a classification problem.\nWe understand that for a dynamical system defined by the equation \\(\\dot{\\boldsymbol{x}} = \\boldsymbol{A}\\boldsymbol{x}\\), where \\(\\boldsymbol{A}\\) is the matrix governing the system, stability is determined by the eigenvalues of \\(\\boldsymbol{A}\\). Specifically, the system is stable if the real part of an eigenvalue is negative, and unstable if this real part is positive. In this example, eigenvalues with a zero real part are considered unstable. This categorization is visually represented in Figure 11.2, where stable eigenvalues are marked in green and unstable ones in red. The aim is to develop a KNN classifier trained on this dataset, enabling it to accurately classify any given eigenvalue as either stable or unstable.\n\n\n\n\n\nFigure 11.2: Distribution of stable and unstable eigen values.\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nnSamp = 500\nData = np.random.uniform(-1,1,size=(nSamp,2))\nlabel = np.ones(nSamp)\nlabel[Data[:,0]&lt;0] = -1   # These are stable.\n# Split the dataset into a training set and a test set\nData_train, Data_test, label_train, label_test = train_test_split(Data, label, test_size=0.2)\n\n# Initialize the KNN classifier with k=5\nknn = KNeighborsClassifier(n_neighbors=5)\n\n# Train the model\nknn.fit(Data_train, label_train)\n\n# Make predictions\nlabel_pred = knn.predict(Data_test)\n\n# Evaluate the model\nprint(confusion_matrix(label_test, label_pred))\nprint(classification_report(label_test, label_pred))\n\n[[54  2]\n [ 1 43]]\n              precision    recall  f1-score   support\n\n        -1.0       0.98      0.96      0.97        56\n         1.0       0.96      0.98      0.97        44\n\n    accuracy                           0.97       100\n   macro avg       0.97      0.97      0.97       100\nweighted avg       0.97      0.97      0.97       100\n\n\n\nWe can also check with any other random eigen-value as shown next.\n\n# Create outcome labels for printing.\nOutcome = {-1:\"a stable mode\", 1:\"an unstable mode\"}\n\n# Test\ncandidate_lambda = np.random.uniform(-1,1,size=(1,2))\nprediction = knn.predict(candidate_lambda)\nprint(f\"Prediction: Eigen value {candidate_lambda[0]} corresponds to {Outcome[prediction[0]]}.\")\n\nPrediction: Eigen value [0.47279472 0.99341029] corresponds to an unstable mode.\n\n\nIt is unlikely that we will apply this method for verifying the stability of a linear system, as it is much simpler to check the sign of the real part of eigenvalues. The purpose of this example is merely to demonstrate that using proximity can often be an effective way to classify data. Indeed, KNN and other machine learning techniques are more likely to be utilized in scenarios where a direct mathematical relationship between input and output isn’t readily apparent.\nLimitations One of the primary drawbacks of KNN based classification is the algorithm’s sensitivity to the scale of the data and irrelevant features. Since KNN uses distance metrics to identify the nearest neighbors, features not scaled uniformly can lead to biased distance calculations, disproportionately influencing the classification. Furthermore, the presence of irrelevant or redundant features can significantly degrade the model’s performance, as KNN does not inherently discern useful features from less useful ones.\nAnother significant challenge is the curse of dimensionality; as the number of features grows, the volume of the feature space increases exponentially, and the data becomes sparse. This sparsity makes it difficult for KNN to find meaningful nearest neighbors, as most points are almost equally far from each other.\nAdditionally, KNN can be computationally expensive for large datasets, as it requires storing the entire dataset and computing distances for each prediction.\nLastly, the choice of the number of neighbors (\\(k\\)) and the distance metric can greatly affect the model’s accuracy, and finding the optimal \\(k\\) can be a non-trivial task, often requiring extensive cross-validation.\nThese limitations necessitate careful pre-processing and tuning when applying KNN to classification problems, especially in complex, high-dimensional datasets.\n\n\n11.1.3 Example: Regression\nHere we consider application of KNN in a scattered data interpolation application. Such a scenarion can occur in aerospace engineering, for example in aerodynamics. In aerodynamics, engineers often deal with experimental data obtained from wind tunnel tests or flight tests. This data, representing various aerodynamic properties like lift, drag, and pressure distribution over aircraft surfaces, is often scattered. Approximating this data accurately is crucial for predicting the performance of the aircraft under different flight conditions.\nIn the following Python code we use KNN to approximate \\(\\sin(x)\\) from a set noisy scattered data. The interpolation is performed as the average of values from \\(k\\) nearest neighbouring points. The results are shown in Figure 11.3.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Step 1: Generate scattered data\n# Creating random data points\nnp.random.seed(0)\nX = np.sort(5 * np.random.rand(40, 1), axis=0)\ny = np.sin(X).ravel()\n\n# Add noise to targets\ny[::5] += 1 * (0.5 - np.random.rand(8))\n\n# Step 2: Apply KNN for interpolation\n# Create a KNN model with n neighbors\nn_neighbors = 5\nknn = KNeighborsRegressor(n_neighbors)\nknn.fit(X, y)\n\n# Generating points for prediction\nT = np.linspace(0, 5, 500)[:, np.newaxis]\n\n# Predicting values for the generated points\ny_ = knn.predict(T)\n\n# Step 3: Plotting the data\nplt.scatter(X, y, color='darkorange', label='data')\nplt.plot(T, y_, color='navy', label='prediction')\nplt.legend()\nplt.title(f\"KNN approximation with {n_neighbors} neighbors\")\nplt.show()\n\n\n\n\n\n\nFigure 11.3: Function approximation with KNN. Here we approximate \\(\\sin(x)\\) from noisy data.\n\n\n\n\n\n\nLimitations \\(k\\)-Nearest Neighbors (KNN) in regression, while useful in certain contexts, has notable limitations. One significant drawback is its sensitivity to the local structure of the data. Since KNN relies on the proximity of neighboring points for predictions, its performance can be severely impacted in areas where the data is sparse or where the neighbors do not represent the underlying trend accurately. This is particularly problematic in high-dimensional spaces due to the curse of dimensionality, where the concept of nearness becomes less meaningful and the nearest neighbors might not be close in all dimensions, leading to poor estimates.\nLike the classification scenario, KNN in regression can also be computationally demanding, particularly with large datasets. This is because it requires the calculation and comparison of distances for each query point. Additionally, the model’s significant reliance on the parameter \\(k\\) complicates tuning for optimal performance, as finding the right \\(k\\) value isn’t always straightforward."
  },
  {
    "objectID": "supervised_learning.html#logistic-regression",
    "href": "supervised_learning.html#logistic-regression",
    "title": "11  Supervised Learning",
    "section": "11.2 Logistic Regression",
    "text": "11.2 Logistic Regression\nLogistic Regression is a statistical method used in machine learning for binary classification problems—where the output is discrete and typically takes on two possible values, like “yes” or “no”, “spam” or “not spam”, “sick” or “healthy”. Despite its name suggesting a regression algorithm, it’s actually used for classification tasks.\nKey concepts of logistic regression are:\n\nSigmoid Function: The core of logistic regression is the sigmoid function (also called the logistic function). This function takes any real-valued number and maps it into a value between 0 and 1, making it particularly suitable for a model that predicts the probability of belonging to a class.\nProbability Estimation: Logistic regression estimates the probability that a given input point belongs to a certain class. For instance, in a binary classification problem, if the output of the model is greater than 0.5, we might classify the outcome as 1 (or “yes”), and if it is less than 0.5, we classify it as 0 (or “no”).\nModel Training: The parameters of the logistic regression model are trained using a method called Maximum Likelihood Estimation (MLE). The goal is to find the parameter values that maximize the likelihood of the observed data.\nLinear Decision Boundary: Logistic regression produces a linear decision boundary. This means that the boundary between the classes is a straight line (or a plane in higher dimensions).\nAdvantages: One of the key strengths of logistic regression is its ability to provide probabilities for different outcomes. This aspect goes beyond simply offering a final classification; it gives a nuanced view of the likelihood of each potential outcome. Such probabilistic outputs can be particularly informative in decision-making processes where understanding the degree of certainty or risk is as important as the decision itself. Additionally, these models can be instrumental in assessing feature importance. By analyzing how variations in input variables affect the predicted probabilities, one can gauge the relative significance of each feature. This not only aids in model interpretation but also guides feature selection and optimization, leading to more effective and efficient models.\nDisadvantages: Logistic regression is built on the assumption of a linear relationship between the independent variables and the logarithm of the odds. This linear framework, while effective for certain datasets, limits its suitability for modeling complex, nonlinear relationships inherent in some types of data. Moreover, logistic regression can be susceptible to overfitting, particularly in scenarios where the number of features significantly outweighs the number of observations. In such cases, the model might perform well on training data but fail to generalize to new, unseen data, thereby reducing its predictive power and reliability.\n\nLogistic regression is a statistical method used for binary classification. It models the probability of a binary response based on one or more predictor (independent) variables. The mathematical working of logistic regression revolves around the logistic function, which transforms linear combinations of predictors into probabilities.\nThe logistic function, also known as the sigmoid function, is defined as: \\[f(z) = \\frac{1}{1 + e^{-z}},\\]\nwhere \\(e\\) is the base of the natural logarithm and \\(z\\) is a linear combination of the independent variables, expressed as: \\[z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n.\\]\nHere, \\(\\beta_0, \\beta_1, \\ldots, \\beta_n\\) are the unknown parameters of the model, and \\(x_1, x_2, \\ldots, x_n\\) are the independent variables.\nThe logistic regression model uses the logistic function to model the probability that the dependent variable \\(Y\\) belongs to a particular category. For a binary classification (0 or 1), the probability that \\(Y = 1\\) is given by: \\[P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n)}}.\\]\nThe coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_n\\) are estimated using Maximum Likelihood Estimation (MLE). The MLE approach seeks to find the values of the coefficients that maximize the likelihood of observing the sample data.\n\n11.2.1 The MLE Approach\nMaximum Likelihood Estimation (MLE) in logistic regression is a method that applies mathematical rigor to estimate the model’s parameters in a way that the observed data are most probable. This process starts with the construction of a likelihood function that represents the probability of observing the given data under certain parameter values. In logistic regression, this likelihood for each observation is expressed as the product of the individual probabilities for each data point, following the logistic model.\nMathematically, the likelihood for an individual observation \\(Y_i\\) given features \\(X_i\\) and parameters \\(\\beta\\) is modeled as: \\[P(Y_i|X_i;\\beta) = \\sigma(X_i\\beta)^{Y_i} \\times [1 - \\sigma(X_i\\beta)]^{(1 - Y_i)}\\] where \\(\\sigma(X_i\\beta)\\) is the logistic function \\(\\frac{1}{1 + e^{-X_i\\beta}}\\), defining the probability of \\(Y_i\\) being 1.\nThe likelihood for the entire dataset of \\(n\\) observations is the product of these individual probabilities: \\[L(\\beta) = \\prod_{i=1}^{n} P(Y_i|X_i;\\beta)\\]\nGiven that direct multiplication can lead to numerically unstable results due to very small probabilities, we use the log-likelihood function: \\[\\ell(\\beta) = \\sum_{i=1}^{n} [ Y_i \\log(\\sigma(X_i\\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i\\beta)) ]\\]\nThe MLE approach involves finding the parameter values \\(\\beta\\) that maximize this log-likelihood function. This maximization problem does not have a closed-form solution like linear regression, so iterative optimization algorithms such as gradient descent are employed. These algorithms adjust \\(\\beta\\) iteratively to find the maximum of \\(\\ell(\\beta)\\).\nDuring each iteration, the gradient of the log-likelihood function with respect to \\(\\beta\\) is computed to determine the direction in which \\(\\beta\\) should be adjusted. The process repeats until it converges to the parameter values that yield the maximum log-likelihood, indicating the most probable parameters given the observed data, under the logistic regression model.\nThis mathematical framework ensures that MLE in logistic regression is not just a heuristic but a statistically sound method for parameter estimation, aligning the model as closely as possible with the observed empirical data.\nThe logistic regression model is essentially modeling the log-odds of the probability of the event. The log-odds are given by the logarithm of the odds ratio:\n\\[\\log\\left(\\frac{P(Y=1)}{1 - P(Y=1)}\\right) = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n.\\]\nThis equation shows that logistic regression is modeling a linear relationship between the independent variables and the log-odds of the dependent variable.\n\n\n11.2.2 Making Predictions\nAfter completing the optimization stage in logistic regression, where the model’s coefficients are determined through Maximum Likelihood Estimation (MLE), the model becomes capable of predicting outcomes for new data. This prediction involves a series of precise mathematical steps. Initially, the logistic regression model utilizes its finely-tuned coefficients, \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n\\), to analyze the fresh data. Given a new observation with features denoted as \\(X_{\\text{new}}\\), the model performs a calculation of the linear combination of these features with the coefficients: \\[z = \\beta_0 + \\beta_1 X_{\\text{new}_1} + \\beta_2 X_{\\text{new}_2} + \\cdots + \\beta_n X_{\\text{new}_n}.\\]\nThis computed value, \\(z\\), is then fed into the logistic (sigmoid) function, represented by \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\). This function converts \\(z\\) into a probability value ranging from 0 to 1, reflecting the probability that the new data point belongs to the positive class, commonly labeled as “1”.\nTo translate this probability into a binary classification, a threshold is applied — commonly set at 0.5. If the probability \\(\\sigma(z) \\geq 0.5\\), the model classifies the observation as belonging to the positive class. Conversely, if it falls below 0.5, the observation is assigned to the negative class (“0”). This threshold can be adjusted to suit specific needs, such as balancing precision and recall in the model’s predictions.\nIn summary, logistic regression mathematically models the relationship between independent variables and the probability of a particular outcome. It’s a linear model for the log-odds, but represents a non-linear relationship between the dependent and independent variables.\nLogistic regression is widely used because of its simplicity and effectiveness in cases where the relationship between the independent variables and the dependent variable is approximately linear. However, in cases where this linearity assumption doesn’t hold, other more complex algorithms might be more appropriate.\n\n\n11.2.3 Example\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Step 1: Generate a synthetic dataset\nX, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n                           random_state=42, n_clusters_per_class=1)\n\n# Step 2: Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Step 3: Train the logistic regression model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Step 4: Evaluate the model\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nplt.subplot(1,2,1);\nplt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], s=30, label=\"class 1\")\nplt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], s=30, label=\"class 1\")\nplt.title(\"Training data.\")\n\nplt.subplot(1,2,2);\nplt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], s=30, label=\"class 1\")\nplt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], s=30, label=\"class 1\")\nplt.title(\"Testing data.\")\nplt.tight_layout()\n\nprint(\"Model Accuracy:\", accuracy)\nprint(\"Classification Report:\\n\", report)\n\n\n\nModel Accuracy: 1.0\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        15\n           1       1.00      1.00      1.00        15\n\n    accuracy                           1.00        30\n   macro avg       1.00      1.00      1.00        30\nweighted avg       1.00      1.00      1.00        30\n\n\n\n\n\n\n\n\nFigure 11.4: Data used for training and testing logistic regression.\n\n\n\n\n\n\nThe above Python code implements data classification based on logistic regression. The training and testing data are shown in Figure 11.4."
  },
  {
    "objectID": "supervised_learning.html#support-vector-machines",
    "href": "supervised_learning.html#support-vector-machines",
    "title": "11  Supervised Learning",
    "section": "11.3 Support Vector Machines",
    "text": "11.3 Support Vector Machines\nSupport Vector Machines (SVMs) represent a powerful and versatile class of supervised learning algorithms, widely used for both classification and regression tasks. At their core, SVMs seek to find the best hyperplane that separates different classes in a dataset, particularly excelling in high-dimensional spaces. This is achieved through the identification of support vectors and the maximization of the margin between data points of different classes. One of the key strengths of SVMs is their ability to use kernel functions, which enables them to handle non-linear relationships by transforming data into higher dimensions where it can be linearly separable. Originally developed in the 1960s, SVMs have evolved significantly and are highly regarded in the machine learning community for their robustness and effectiveness, especially in complex domains where the relationship between attributes is not readily apparent.\n\n11.3.1 Linear Classification\nSVMs operate by finding a hyperplane in an \\(N\\)-dimensional space (where \\(N\\) is the number of features) that distinctly classifies data points. A hyperplane is essentially a decision boundary that separates data points of different classes.\nIn two dimensions, this hyperplane is a line, and in three dimensions, it’s a plane. For higher dimensions, we still refer to it as a hyperplane. Mathematically, a hyperplane can be described by the equation:\n\\[\\boldsymbol{w}^T\\boldsymbol{x}+ b = 0.\\]\nHere, \\(\\boldsymbol{w}\\in\\mathcal{R}^n\\) is the weight vector, \\(\\boldsymbol{x}\\in\\mathcal{R}^n\\) represents the input features, and \\(b\\) is the bias.\nSupport vectors are the data points that are closest to the hyperplane and influence its position and orientation. They essentially “support” the hyperplane in the SVM model.\nMargin is the distance between the hyperplane and the nearest data point from either class. Maximizing this margin is the key objective in SVMs. The margin is calculated as the perpendicular distance from the line to the support vectors.\nIn a classification task, we deal with two hyperplanes that pass through the support vectors. These are defined as: \\[\\begin{align*}\n\\boldsymbol{w}^T\\boldsymbol{x}+ b = 1 \\quad \\text{(for one class)},\\\\\n\\boldsymbol{w}^T\\boldsymbol{x}+ b = -1 \\quad \\text{(for the other class)}.\n\\end{align*}\\]\nFor a linearly separable set of 2D-points which belong to one of two classes, the goal is to find the maximum-margin hyperplane that divides the classes. The optimization problem is formulated as: \\[\n\\min_{\\boldsymbol{w}\\in\\mathcal{R}^n} \\; \\frac{1}{2} \\|\\boldsymbol{w} \\|^2,\\;\\; \\text{subject to }  y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i + b) \\geq 1 \\text{ for each data point } i.\n\\] Here, \\(y_i\\) are the labels (e.g., \\(-1\\) or \\(1\\) for a binary classification).\nHere is a Python code example using Scikit-learn that demonstrates the application of a Support Vector Machine (SVM) for a linearly separable dataset. The code first generates a simple, linearly separable dataset using datasets.make_blobs. This function creates two clusters of data points, ideal for binary classification. Next, a linear SVM classifier (SVC with kernel='linear') is created and fitted to the data. The parameter C=1000 is chosen to emphasize the decision boundary. The data points are plotted using different colors for each class, along with the decision function of the SVM – which includes the decision bounday (where the decision function is zero) and the margins (where the decision function is \\(-1\\) and \\(1\\)). The support vectors, which are critical in defining the decision boundary, are circled.\n\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load a simple, linearly separable dataset\nX, y = datasets.make_blobs(n_samples=100, centers=2, random_state=6)\n\n# Create a linear SVM classifier\nclf = SVC(kernel='linear', C=1000)\nclf.fit(X, y)\n\n# Plotting the data points\nplt.scatter(X[y==0, 0], X[y==0, 1], s=30, label=\"class 1\")\nplt.scatter(X[y==1, 0], X[y==1, 1], s=30, label=\"class 2\")\n\n# Plotting the decision function\nax = plt.gca()\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\n\n# Creating a grid to evaluate the model\nxx = np.linspace(xlim[0], xlim[1], 30)\nyy = np.linspace(ylim[0], ylim[1], 30)\nYY, XX = np.meshgrid(yy, xx)\nxy = np.vstack([XX.ravel(), YY.ravel()]).T\nZ = clf.decision_function(xy).reshape(XX.shape)\n\n# Plotting decision boundary and margins\nax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n\n# Highlighting support vectors\nax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k',label=\"support vectors\")\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('SVM Linear Classifier on Linearly Separable Data')\nplt.legend()\nplt.show()\n\n\n\n\nThe resulting plot visually demonstrates how the linear SVM successfully separates the two classes with a clear margin. The support vectors are the points that lie on the margins of the classifier.\n\n\n11.3.2 Non-linear Classification with Kernel Trick\nIn numerous practical situations, data doesn’t naturally separate into distinct linear categories. This challenge is effectively addressed by the kernel trick. The essence of the kernel trick is to project the data into a space of higher dimensions, where linear separation is feasible. This technique enables the operation within an implicitly high-dimensional feature space without the need to explicitly calculate the data coordinates in that space. This approach intelligently sidesteps the usually extensive computational demands associated with high-dimensional data processing.\nLet us consider a mapping \\(\\phi: \\mathcal{R}^n \\rightarrow \\mathcal{R}^m\\), which transforms the original feature space \\(\\mathcal{R}^n\\) to a higher-dimensional feature space \\(\\mathcal{R}^m\\). In this higher-dimensional space, the inner product of two vectors \\(\\boldsymbol{x}_i\\) and \\(\\boldsymbol{x}_j\\) is given by \\(\\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x}_j) \\rangle\\).\nCalculating this inner product directly in the higher-dimensional space can be highly computationally intensive. The kernel trick involves using a kernel function \\(K(\\cdot,\\cdot)\\) that corresponds to this inner product, i.e.,\n\\[K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x}_j) \\rangle.\\]\nThis kernel function computes the inner product in the transformed space without explicitly performing the transformation \\(\\phi(\\cdot)\\). Essentially, \\(K(\\cdot,\\cdot)\\) is a measure of similarity between \\(\\boldsymbol{x}_i\\) and \\(\\boldsymbol{x}_j\\) in the transformed space.\nSome of the common kernels are:\n\nLinear Kernels: This is the simplest kernel function, defined as \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\boldsymbol{x}_i^T \\boldsymbol{x}_j\\). It does not actually map the data into a higher-dimensional space, and is equivalent to no mapping.\nPolynomial Kernels: Defined as \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = (1 + \\boldsymbol{x}_i^T \\boldsymbol{x}_j)^d\\), where \\(d\\) is the degree of the polynomial. This kernel maps the data into a polynomial feature space.\nRadial Basis Function (RBF) or Gaussian Kernels: It’s given by \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\exp(-\\gamma \\| \\boldsymbol{x}_i - \\boldsymbol{x}_j \\|^2)\\), where \\(\\gamma\\) is a parameter. It maps data into an infinite-dimensional space and is widely used for its properties in non-linear separation.\nPerceptron Kernels: Defined as \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\tanh(\\alpha \\boldsymbol{x}_i^T \\boldsymbol{x}_j + c)\\), where \\(\\alpha\\) and \\(c\\) are constants. This kernel transforms the data similarly to a neural network.\nAdditive Kernels: More complicated kernels can be formed by adding several kernels, possibly of different kinds, since sum of positive definite functions is also positive definite. Such kernels are defined as \\(K(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\sum_k K_k(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\\).\nTensor Product Kernels:: Multi-dimensional kernels can be formed from tensor product of different kernels, i.e., \\(K(\\boldsymbol{x}_i,\\boldsymbol{x}_j) = \\Pi_k K_k(\\boldsymbol{x}_i,\\boldsymbol{x}_j)\\).\n\nThe kernel trick is primarily used in SVMs but is also applicable in other areas like principal component analysis (kernel PCA), ridge regression, and more. It allows these algorithms to solve non-linear problems by implicitly using higher-dimensional feature spaces, thereby greatly expanding their applicability without a significant increase in computational cost. However, the choice of the kernel and its parameters can significantly affect the performance of the algorithm and requires careful tuning based on the specific data and problem.\nExample Here is a Python example demonstrating non-linear classification using Support Vector Machines (SVMs) with a Radial Basis Function (RBF) kernel.\n\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate a non-linearly separable dataset (e.g., two interleaving half circles)\nX, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardizing the dataset\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create a non-linear SVM classifier with RBF kernel\nclf = SVC(kernel='rbf', C=1, gamma='auto')\nclf.fit(X_train_scaled, y_train)\n\n# Plotting the decision boundary\ndef plot_decision_boundary(clf, X, y, plot_support=True):\n    # Create a grid to plot decision boundaries\n    x0, x1 = np.meshgrid(\n        np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, num=100),\n        np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, num=100)\n    )\n    X_new = np.c_[x0.ravel(), x1.ravel()]\n    \n    # Prediction for each point in the grid\n    y_pred = clf.predict(X_new).reshape(x0.shape)\n    \n    # Plotting the contour and training points\n    plt.contourf(x0, x1, y_pred, alpha=0.3, cmap=plt.cm.brg)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.brg)\n    if plot_support:\n        # Highlight support vectors\n        sv = clf.support_vectors_\n        plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='k')\n\n# Plotting the decision boundary for the classifier\nplot_decision_boundary(clf, X_train_scaled, y_train)\nplt.title(\"Non-linear SVM Classifier with RBF Kernel\")\nplt.xlabel(\"Feature 1\")\nplt.ylabel(\"Feature 2\")\nplt.show()\n\n\n\n\n\n\nFigure 11.5: Nonlinear classification using SVM.\n\n\n\n\n\n\nIn this Python example, a non-linear classification is demonstrated using Support Vector Machines (SVMs) with a Radial Basis Function (RBF) kernel. The process starts with generating a non-linearly separable dataset using Scikit-learn’s make_moons function, which creates two interleaving half-circles. The dataset is then split into training and testing sets and standardized using StandardScaler to ensure equal contribution of each feature in the SVM’s distance calculations.\nThe SVM model is configured with an RBF kernel (SVC(kernel='rbf')), chosen for its effectiveness in handling non-linear data. Parameters C and gamma are set to balance between low training error and generalization. A custom function, plot_decision_boundary, visualizes the decision boundary of the SVM. It produces a contour plot (see Figure 11.5) showing how the SVM with an RBF kernel successfully classifies the complex dataset, with the support vectors prominently highlighted. This example showcases the SVM’s capability to handle non-linear data and the adaptability of the RBF kernel to complex data patterns.\nFigure 11.5 shows that the classification is not clean. Non-linear Support Vector Machines (SVMs) may struggle to cleanly separate data classes due to several inherent challenges related to the data and the model itself. The primary issue often lies in the inherent overlap within the data: real-world datasets frequently exhibit classes with overlapping distributions, meaning some data points naturally share characteristics of more than one class. This overlap makes perfect separation, even with sophisticated models, inherently challenging.\nThe effectiveness of a non-linear SVM greatly depends on the choice of the kernel function, such as RBF, polynomial, or sigmoid, and the tuning of its parameters. An inappropriate kernel or poorly chosen parameters can lead to inadequate separation of the classes. Furthermore, non-linear SVMs can suffer from overfitting, especially if the decision boundary becomes excessively complex in an attempt to capture subtle patterns in the training data. This complexity might make the model too sensitive to the noise and outliers in the data, impairing its ability to generalize to new, unseen data.\nThe transformation of data into a higher-dimensional space, a common strategy in non-linear SVMs to achieve separability, can paradoxically complicate the relationships within the data, making clean separation more difficult. Additionally, the presence of noise and outliers can significantly skew the decision boundary. Non-linear SVMs, in their effort to accommodate these anomalies, might fail to establish a clear division between classes.\nLastly, the representation and preprocessing of features play a crucial role. If the features do not adequately capture the distinct characteristics of each class, the SVM may not be able to effectively differentiate between them. Thus, while non-linear SVMs are powerful tools for complex, non-linear datasets, their success in cleanly separating classes hinges on the nature of the data, the selection and tuning of the kernel, and the presence of noise and outliers. Effective separation often requires meticulous data preprocessing, feature engineering, and model parameter tuning.\n\n\n11.3.3 Data Normalization\nData normalization is a crucial preprocessing step in machine learning, especially when working with Support Vector Machines (SVMs) and their associated kernel functions. Certain kernels, due to their intrinsic mathematical properties, have a restricted domain where they operate effectively. For these kernels, normalization of the input data becomes not just beneficial but necessary. However, even for kernels without such restrictions, normalization can still be advantageous.\n\nKernels with Restricted Domain: Some kernel functions, like the Radial Basis Function (RBF) or Gaussian kernel, are sensitive to the scale of the input features. These kernels compute distances between data points; thus, features on larger scales can dominate the distance metric, leading to biased results. Normalizing data ensures that each feature contributes proportionately to the distance calculations.\nAdvantages for Unrestricted Kernels: Even for kernels that do not have a restricted domain, such as the linear kernel, normalization can be beneficial. It helps in avoiding numerical instability and ensures that the optimization algorithm used for training the SVM converges more efficiently.\nIsotropic vs. Non-Isotropic Normalization:\n\nIsotropic Normalization: This involves scaling the data so that the variance is the same for each feature. It treats all dimensions equally and is commonly achieved through methods like standardization, where each feature is centered around zero with unit variance.\nNon-Isotropic Normalization: Here, different scaling is applied to different features. This might be necessary when features have different units or scales of measurement, and you want to preserve these differences to some extent.\n\nConsideration of Input Features: Deciding whether to normalize the data (and what type of normalization to use) requires careful consideration of the input features. Features with different scales, units, and distributions might influence the SVM’s performance, and choosing the right normalization technique can significantly impact the effectiveness of the model.\nImproving the Condition Number of the Hessian: In the optimization problem solved during SVM training, the Hessian matrix plays a critical role. Normalization can improve the condition number of this matrix, which is a measure of its sensitivity to numerical errors. A well-conditioned Hessian ensures that the optimization algorithm is stable and converges efficiently, leading to a more robust and accurate SVM model.\n\nIn summary, data normalization is a key step in preparing data for SVMs with different kernels. It not only accommodates the mathematical requirements of certain kernels but also enhances the overall stability and performance of the SVM training process. The choice of normalization technique should be made in the context of the specific dataset and the characteristics of the input features.\n\n\n11.3.4 Vapnik-Chervonenkis (VC) Dimension\nThe Vapnik-Chervonenkis (VC) dimension is a fundamental concept in statistical learning theory, named after Vladimir Vapnik and Alexey Chervonenkis. It measures the capacity of a set of functions to classify sets of points in all possible ways, essentially quantifying the model’s complexity or expressive power.\nDefinition: For a given set of functions (hypotheses), the VC dimension is the largest number of points that can be shattered by these functions. “Shattering” means that for every possible way of labeling these points (into two classes), there is a function in the set that can separate the points into the two classes exactly as per the labeling.\nImplication in Machine Learning: In the context of machine learning models, like SVMs, the VC dimension provides a theoretical upper bound on the model’s capacity to learn from data. A higher VC dimension indicates a more complex model, which can lead to a better fit to the training data. However, it also increases the risk of overfitting, where the model captures noise rather than the underlying pattern.\nCalculating the exact VC dimension for a given machine learning model can be complex and is often not straightforward. There are no generic algorithms or formulae that can directly compute the VC dimension for all types of models, especially for non-linear models or those involving kernel methods like SVMs.\nHowever, we can estimate or get insights into the complexity of a model (akin to understanding its VC dimension) using certain practical approaches:\n\nModel Complexity Parameters: For some models, the complexity parameters give an indication of the VC dimension. For instance, in SVMs, the choice of the kernel and its parameters can influence the VC dimension.\nEmpirical Estimation: We can empirically estimate the model’s capacity by observing its performance on training and validation datasets. If a model can perfectly classify a training set of a certain size but fails to generalize to new data, it may indicate a high VC dimension relative to the size of the training data.\nTheoretical Calculation: For simpler models (like linear classifiers in low-dimensional spaces), the VC dimension can sometimes be calculated directly. For example, the VC dimension of a linear classifier in an \\(n\\)-dimensional space is \\(n+1\\).\n\nIn Python, directly calculating the VC dimension is not commonly done for complex models. Instead, techniques like cross-validation and observing training versus validation performance are used to gauge a model’s complexity and generalization capability.\nWe can use libraries like Scikit-learn in Python to experiment with different model complexities and observe overfitting versus underfitting. This empirical approach doesn’t calculate the VC dimension explicitly but helps understand the model’s capacity, which is what the VC dimension conceptually represents.\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\nimport numpy as np\n\n# Example using a dataset (X, y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\n# Experiment with different complexities (like different kernels or C values for SVM)\nmodel = SVC(kernel='linear', C=1)\nmodel.fit(X_train, y_train)\n\n# Evaluate on training and test sets\ntrain_score = model.score(X_train, y_train)\ntest_score = model.score(X_test, y_test)\n\n# Perform cross-validation\ncv_scores = cross_val_score(model, X, y, cv=5)\n\n# Observing the differences in scores can give insights into the model's complexity and generalization ability\nprint(f\"Training Score: {train_score}, \\nTest Score: {test_score}, \\nCV Scores: {np.mean(cv_scores)}\")\n\nTraining Score: 0.8714285714285714, \nTest Score: 0.8333333333333334, \nCV Scores: 0.8699999999999999\n\n\nIn summary, while calculating the VC dimension for complex models in Python is not straightforward, understanding the underlying concept and using empirical methods to evaluate model complexity can serve a similar purpose in practical machine learning applications.\n\n\n11.3.5 Kernel Selection\nThe selection of an appropriate kernel function in Support Vector Machines (SVMs) is a critical step that directly impacts the performance of the model. Given the plethora of kernel mappings available, the decision on which kernel to use for a specific problem is not straightforward. This challenge has been a longstanding topic in the field of machine learning, and the integration of various kernels within a unified framework facilitates a more systematic comparison of their performance.\n\nComparing Kernels Using Theoretical Measures: One potential method for comparing different kernels is to use theoretical measures like the upper bound on the Vapnik-Chervonenkis (VC) dimension. The VC dimension provides a measure of the capacity or complexity of a set of functions (in this case, the kernel functions), with a lower VC dimension suggesting a potentially more generalizable model. However, applying this measure practically is challenging. It involves estimating parameters like the radius of the hypersphere that encloses the data in the transformed, non-linear feature space. This estimation can be complex and may not always yield practical insights for kernel selection.\nPractical Challenges in Theoretical Selection: While theoretical frameworks can guide kernel selection, they often require complex calculations and assumptions that may not hold in real-world scenarios. For instance, estimating the radius of the hypersphere in the feature space is not straightforward and could lead to inaccuracies in the theoretical comparison of kernels.\nEmpirical Methods Remain Predominant: Due to these challenges, empirical methods like bootstrapping and cross-validation continue to be the preferred approaches for kernel selection. These methods involve training the SVM with different kernels on a subset of the data and validating their performance on an independent test set. By comparing the model’s performance across various kernels using these techniques, practitioners can select the kernel that offers the best trade-off between complexity and accuracy for their specific problem.\nImportance of Independent Test Sets: Independent test sets are crucial in this process as they provide an unbiased evaluation of the model’s performance. A kernel that performs well on the training data might not necessarily generalize well to unseen data. Therefore, validation on independent test sets is essential to ensure that the selected kernel is not only theoretically sound but also practically effective.\nFinal Caution: It’s important to remember that even if a robust theoretical method for kernel selection is developed, it must be validated empirically across a wide range of problems to ensure its reliability. Machine learning, especially in complex real-world applications, often requires a balance between theoretical soundness and empirical validation.\n\nIn conclusion, kernel selection in SVMs is a nuanced process that involves considering both theoretical measures and empirical validation. While theoretical tools like the upper bound on the VC dimension can offer insights, practical methods like cross-validation and bootstrapping, validated against independent test sets, remain essential for making informed decisions about kernel selection.\n\n\n11.3.6 SVM for Regression (SVR)\nSupport Vector Machine (SVM) regression, also known as Support Vector Regression (SVR), is an extension of the SVM algorithm from classification to regression problems. While the classification version of SVM focuses on finding a hyperplane that best separates two classes, SVR aims to find a function that approximates the relationship between input features and continuous target values. Here’s a mathematical explanation of how SVM can be used for regression:\nIn SVM for regression, the goal is to find a function \\(f(\\boldsymbol{x}): \\mathcal{R}^n\\mapsto \\mathcal{R}\\) that has at most an ε (epsilon) deviation from the actually obtained targets \\(y_i\\in\\mathcal{R}\\) for all the training data, and at the same time is as flat as possible.\nLet’s consider a dataset with inputs \\(\\boldsymbol{x}_i\\in\\mathcal{R}^n\\) and outputs \\(y_i\\), where \\(i = 1, ..., n\\). SVM regression tries to fit the function: \\[ f(\\boldsymbol{x}) = \\boldsymbol{w}^T\\boldsymbol{x}+ b.\\]\nHere, \\(\\boldsymbol{w}\\in\\mathcal{R}^n\\) is the weight vector and \\(b\\in\\mathcal{R}\\) is the bias. The objective is to minimize the norm of \\(\\boldsymbol{w}\\) (i.e., \\(\\boldsymbol{w}^T\\boldsymbol{w}\\) to keep the model as simple or as flat as possible, which helps in generalization.\nEpsilon-Insensitive Tube SVR allows some errors in the approximation of the target values while keeping the model simple. This is achieved by introducing an ε-insensitive loss function, which does not penalize errors that are within a margin of ε from the true value. Mathematically, this can be represented as: \\[\\text{Loss} = \\max(0, |y_i - f(\\boldsymbol{x}_i)| - \\epsilon).\\]\nThis creates an ε-insensitive tube or band around the regression function. Points that fall within this tube do not contribute to the loss in the model.\nOptimization Problem The optimization problem for Support Vector Regression (SVR) involves finding a set of parameters that best fit the regression function to the data while maintaining a balance between the model’s complexity and the allowance for deviations beyond a certain threshold. When formulated in terms of vectors, the optimization problem becomes a convex quadratic programming problem.\nThe primary objective in SVR is to find a function \\(f(x) = \\boldsymbol{w}^T\\boldsymbol{x}+ b\\) that approximates the relationship between the input vectors \\(\\boldsymbol{x}\\) and the target values \\(y\\), with a certain tolerance for errors. The objective function aims to minimize the norm of the weight vector \\(\\boldsymbol{w}\\) (which corresponds to the flatness of the function) along with the penalty for errors exceeding a margin \\(\\epsilon\\).\nThe optimization problem is given by \\[\\begin{align*}\n& \\min_{\\boldsymbol{w},b} \\boldsymbol{w}^T\\boldsymbol{w}, \\\\\n\\text{subject to } & \\\\\n& y_i - (\\boldsymbol{w}^T\\boldsymbol{x}_i + b) \\leq \\epsilon,\\\\\n& (\\boldsymbol{w}^T\\boldsymbol{x}_i + b) - y_i \\leq \\epsilon.\\\\\n\\end{align*}\\]\nTo allow for some flexibility in this model (tolerating deviations larger than ε for some points), slack variables \\(\\xi_i\\) and \\(\\xi_i^*\\) are introduced. The new optimization problem becomes:\n\\[\\begin{align*}\n& \\min_{\\boldsymbol{w},b,\\boldsymbol{\\xi},\\boldsymbol{\\xi}^\\ast} \\boldsymbol{w}^T\\boldsymbol{w} + C \\sum_{i=1}^{n} (\\xi_i + \\xi_i^\\ast), \\\\\n\\text{subject to } & \\\\\n& y_i - (\\boldsymbol{w}^T\\boldsymbol{x}_i + b) \\leq \\epsilon + \\xi_i,\\\\\n& (\\boldsymbol{w}^T\\boldsymbol{x}_i + b) - y_i \\leq \\epsilon + \\xi_i^\\ast,\n\\end{align*}\\] where \\(\\boldsymbol{\\xi}\\) and \\(\\boldsymbol{\\xi}^\\ast\\) are defined by components \\(\\xi_i\\) and \\(\\xi_i^\\ast\\) respectively.\nKernel Extension For non-linear regression, the optimization problem can incorporate the kernel trick, where the dot product \\(\\boldsymbol{w}^T\\boldsymbol{x}_i\\) is replaced by a kernel function \\(K(\\boldsymbol{x}_i, \\boldsymbol{x}_j)\\). This allows the SVR to capture non-linear relationships without explicitly transforming the data into a higher-dimensional space.\nIn summary, the SVR optimization problem in vector form involves minimizing a function that balances the flatness of the regression model and the penalty for errors exceeding an ε-margin, subject to constraints that allow some flexibility for deviations. This formulation is solved using quadratic programming techniques, and the incorporation of the kernel trick extends its applicability to non-linear regression problems.\nExample\n\nfrom sklearn.svm import SVR\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Generate a non-linear regression dataset\nX, y = make_regression(n_samples=300, n_features=1, noise=10, random_state=42)\ny = np.sin(X).ravel()  # Making the relationship non-linear\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Standardize the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ndef svr(X_train,y_train,kernel,C,gamma,epsilon):\n    svr_rbf = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)\n    svr_rbf.fit(X_train, y_train)\n    y_pred = svr_rbf.predict(X_test_scaled)\n    mse = mean_squared_error(y_test, y_pred)\n    return y_pred, mse\n\nC = 100\ngamma = 0.1\nkernel = 'rbf'\n\n# Nominal accuracy\nepsilon1 = 0.1\ny_pred1, mse1 =  svr(X_train_scaled,y_train,kernel,C,gamma,epsilon1)\nprint(f'Mean square error (nominal):{mse1}')\n\n# High accuracy\nepsilon2 = 0.01\ny_pred2, mse2 =  svr(X_train_scaled,y_train,kernel,C,gamma,epsilon2)\nprint(f'Mean square error (high accuracy):{mse2}')\n\ndef plot_results(X_train, X_test, y_train, y_test, y_pred,epsilon):\n    s = 10\n    plt.scatter(X_train, y_train, color='gray', label='Training data',s=s)\n    plt.scatter(X_test, y_test, color='red', label='Test data',s=s)\n    plt.scatter(X_test, y_pred,  label='SVR prediction',s=s)\n    plt.title(f'SVR for Non-linear Regression, $\\epsilon$={epsilon}.')\n    plt.xlabel('Feature: x')\n    plt.ylabel('Target: f(x)')\n    plt.legend()\n\nplt.figure(figsize=(10,6))\nplt.subplot(1,2,1); plot_results(X_train_scaled, X_test_scaled, y_train, y_test, y_pred1, epsilon1)\nplt.subplot(1,2,2); plot_results(X_train_scaled, X_test_scaled, y_train, y_test, y_pred2, epsilon2)\nplt.tight_layout()\nplt.show()\n\n\n\nMean square error (nominal):0.0054106869508642776\nMean square error (high accuracy):0.00011740223018556576\n\n\n\n\n\n\n\nFigure 11.6: Approximation of \\(\\sin(x)\\) from scattered data using SVR with an RBF kernel. Parameters \\(\\gamma\\), \\(C\\) and \\(\\epsilon\\) control tradeoff between overfit and accuracy.\n\n\n\n\n\n\nIn this Python example, we demonstrate the use of Support Vector Regression (SVR) for a non-linear regression task, employing the Radial Basis Function (RBF) kernel. The process begins with the generation of a regression dataset using Scikit-learn’s make_regression function. To introduce non-linearity into the dataset, the target variable y is transformed using a sine function, creating a more complex relationship between the input features and the target.\nThe dataset is then divided into training and testing sets to enable model evaluation. Before training the model, the data undergoes standardization using StandardScaler. This step ensures that each feature contributes equally to the distance calculations in the SVR model, which is particularly important for kernels like RBF that are sensitive to the scale of input features.\nThe SVR model is instantiated with an RBF kernel by setting kernel='rbf'. Parameters such as C, gamma, and epsilon are configured to control the complexity of the model and its sensitivity to deviations from the training data. The model is trained on the scaled training data and subsequently used to make predictions on the scaled test data.\nTo evaluate the performance of the model, the mean squared error (MSE) is calculated, comparing the predicted values with the actual values in the test set. A lower MSE value indicates a better fit of the model to the data.\nFinally, the results are visualized through a scatter plot that displays the training data, test data, and the predictions made by the SVR model. Figure 11.6 clearly shows how the SVR with an RBF kernel is able to capture the underlying non-linear pattern in the data. The calculated MSE of approximately 0.0054, with \\(\\epsilon = 0.1\\), further confirms the model’s effectiveness in accurately modeling the complex, non-linear relationships present in the dataset. Reducing \\(\\epsilon\\) to \\(0.01\\) further reduces the MSE.\n\n\n11.3.7 Example: Fault Diagnostics Using Classification\nNeed to think about this one."
  },
  {
    "objectID": "dimensionality_reduction.html#pca",
    "href": "dimensionality_reduction.html#pca",
    "title": "13  Dimensionality Reduction",
    "section": "13.1 PCA",
    "text": "13.1 PCA\n\nExamples\nWe next consider an image approximation (or compression) example where matrix basis are used. Consider the following image:\n\n\nTrue\n\n\n\n\n\nFigure 13.1: An 100x100 image of a circle.\n\n\nFigure 13.1 shows an image of a circle, which is \\(100 \\times 100\\) in size. While the image is in 2D, the pixel space is \\(\\mathcal{R}^{100\\times 100}\\), which needs \\(100\\times 100=10000\\) basis matrices!\nHowever, looking at the image, we can see that most of the image is empty (black pixels) and the circle can be represented with much smaller number of basis functions. Thus, by inspection, we can infer that the image can be represented in a lower dimensional space. We can apply singular value decomposition to determine lower dimensional (or reduced order) representation of the image. This is also known as image compression.\nThe following Python code demonstrates it.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.linalg import svd\nfrom PIL import Image\n\n# Load the image into a numpy array.\nimg = Image.open('images/circle.png')\n\n# Perform SVD\nU, Sigma, Vt = svd(img)\n\nmarkerline, stemline, baseline = plt.stem(range(len(Sigma)),Sigma)\nplt.setp(markerline, markersize = 3)\nplt.title(\"Singular values of the image matrix.\")\nplt.xlabel(\"Index k.\")\nplt.ylabel(\"Singular Value\")\n\n\n\n\n\nText(0, 0.5, 'Singular Value')\n(a) Singular values of the image matrix. Most of the singular values are zero, which means the image can be represented in lower dimensional space.\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\nFigure 13.2: ?(caption)\n\n\n\nFigure 13.2 shows the singular values of the image matrix. We observe that most of the singular values are zero, indicating the image can be represented in a much lower dimensional space. The next Python code shows how it can be done.\n\ndef compress_image(U,Vt,Sigma,k):\n  U_k = U[:, :k]\n  Sigma_k = np.diag(Sigma[:k])\n  Vt_k = Vt[:k, :]\n  A_k = np.dot(U_k, np.dot(Sigma_k, Vt_k))\n  print(U_k.shape)\n  return A_k\n\nimg1 = compress_image(U,Vt,Sigma,20)\nimg2 = compress_image(U,Vt,Sigma,10)\nimg3 = compress_image(U,Vt,Sigma,5)\n\n\n# Plot the original and the compressed image\nplt.figure()\nplt.subplot(2, 2, 1)\nplt.imshow(img, cmap='gray')\nplt.title('Original Image')\n\nplt.subplot(2, 2, 2)\nplt.imshow(img1, cmap='gray')\nplt.title('Compressed Image with k=20')\n\nplt.subplot(2, 2, 3)\nplt.imshow(img2, cmap='gray')\nplt.title('Compressed Image with k=10')\n\nplt.subplot(2, 2, 4)\nplt.imshow(img3, cmap='gray')\nplt.title('Compressed Image with k=5')\n\nplt.tight_layout()\nplt.show()\n\n\n\n(100, 20)\n(100, 10)\n(100, 5)\n\n\n\n\n\n\n\nVarious representations of original image in lower dimensional spaces."
  },
  {
    "objectID": "application1.html#surrogate-modeling",
    "href": "application1.html#surrogate-modeling",
    "title": "14  Aerospace Application 1",
    "section": "14.1 Surrogate Modeling",
    "text": "14.1 Surrogate Modeling\nFunction approximation is intricately connected to the concept of surrogate modeling, a technique widely used in various scientific and engineering disciplines. Surrogate modeling involves creating a simpler, computationally efficient model (the surrogate) that approximates a more complex or computationally expensive simulation or function. Essentially, it’s about using function approximation methods to build a model that can predict the outputs of a complex system based on its inputs, without having to run the full, detailed simulation or process every time. This approach is particularly valuable in scenarios where the actual computational model is too time-consuming or resource-intensive. By approximating the function of interest, surrogate models enable quicker evaluations, facilitating tasks like optimization, sensitivity analysis, and uncertainty quantification. These models often employ polynomial approximations, piecewise functions, or advanced machine learning techniques, including neural networks, to capture the behavior of the complex system with sufficient accuracy. This not only saves significant computational resources but also opens up new possibilities for analyzing and understanding the underlying system, making surrogate modeling an essential tool in fields ranging from aerospace engineering to financial modeling.\nIn aerospace engineering, surrogate modeling plays a pivotal role in streamlining the design and analysis processes, where high-fidelity simulations are often computationally expensive and time-consuming. These surrogate models, also known as metamodels, are used extensively for aerodynamic shape optimization, where they approximate complex fluid dynamics simulations to quickly evaluate the performance of various aircraft designs under different flight conditions. This allows for rapid exploration of a large design space, identifying optimal shapes that balance factors like lift, drag, and stability with computational efficiency. Surrogate models are also crucial in structural optimization, helping engineers to predict the strength, weight, and durability of aircraft components while minimizing the need for costly physical prototypes. Additionally, they are employed in the analysis of combustion processes in jet engines, where the models approximate the behavior of fuel burn and emissions, enabling more efficient and environmentally friendly engine designs. By leveraging these models, aerospace engineers can significantly reduce development cycles, optimize performance, and ensure safety, all while managing the immense computational costs associated with simulating complex aerospace systems."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "16  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Machine Learning for Aerospace Systems",
    "section": "",
    "text": "Preface\nThis book aims to introduce machine learning to senior-level undergraduates and graduate students in aerospace engineering.\n\nWhat is Machine Learning?\nMachine learning, a transformative branch of artificial intelligence, has revolutionized how we approach problem-solving across various domains. Machine learning involves training computers to learn from data, identify patterns, and make decisions with minimal human intervention. This rapidly evolving field leverages statistical methods to enable machines to improve tasks through feedback and data, offering immense potential for innovation and efficiency.\n\n\nMachine Learning in Aerospace Engineering\nMachine learning (ML) is revolutionizing aerospace engineering with its current applications and promising future potential. There are several emerging areas within aerospace engineering where ML is expected to have significant impact. Some of the emerging trends are summarized below.\nSpace Exploration – ML in driving autonomy is particularly critical in space exploration. Space missions, often extending over vast distances and durations, necessitate systems that can operate with minimal human oversight. ML algorithms are instrumental in this context, enabling spacecraft to navigate, manage systems, and make crucial decisions autonomously. This autonomy is vital for deep space missions, where communication delays with Earth make real-time human control impractical. ML allows spacecraft to adapt to unforeseen circumstances, such as changes in trajectory, asteroid avoidance, or equipment malfunctions, ensuring mission success despite space environments’ challenging and unpredictable nature. Additionally, ML-driven autonomous systems are essential for analyzing and processing the vast amounts of scientific data collected during these missions, identifying critical findings that human researchers might miss. As ML technology advances, its role in enhancing the autonomy of space missions becomes increasingly significant, promising groundbreaking discoveries and more ambitious explorations beyond our current capabilities.\nAir Traffic Management – ML is increasingly pivotal in advancing aircraft flight management systems, and it plays a crucial role in the emerging sector of urban air mobility (UAM). In flight management, ML algorithms enhance route optimization, fuel efficiency, and in-flight safety by analyzing vast amounts of flight data and environmental variables in real-time. These systems can adaptively manage complex flight dynamics, navigate busy airspaces, and respond proactively to unforeseen circumstances like adverse weather or mechanical issues. The application of ML is particularly significant in UAM, a field focusing on developing small, automated aircraft for urban transportation. Here, ML is essential for ensuring safe, efficient, and autonomous operation in densely populated urban environments. It enables these aircraft to make intelligent navigation decisions, manage air traffic autonomously, and ensure passenger safety, all while adhering to stringent regulatory standards. As UAM evolves, ML will be at the forefront of this innovation, driving the development of smart, efficient, and safe urban air transport solutions.\nAerospace Digital Twins – ML is revolutionizing the concept of digital twins in aerospace engineering, offering a sophisticated approach to simulate, monitor, and analyze real-world aircraft and spacecraft systems. Digital twins, essentially virtual replicas of physical systems, utilize ML to process real-time data from various sensors and systems on the actual aircraft or spacecraft. This integration enables predictive analytics and real-time diagnostics, allowing engineers to anticipate potential failures, optimize maintenance schedules, and improve overall operational efficiency. ML algorithms also play a crucial role in simulating complex scenarios, from aerodynamic performance to structural integrity under various conditions, providing invaluable insights for design improvements and innovation. This synergy of ML and digital twin technology in aerospace not only enhances current operational performance but also significantly aids in the development of more advanced, reliable, and efficient future aerospace systems.\nComputational Fluid Dynamics – ML is increasingly being integrated into computational fluid dynamics (CFD) for aerospace applications, offering transformative capabilities in simulating and analyzing aerodynamic behaviors. In CFD, ML algorithms are used to refine and accelerate simulations, making them more efficient and accurate. These algorithms can learn from vast datasets of previous simulations and real-world experiments, enabling them to predict fluid flow patterns, pressure distributions, and thermal characteristics more quickly than traditional methods. This is particularly beneficial in complex scenarios like turbulent flows or supersonic speeds, where traditional CFD can be computationally intensive. ML also assists in optimizing designs for better aerodynamic performance, reducing drag, and enhancing fuel efficiency. By improving the predictive capabilities of CFD models, ML helps aerospace engineers in designing more efficient aircraft, spacecraft, and propulsion systems, ultimately leading to advancements in performance and energy efficiency.\nAircraft Design – ML is significantly impacting aircraft design, offering new avenues for innovation and efficiency. In aircraft design, ML algorithms analyze vast datasets to identify optimal design parameters that traditional methods might overlook. This includes refining aerodynamic shapes, optimizing weight distribution, and enhancing material selection for better performance and fuel efficiency. ML also plays a pivotal role in structural health monitoring, where it helps predict stress points and potential fatigue in aircraft components, leading to safer and more reliable designs. Furthermore, ML algorithms assist in noise reduction, both internally and externally, by analyzing and predicting acoustic patterns, thereby contributing to more environmentally friendly and passenger-comfort-focused designs. The integration of ML in aircraft design not only accelerates the design process but also enables the development of more advanced, efficient, and sustainable aircraft, pushing the boundaries of what is currently achievable in aerospace engineering.\nAerospace Material Science – The integration of ML in material science is revolutionizing aerospace engineering, significantly enhancing the development and optimization of aerospace materials. ML algorithms are adept at sifting through and analyzing large datasets, uncovering patterns and relationships in material properties that might be missed by conventional methods. This capability is crucial for discovering new materials with desired properties such as lightweight, high strength, and thermal resistance, essential for aerospace applications. ML also accelerates the process of material testing and validation, predicting how new materials will perform under various stressors and environmental conditions. This predictive power enables more efficient design and testing cycles, reducing the time and cost associated with material development. In essence, ML is not just optimizing existing materials for aerospace use but also paving the way for the discovery and creation of novel materials, potentially leading to lighter, stronger, and more efficient aerospace components and structures.\nIn conclusion, ML is not just an auxiliary tool but a transformative force in aerospace engineering, enhancing current practices and promising to redefine the future of air and space travel. As ML technology continues to evolve, its role in aerospace engineering is set to become increasingly pivotal, pushing the boundaries of what’s possible in this field.\n\n\nWhat Machine Learning Is Not\nIt is essential to clarify a common misconception: machines don’t actually learn in the way we traditionally understand learning. What we call a “learning machine” is a system that discovers a mathematical formula. When applied to a set of inputs known as “training data,” this formula yields the expected outputs. Furthermore, it can produce accurate outputs for various inputs, provided these inputs share a statistical distribution similar to the training data.\nHowever, this process differs significantly from learning in the animal world. For instance, if you learn to play a video game by looking directly at the screen, a slight rotation of the screen won’t drastically affect your ability to play. In contrast, a machine learning algorithm trained under specific conditions may fail when those conditions change slightly, such as in the case of screen rotation, unless it’s specifically trained to recognize such changes.\nWhile working at IBM, the term “machine learning” was coined in 1959 by Arthur Samuel, an American pioneer in computer gaming and artificial intelligence. Much like IBM’s later promotion of “cognitive computing,” this naming was part of a marketing strategy to attract clients and talent. Thus, while “artificial intelligence” doesn’t imply human-like intelligence, “machine learning” does not equate to the learning process seen in animals. Instead, it refers to creating machines that can perform various tasks without being explicitly programmed for each task. The word “learning” here is more analogous to animal learning rather than a literal interpretation.\nIn contemporary media, machine learning is often shrouded in misconceptions, frequently portrayed as a near-magical or ominously omnipotent technology. Contrary to these dramatizations, machine learning is fundamentally grounded in sophisticated yet comprehensible mathematics. It involves algorithms that analyze and “learn” from data to optimize specific tasks. While it’s highly effective at tasks like pattern recognition and predictive analysis, machine learning isn’t a cure-all. Its “intelligence” is derived from the performance of algorithms tailored and improved over time, informed by data and the laws of physics. It cannot understand moral or ethical issues or grasp abstract concepts beyond its programming. Recognizing these limitations is critical to appreciating the real capabilities and scope of machine learning, which remains a potent yet human-dependent tool.\n\n\nWords of Caution\nIn the realm of engineering, particularly in disciplines as exacting as aerospace engineering, the concept of correctness carries significant weight. Engineering solutions are expected to adhere to stringent standards of accuracy and reliability, a criterion that current machine-learning applications may not always meet. The outputs of machine learning algorithms are often more suited to contexts where accuracy is subjective and less critical. This divergence becomes especially relevant when integrating machine learning into engineering tasks, where the margin for error is minimal. In aerospace engineering, where the stakes are exceptionally high, carefully assessing errors and uncertainties in machine learning outputs becomes imperative. Ensuring the safety, efficiency, and reliability of aerospace systems demands a meticulous evaluation of machine learning applications, underlining the need for precise error analysis and validation against the uncompromising standards of engineering accuracy.\n\n\nAn Optimization Mind Set\nThis book presents a practical and insightful approach, positioning machine learning as a key optimization tool across various domains, especially critical in aerospace engineering. It emphasizes machine learning’s role in discovering innovative solutions to complex challenges within this field. By leveraging the power of machine learning algorithms to process and analyze intricate, multi-faceted data, we can unearth previously hidden patterns and connections. This strategy isn’t about replacing human creativity; rather, it’s about enhancing it with machine-based computational efficiency. By employing these algorithms, we can more effectively explore vast solution spaces within an optimization framework, thereby accelerating the pace of innovation. This viewpoint underscores the significance of machine learning as an indispensable resource in driving forward advanced designs, elevating safety and operational efficiency, and pioneering new frontiers in aerospace engineering. The book encourages the development of an optimization mindset, crucial for tackling the complex problems of aerospace engineering with machine learning, blending computational power with human expertise to push the boundaries of what’s possible.\n\n\nTheory and Data Driven Aerospace Machine Learning\nAn ideal strategy for aerospace machine learning should effectively integrate physics principles with machine learning concepts, utilizing mathematical equations to compensate for any data deficiencies. Rather than viewing physical laws as mere restrictions, they should be considered as crucial insights that steer the machine learning process. The efficiency of problem solving can be greatly improved by adeptly combining empirical data with these core physical principles. The result of merging data with physics is the generation of solutions that are as firmly based on empirical evidence as they are on the foundational principles of physics. Adopting such an all-encompassing approach ensures that machine learning algorithms are not only responsive to data but also precisely adjusted in accordance with the fundamental laws that drive aerospace phenomena. This synergy of data and physics carves out a route towards the development of innovative, dependable, and accurately fine-tuned answers to the intricate problems faced in aerospace engineering.\n\n\nScope of the book\nThis book aims to complement the newly introduced machine learning course in the Aerospace Engineering Department at Texas A&M University. Recognizing the extensive range of machine learning’s applicability in aerospace, the book offers a detailed examination of select, complex issues within this domain, tailored for machine learning solutions. It is crafted with undergraduate seniors and graduate students in mind, aiming to deepen their comprehension of the interplay and enhancement of aerospace technologies by machine learning. The book is also structured to be approachable for professionals, given they have a foundational understanding of mathematical concepts and computer programming. This book strives to connect academic theory with real-world industry practices by integrating theoretical knowledge with practical case studies. It is envisioned as a critical resource for those keen on exploring the cutting-edge applications of machine learning in aerospace, whether for academic advancement or practical application. As the course evolves, we anticipate expanding the topics covered in this book and hope to achieve these lofty goals.\nRaktim Bhattacharya Professor Aerospace Engineering, Texas A&M University. College Station, TX, 77843-3141, USA."
  }
]