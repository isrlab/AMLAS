<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Applied Machine Learning for Aerospace Systems - 12&nbsp; Unsupervised Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./dimensionality_reduction.html" rel="next">
<link href="./supervised_learning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script>
    MathJax = {
      tex: {
        tags: 'ams'  // should be 'ams', 'none', or 'all'
      }
    };
  </script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./algorithms.html">Machine Learning Algorithms</a></li><li class="breadcrumb-item"><a href="./unsupervised_learning.html"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Applied Machine Learning for Aerospace Systems</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction to Machine Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./math.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Foundational Mathematics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear_algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Algebra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./functions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Functions &amp; Function Spaces</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./random_processes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Random Processes and Sequences</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bayesian_inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Bayesian Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./monte_carlo_methods.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Monte Carlo Methods*</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Optimization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./function_approximation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Function Approximation</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./algorithms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Machine Learning Algorithms</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data_pre_processing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Data Pre-Processing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./unsupervised_learning.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dimensionality_reduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Dimensionality Reduction</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./aerospace_applications.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Aerospace Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./application1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Aerospace Application 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./application2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Aerospace Application 2</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Summary</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="7">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#machine-learning-algorithms" id="toc-machine-learning-algorithms" class="nav-link active" data-scroll-target="#machine-learning-algorithms"><span class="header-section-number">13</span> Machine Learning Algorithms</a></li>
  <li><a href="#aerospace-application-1" id="toc-aerospace-application-1" class="nav-link" data-scroll-target="#aerospace-application-1"><span class="header-section-number">14</span> Aerospace Application 1</a>
  <ul class="collapse">
  <li><a href="#surrogate-modeling" id="toc-surrogate-modeling" class="nav-link" data-scroll-target="#surrogate-modeling"><span class="header-section-number">14.1</span> Surrogate Modeling</a></li>
  </ul></li>
  <li><a href="#aerospace-application-2" id="toc-aerospace-application-2" class="nav-link" data-scroll-target="#aerospace-application-2"><span class="header-section-number">15</span> Aerospace Application 2</a>
  <ul class="collapse">
  <li><a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference"><span class="header-section-number">15.0.1</span> Bayesian Inference</a></li>
  </ul></li>
  <li><a href="#data-pre-processing" id="toc-data-pre-processing" class="nav-link" data-scroll-target="#data-pre-processing"><span class="header-section-number">16</span> Data Pre-Processing</a>
  <ul class="collapse">
  <li><a href="#data-cleaning" id="toc-data-cleaning" class="nav-link" data-scroll-target="#data-cleaning"><span class="header-section-number">16.1</span> Data Cleaning</a>
  <ul class="collapse">
  <li><a href="#handling-missing-values" id="toc-handling-missing-values" class="nav-link" data-scroll-target="#handling-missing-values"><span class="header-section-number">16.1.1</span> Handling Missing Values</a></li>
  <li><a href="#smoothing-noisy-data" id="toc-smoothing-noisy-data" class="nav-link" data-scroll-target="#smoothing-noisy-data"><span class="header-section-number">16.1.2</span> Smoothing Noisy Data</a></li>
  <li><a href="#identifying-and-removing-outliers" id="toc-identifying-and-removing-outliers" class="nav-link" data-scroll-target="#identifying-and-removing-outliers"><span class="header-section-number">16.1.3</span> Identifying and Removing Outliers</a></li>
  </ul></li>
  <li><a href="#survivorship-bias" id="toc-survivorship-bias" class="nav-link" data-scroll-target="#survivorship-bias"><span class="header-section-number">16.2</span> Survivorship Bias</a></li>
  <li><a href="#data-transformation" id="toc-data-transformation" class="nav-link" data-scroll-target="#data-transformation"><span class="header-section-number">16.3</span> Data Transformation</a></li>
  <li><a href="#data-reduction" id="toc-data-reduction" class="nav-link" data-scroll-target="#data-reduction"><span class="header-section-number">16.4</span> Data Reduction</a></li>
  <li><a href="#data-encoding" id="toc-data-encoding" class="nav-link" data-scroll-target="#data-encoding"><span class="header-section-number">16.5</span> Data Encoding</a></li>
  <li><a href="#handling-imbalanced-data" id="toc-handling-imbalanced-data" class="nav-link" data-scroll-target="#handling-imbalanced-data"><span class="header-section-number">16.6</span> Handling Imbalanced Data</a></li>
  <li><a href="#feature-engineering" id="toc-feature-engineering" class="nav-link" data-scroll-target="#feature-engineering"><span class="header-section-number">16.7</span> Feature Engineering</a></li>
  <li><a href="#time-series-specific-techniques" id="toc-time-series-specific-techniques" class="nav-link" data-scroll-target="#time-series-specific-techniques"><span class="header-section-number">16.8</span> Time Series Specific Techniques</a></li>
  <li><a href="#data-integration" id="toc-data-integration" class="nav-link" data-scroll-target="#data-integration"><span class="header-section-number">16.9</span> Data Integration</a></li>
  </ul></li>
  <li><a href="#deep-learning" id="toc-deep-learning" class="nav-link" data-scroll-target="#deep-learning"><span class="header-section-number">17</span> Deep Learning</a></li>
  <li><a href="#dimensionality-reduction" id="toc-dimensionality-reduction" class="nav-link" data-scroll-target="#dimensionality-reduction"><span class="header-section-number">18</span> Dimensionality Reduction</a>
  <ul class="collapse">
  <li><a href="#pca" id="toc-pca" class="nav-link" data-scroll-target="#pca"><span class="header-section-number">18.1</span> PCA</a>
  <ul class="collapse">
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples">Examples</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#function-approximation" id="toc-function-approximation" class="nav-link" data-scroll-target="#function-approximation"><span class="header-section-number">19</span> Function Approximation</a>
  <ul class="collapse">
  <li><a href="#linear-spaces-with-basis-elements" id="toc-linear-spaces-with-basis-elements" class="nav-link" data-scroll-target="#linear-spaces-with-basis-elements"><span class="header-section-number">19.1</span> Linear Spaces with Basis Elements</a></li>
  <li><a href="#eulidean-space" id="toc-eulidean-space" class="nav-link" data-scroll-target="#eulidean-space"><span class="header-section-number">19.2</span> Eulidean Space</a>
  <ul class="collapse">
  <li><a href="#minimal-basis" id="toc-minimal-basis" class="nav-link" data-scroll-target="#minimal-basis"><span class="header-section-number">19.2.1</span> Minimal Basis</a></li>
  <li><a href="#matrix-spaces" id="toc-matrix-spaces" class="nav-link" data-scroll-target="#matrix-spaces"><span class="header-section-number">19.2.2</span> Matrix Spaces</a></li>
  </ul></li>
  <li><a href="#function-space" id="toc-function-space" class="nav-link" data-scroll-target="#function-space"><span class="header-section-number">19.3</span> Function Space</a>
  <ul class="collapse">
  <li><a href="#basis-functions" id="toc-basis-functions" class="nav-link" data-scroll-target="#basis-functions"><span class="header-section-number">19.3.1</span> Basis Functions</a></li>
  <li><a href="#a-projection-perspective-1" id="toc-a-projection-perspective-1" class="nav-link" data-scroll-target="#a-projection-perspective-1"><span class="header-section-number">19.3.2</span> A Projection Perspective</a></li>
  <li><a href="#function-approximation-over-discrete-data" id="toc-function-approximation-over-discrete-data" class="nav-link" data-scroll-target="#function-approximation-over-discrete-data"><span class="header-section-number">19.3.3</span> Function Approximation Over Discrete Data</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#generate-some-scattered-data" id="toc-generate-some-scattered-data" class="nav-link" data-scroll-target="#generate-some-scattered-data"><span class="header-section-number">20</span> Generate some scattered data</a></li>
  <li><a href="#function-to-approximate-the-data-using-legendre-polynomials-with-linear-algebra" id="toc-function-to-approximate-the-data-using-legendre-polynomials-with-linear-algebra" class="nav-link" data-scroll-target="#function-to-approximate-the-data-using-legendre-polynomials-with-linear-algebra"><span class="header-section-number">21</span> Function to approximate the data using Legendre polynomials with linear algebra</a></li>
  <li><a href="#approximating-the-data" id="toc-approximating-the-data" class="nav-link" data-scroll-target="#approximating-the-data"><span class="header-section-number">22</span> Approximating the data</a></li>
  <li><a href="#plotting-the-original-data-and-the-approximation" id="toc-plotting-the-original-data-and-the-approximation" class="nav-link" data-scroll-target="#plotting-the-original-data-and-the-approximation"><span class="header-section-number">23</span> Plotting the original data and the approximation</a>
  <ul class="collapse">
  <li><a href="#local-vs-global-basis-functions" id="toc-local-vs-global-basis-functions" class="nav-link" data-scroll-target="#local-vs-global-basis-functions"><span class="header-section-number">23.0.1</span> Local vs Global Basis Functions</a></li>
  </ul></li>
  <li><a href="#sample-data-points" id="toc-sample-data-points" class="nav-link" data-scroll-target="#sample-data-points"><span class="header-section-number">24</span> Sample data points</a></li>
  <li><a href="#create-a-cubic-spline-interpolation-of-the-data" id="toc-create-a-cubic-spline-interpolation-of-the-data" class="nav-link" data-scroll-target="#create-a-cubic-spline-interpolation-of-the-data"><span class="header-section-number">25</span> Create a cubic spline interpolation of the data</a></li>
  <li><a href="#generate-more-points-to-evaluate-the-spline" id="toc-generate-more-points-to-evaluate-the-spline" class="nav-link" data-scroll-target="#generate-more-points-to-evaluate-the-spline"><span class="header-section-number">26</span> Generate more points to evaluate the spline</a></li>
  <li><a href="#plot-the-original-data-points-and-the-interpolated-values" id="toc-plot-the-original-data-points-and-the-interpolated-values" class="nav-link" data-scroll-target="#plot-the-original-data-points-and-the-interpolated-values"><span class="header-section-number">27</span> Plot the original data points and the interpolated values</a></li>
  <li><a href="#sample-data-points-1" id="toc-sample-data-points-1" class="nav-link" data-scroll-target="#sample-data-points-1"><span class="header-section-number">28</span> Sample data points</a></li>
  <li><a href="#create-a-b-spline-representation-of-the-data" id="toc-create-a-b-spline-representation-of-the-data" class="nav-link" data-scroll-target="#create-a-b-spline-representation-of-the-data"><span class="header-section-number">29</span> Create a B-Spline representation of the data</a></li>
  <li><a href="#generate-more-points-to-evaluate-the-spline-1" id="toc-generate-more-points-to-evaluate-the-spline-1" class="nav-link" data-scroll-target="#generate-more-points-to-evaluate-the-spline-1"><span class="header-section-number">30</span> Generate more points to evaluate the spline</a></li>
  <li><a href="#plot-the-original-data-points-and-the-b-spline-interpolation" id="toc-plot-the-original-data-points-and-the-b-spline-interpolation" class="nav-link" data-scroll-target="#plot-the-original-data-points-and-the-b-spline-interpolation"><span class="header-section-number">31</span> Plot the original data points and the B-Spline interpolation</a></li>
  <li><a href="#define-the-function-to-approximate" id="toc-define-the-function-to-approximate" class="nav-link" data-scroll-target="#define-the-function-to-approximate"><span class="header-section-number">32</span> Define the function to approximate</a></li>
  <li><a href="#generate-sample-data" id="toc-generate-sample-data" class="nav-link" data-scroll-target="#generate-sample-data"><span class="header-section-number">33</span> Generate sample data</a></li>
  <li><a href="#choose-a-wavelet-type-and-level-of-decomposition" id="toc-choose-a-wavelet-type-and-level-of-decomposition" class="nav-link" data-scroll-target="#choose-a-wavelet-type-and-level-of-decomposition"><span class="header-section-number">34</span> Choose a wavelet type and level of decomposition</a></li>
  <li><a href="#compute-the-wavelet-coefficients" id="toc-compute-the-wavelet-coefficients" class="nav-link" data-scroll-target="#compute-the-wavelet-coefficients"><span class="header-section-number">35</span> Compute the wavelet coefficients</a></li>
  <li><a href="#reconstruct-the-signal-from-the-coefficients" id="toc-reconstruct-the-signal-from-the-coefficients" class="nav-link" data-scroll-target="#reconstruct-the-signal-from-the-coefficients"><span class="header-section-number">36</span> Reconstruct the signal from the coefficients</a></li>
  <li><a href="#plot-the-original-and-approximated-function" id="toc-plot-the-original-and-approximated-function" class="nav-link" data-scroll-target="#plot-the-original-and-approximated-function"><span class="header-section-number">37</span> Plot the original and approximated function</a></li>
  <li><a href="#define-the-2d-function-to-approximate" id="toc-define-the-2d-function-to-approximate" class="nav-link" data-scroll-target="#define-the-2d-function-to-approximate"><span class="header-section-number">38</span> Define the 2D function to approximate</a></li>
  <li><a href="#create-scattered-data-points" id="toc-create-scattered-data-points" class="nav-link" data-scroll-target="#create-scattered-data-points"><span class="header-section-number">39</span> Create scattered data points</a></li>
  <li><a href="#create-the-radial-basis-function-interpolator" id="toc-create-the-radial-basis-function-interpolator" class="nav-link" data-scroll-target="#create-the-radial-basis-function-interpolator"><span class="header-section-number">40</span> Create the Radial Basis Function interpolator</a></li>
  <li><a href="#create-a-grid-to-evaluate-the-interpolator" id="toc-create-a-grid-to-evaluate-the-interpolator" class="nav-link" data-scroll-target="#create-a-grid-to-evaluate-the-interpolator"><span class="header-section-number">41</span> Create a grid to evaluate the interpolator</a></li>
  <li><a href="#plot-the-results" id="toc-plot-the-results" class="nav-link" data-scroll-target="#plot-the-results"><span class="header-section-number">42</span> Plot the results</a></li>
  <li><a href="#original-function" id="toc-original-function" class="nav-link" data-scroll-target="#original-function"><span class="header-section-number">43</span> Original function</a></li>
  <li><a href="#rbf-interpolation" id="toc-rbf-interpolation" class="nav-link" data-scroll-target="#rbf-interpolation"><span class="header-section-number">44</span> RBF Interpolation</a>
  <ul class="collapse">
  <li><a href="#function-spaces" id="toc-function-spaces" class="nav-link" data-scroll-target="#function-spaces"><span class="header-section-number">44.1</span> Function Spaces</a>
  <ul class="collapse">
  <li><a href="#vector-spaces" id="toc-vector-spaces" class="nav-link" data-scroll-target="#vector-spaces"><span class="header-section-number">44.1.1</span> Vector Spaces</a></li>
  <li><a href="#hilbert-spaces" id="toc-hilbert-spaces" class="nav-link" data-scroll-target="#hilbert-spaces"><span class="header-section-number">44.1.2</span> Hilbert Spaces</a></li>
  <li><a href="#banach-spaces" id="toc-banach-spaces" class="nav-link" data-scroll-target="#banach-spaces"><span class="header-section-number">44.1.3</span> Banach Spaces</a></li>
  <li><a href="#sobolev-spaces" id="toc-sobolev-spaces" class="nav-link" data-scroll-target="#sobolev-spaces"><span class="header-section-number">44.1.4</span> Sobolev Spaces</a></li>
  <li><a href="#measure-spaces" id="toc-measure-spaces" class="nav-link" data-scroll-target="#measure-spaces"><span class="header-section-number">44.1.5</span> Measure Spaces</a></li>
  <li><a href="#lebesgue-spaces" id="toc-lebesgue-spaces" class="nav-link" data-scroll-target="#lebesgue-spaces"><span class="header-section-number">44.1.6</span> Lebesgue Spaces</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#preface" id="toc-preface" class="nav-link" data-scroll-target="#preface">Preface</a>
  <ul class="collapse">
  <li><a href="#what-is-machine-learning" id="toc-what-is-machine-learning" class="nav-link" data-scroll-target="#what-is-machine-learning"><span class="header-section-number">44.1.7</span> What is Machine Learning?</a></li>
  <li><a href="#machine-learning-in-aerospace-engineering" id="toc-machine-learning-in-aerospace-engineering" class="nav-link" data-scroll-target="#machine-learning-in-aerospace-engineering"><span class="header-section-number">44.1.8</span> Machine Learning in Aerospace Engineering</a></li>
  <li><a href="#what-machine-learning-is-not" id="toc-what-machine-learning-is-not" class="nav-link" data-scroll-target="#what-machine-learning-is-not"><span class="header-section-number">44.1.9</span> What Machine Learning Is Not</a></li>
  <li><a href="#words-of-caution" id="toc-words-of-caution" class="nav-link" data-scroll-target="#words-of-caution"><span class="header-section-number">44.1.10</span> Words of Caution</a></li>
  <li><a href="#an-optimization-mind-set" id="toc-an-optimization-mind-set" class="nav-link" data-scroll-target="#an-optimization-mind-set"><span class="header-section-number">44.1.11</span> An Optimization Mind Set</a></li>
  <li><a href="#theory-and-data-driven-aerospace-machine-learning" id="toc-theory-and-data-driven-aerospace-machine-learning" class="nav-link" data-scroll-target="#theory-and-data-driven-aerospace-machine-learning"><span class="header-section-number">44.1.12</span> Theory and Data Driven Aerospace Machine Learning</a></li>
  <li><a href="#scope-of-the-book" id="toc-scope-of-the-book" class="nav-link" data-scroll-target="#scope-of-the-book"><span class="header-section-number">44.1.13</span> Scope of the book</a></li>
  </ul></li>
  <li><a href="#introduction-to-machine-learning" id="toc-introduction-to-machine-learning" class="nav-link" data-scroll-target="#introduction-to-machine-learning"><span class="header-section-number">45</span> Introduction to Machine Learning</a>
  <ul class="collapse">
  <li><a href="#types-of-learning" id="toc-types-of-learning" class="nav-link" data-scroll-target="#types-of-learning"><span class="header-section-number">45.1</span> Types of Learning</a></li>
  <li><a href="#how-does-a-machine-learn" id="toc-how-does-a-machine-learn" class="nav-link" data-scroll-target="#how-does-a-machine-learn"><span class="header-section-number">45.2</span> How does a Machine Learn?</a>
  <ul class="collapse">
  <li><a href="#a-function-approximation-perspective" id="toc-a-function-approximation-perspective" class="nav-link" data-scroll-target="#a-function-approximation-perspective"><span class="header-section-number">45.2.1</span> A Function Approximation Perspective</a></li>
  <li><a href="#learning-as-an-optimization-process" id="toc-learning-as-an-optimization-process" class="nav-link" data-scroll-target="#learning-as-an-optimization-process"><span class="header-section-number">45.2.2</span> Learning as an Optimization Process</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">45.3</span> Summary</a></li>
  </ul></li>
  <li><a href="#linear-algebra" id="toc-linear-algebra" class="nav-link" data-scroll-target="#linear-algebra"><span class="header-section-number">46</span> Linear Algebra</a>
  <ul class="collapse">
  <li><a href="#vectors" id="toc-vectors" class="nav-link" data-scroll-target="#vectors"><span class="header-section-number">46.1</span> Vectors</a>
  <ul class="collapse">
  <li><a href="#dimension" id="toc-dimension" class="nav-link" data-scroll-target="#dimension"><span class="header-section-number">46.1.1</span> Dimension</a></li>
  <li><a href="#transpose" id="toc-transpose" class="nav-link" data-scroll-target="#transpose"><span class="header-section-number">46.1.2</span> Transpose</a></li>
  <li><a href="#operations" id="toc-operations" class="nav-link" data-scroll-target="#operations"><span class="header-section-number">46.1.3</span> Operations</a></li>
  <li><a href="#length-of-a-vector" id="toc-length-of-a-vector" class="nav-link" data-scroll-target="#length-of-a-vector"><span class="header-section-number">46.1.4</span> Length of a Vector</a></li>
  <li><a href="#vector-norms" id="toc-vector-norms" class="nav-link" data-scroll-target="#vector-norms"><span class="header-section-number">46.1.5</span> Vector Norms</a></li>
  </ul></li>
  <li><a href="#matrices" id="toc-matrices" class="nav-link" data-scroll-target="#matrices"><span class="header-section-number">46.2</span> Matrices</a>
  <ul class="collapse">
  <li><a href="#dimension-1" id="toc-dimension-1" class="nav-link" data-scroll-target="#dimension-1"><span class="header-section-number">46.2.1</span> Dimension</a></li>
  <li><a href="#transpose-1" id="toc-transpose-1" class="nav-link" data-scroll-target="#transpose-1"><span class="header-section-number">46.2.2</span> Transpose</a></li>
  <li><a href="#matrix-construction-from-vectors" id="toc-matrix-construction-from-vectors" class="nav-link" data-scroll-target="#matrix-construction-from-vectors"><span class="header-section-number">46.2.3</span> Matrix Construction from Vectors</a></li>
  <li><a href="#rank-of-a-matrix" id="toc-rank-of-a-matrix" class="nav-link" data-scroll-target="#rank-of-a-matrix"><span class="header-section-number">46.2.4</span> Rank of a Matrix</a></li>
  <li><a href="#eigen-values-and-eigen-vectors-of-a-matrix" id="toc-eigen-values-and-eigen-vectors-of-a-matrix" class="nav-link" data-scroll-target="#eigen-values-and-eigen-vectors-of-a-matrix"><span class="header-section-number">46.2.5</span> Eigen-Values and Eigen-Vectors of a Matrix</a></li>
  <li><a href="#operations-1" id="toc-operations-1" class="nav-link" data-scroll-target="#operations-1"><span class="header-section-number">46.2.6</span> Operations</a></li>
  <li><a href="#symmetric-matrices" id="toc-symmetric-matrices" class="nav-link" data-scroll-target="#symmetric-matrices"><span class="header-section-number">46.2.7</span> Symmetric Matrices</a></li>
  <li><a href="#skew-symmetric-matrices" id="toc-skew-symmetric-matrices" class="nav-link" data-scroll-target="#skew-symmetric-matrices"><span class="header-section-number">46.2.8</span> Skew Symmetric Matrices</a></li>
  <li><a href="#positive-semi-definite-matrices" id="toc-positive-semi-definite-matrices" class="nav-link" data-scroll-target="#positive-semi-definite-matrices"><span class="header-section-number">46.2.9</span> Positive (Semi) Definite Matrices</a></li>
  <li><a href="#identity-matrix" id="toc-identity-matrix" class="nav-link" data-scroll-target="#identity-matrix"><span class="header-section-number">46.2.10</span> Identity Matrix</a></li>
  <li><a href="#determininant-of-a-matrix" id="toc-determininant-of-a-matrix" class="nav-link" data-scroll-target="#determininant-of-a-matrix"><span class="header-section-number">46.2.11</span> Determininant of a Matrix</a></li>
  <li><a href="#matrix-inverse" id="toc-matrix-inverse" class="nav-link" data-scroll-target="#matrix-inverse"><span class="header-section-number">46.2.12</span> Matrix Inverse</a></li>
  <li><a href="#moore-penrose-inverse" id="toc-moore-penrose-inverse" class="nav-link" data-scroll-target="#moore-penrose-inverse"><span class="header-section-number">46.2.13</span> Moore-Penrose Inverse</a></li>
  <li><a href="#orthonormal-matrices" id="toc-orthonormal-matrices" class="nav-link" data-scroll-target="#orthonormal-matrices"><span class="header-section-number">46.2.14</span> Orthonormal Matrices</a></li>
  <li><a href="#kronecker-product" id="toc-kronecker-product" class="nav-link" data-scroll-target="#kronecker-product"><span class="header-section-number">46.2.15</span> Kronecker Product</a></li>
  <li><a href="#trace-of-a-square-matrix" id="toc-trace-of-a-square-matrix" class="nav-link" data-scroll-target="#trace-of-a-square-matrix"><span class="header-section-number">46.2.16</span> Trace of a Square Matrix</a></li>
  <li><a href="#matrix-inner-product" id="toc-matrix-inner-product" class="nav-link" data-scroll-target="#matrix-inner-product"><span class="header-section-number">46.2.17</span> Matrix Inner Product</a></li>
  <li><a href="#matrix-norms" id="toc-matrix-norms" class="nav-link" data-scroll-target="#matrix-norms"><span class="header-section-number">46.2.18</span> Matrix Norms</a></li>
  <li><a href="#singular-values-of-matrix" id="toc-singular-values-of-matrix" class="nav-link" data-scroll-target="#singular-values-of-matrix"><span class="header-section-number">46.2.19</span> Singular Values of Matrix</a></li>
  <li><a href="#matrix-decompositions" id="toc-matrix-decompositions" class="nav-link" data-scroll-target="#matrix-decompositions"><span class="header-section-number">46.2.20</span> Matrix Decompositions</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#foundational-mathematics" id="toc-foundational-mathematics" class="nav-link" data-scroll-target="#foundational-mathematics"><span class="header-section-number">47</span> Foundational Mathematics</a>
  <ul class="collapse">
  <li><a href="#monte-carlo-methods" id="toc-monte-carlo-methods" class="nav-link" data-scroll-target="#monte-carlo-methods"><span class="header-section-number">47.0.1</span> Monte Carlo Methods*</a></li>
  </ul></li>
  <li><a href="#optimization" id="toc-optimization" class="nav-link" data-scroll-target="#optimization"><span class="header-section-number">48</span> Optimization</a>
  <ul class="collapse">
  <li><a href="#convex-sets" id="toc-convex-sets" class="nav-link" data-scroll-target="#convex-sets"><span class="header-section-number">48.1</span> Convex Sets</a></li>
  <li><a href="#convex-functions" id="toc-convex-functions" class="nav-link" data-scroll-target="#convex-functions"><span class="header-section-number">48.2</span> Convex Functions</a></li>
  <li><a href="#some-useful-inequalities" id="toc-some-useful-inequalities" class="nav-link" data-scroll-target="#some-useful-inequalities"><span class="header-section-number">48.3</span> Some Useful Inequalities</a></li>
  </ul></li>
  <li><a href="#random-processes-and-sequences" id="toc-random-processes-and-sequences" class="nav-link" data-scroll-target="#random-processes-and-sequences"><span class="header-section-number">49</span> Random Processes and Sequences</a>
  <ul class="collapse">
  <li><a href="#discrete-time-processes" id="toc-discrete-time-processes" class="nav-link" data-scroll-target="#discrete-time-processes"><span class="header-section-number">49.1</span> Discrete-Time Processes</a>
  <ul class="collapse">
  <li><a href="#bernoulli-process" id="toc-bernoulli-process" class="nav-link" data-scroll-target="#bernoulli-process"><span class="header-section-number">49.1.1</span> Bernoulli Process</a></li>
  <li><a href="#random-walk" id="toc-random-walk" class="nav-link" data-scroll-target="#random-walk"><span class="header-section-number">49.1.2</span> Random Walk</a></li>
  <li><a href="#markov-chain" id="toc-markov-chain" class="nav-link" data-scroll-target="#markov-chain"><span class="header-section-number">49.1.3</span> Markov Chain</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#define-the-transition-matrix" id="toc-define-the-transition-matrix" class="nav-link" data-scroll-target="#define-the-transition-matrix"><span class="header-section-number">50</span> Define the transition matrix</a></li>
  <li><a href="#compute-left-eigenvectors-and-eigenvalues" id="toc-compute-left-eigenvectors-and-eigenvalues" class="nav-link" data-scroll-target="#compute-left-eigenvectors-and-eigenvalues"><span class="header-section-number">51</span> Compute left eigenvectors and eigenvalues</a></li>
  <li><a href="#find-the-left-eigenvector-corresponding-to-the-eigenvalue-1-steady-state" id="toc-find-the-left-eigenvector-corresponding-to-the-eigenvalue-1-steady-state" class="nav-link" data-scroll-target="#find-the-left-eigenvector-corresponding-to-the-eigenvalue-1-steady-state"><span class="header-section-number">52</span> Find the left eigenvector corresponding to the eigenvalue 1 (steady state)</a></li>
  <li><a href="#normalize-the-steady-state-vector" id="toc-normalize-the-steady-state-vector" class="nav-link" data-scroll-target="#normalize-the-steady-state-vector"><span class="header-section-number">53</span> Normalize the steady state vector</a>
  <ul class="collapse">
  <li><a href="#poisson-process" id="toc-poisson-process" class="nav-link" data-scroll-target="#poisson-process"><span class="header-section-number">53.0.1</span> Poisson Process</a></li>
  </ul></li>
  <li><a href="#parameters" id="toc-parameters" class="nav-link" data-scroll-target="#parameters"><span class="header-section-number">54</span> Parameters</a></li>
  <li><a href="#simulate-the-poisson-process" id="toc-simulate-the-poisson-process" class="nav-link" data-scroll-target="#simulate-the-poisson-process"><span class="header-section-number">55</span> Simulate the Poisson process</a></li>
  <li><a href="#plotting" id="toc-plotting" class="nav-link" data-scroll-target="#plotting"><span class="header-section-number">56</span> Plotting</a>
  <ul class="collapse">
  <li><a href="#continuous-time-random-walks" id="toc-continuous-time-random-walks" class="nav-link" data-scroll-target="#continuous-time-random-walks"><span class="header-section-number">56.0.1</span> Continuous Time Random Walks</a></li>
  <li><a href="#some-important-definitions" id="toc-some-important-definitions" class="nav-link" data-scroll-target="#some-important-definitions"><span class="header-section-number">56.1</span> Some Important Definitions</a>
  <ul class="collapse">
  <li><a href="#ergodic-process" id="toc-ergodic-process" class="nav-link" data-scroll-target="#ergodic-process"><span class="header-section-number">56.1.1</span> Ergodic Process</a></li>
  <li><a href="#markov-processes" id="toc-markov-processes" class="nav-link" data-scroll-target="#markov-processes"><span class="header-section-number">56.1.2</span> Markov Processes</a></li>
  <li><a href="#stationary-non-stationary-processes" id="toc-stationary-non-stationary-processes" class="nav-link" data-scroll-target="#stationary-non-stationary-processes"><span class="header-section-number">56.1.3</span> Stationary &amp; Non Stationary Processes</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#random-variables" id="toc-random-variables" class="nav-link" data-scroll-target="#random-variables"><span class="header-section-number">57</span> Random Variables</a>
  <ul class="collapse">
  <li><a href="#definitions-and-probability-axioms" id="toc-definitions-and-probability-axioms" class="nav-link" data-scroll-target="#definitions-and-probability-axioms"><span class="header-section-number">57.1</span> Definitions and Probability Axioms</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability"><span class="header-section-number">57.2</span> Conditional Probability</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">57.3</span> Bayes’ Theorem</a></li>
  <li><a href="#discrete-random-variables" id="toc-discrete-random-variables" class="nav-link" data-scroll-target="#discrete-random-variables"><span class="header-section-number">57.4</span> Discrete Random Variables</a>
  <ul class="collapse">
  <li><a href="#continuous-random-variables" id="toc-continuous-random-variables" class="nav-link" data-scroll-target="#continuous-random-variables"><span class="header-section-number">57.4.1</span> Continuous Random Variables</a></li>
  </ul></li>
  <li><a href="#cumulative-distribution-functions-cdfs" id="toc-cumulative-distribution-functions-cdfs" class="nav-link" data-scroll-target="#cumulative-distribution-functions-cdfs"><span class="header-section-number">57.5</span> Cumulative Distribution Functions (CDFs)</a></li>
  <li><a href="#expected-value" id="toc-expected-value" class="nav-link" data-scroll-target="#expected-value"><span class="header-section-number">57.6</span> Expected Value</a></li>
  <li><a href="#variance" id="toc-variance" class="nav-link" data-scroll-target="#variance"><span class="header-section-number">57.7</span> Variance</a></li>
  <li><a href="#law-of-large-numbers" id="toc-law-of-large-numbers" class="nav-link" data-scroll-target="#law-of-large-numbers"><span class="header-section-number">57.8</span> Law of Large Numbers</a></li>
  <li><a href="#central-limit-theorem" id="toc-central-limit-theorem" class="nav-link" data-scroll-target="#central-limit-theorem"><span class="header-section-number">57.9</span> Central Limit Theorem</a></li>
  <li><a href="#common-distributions" id="toc-common-distributions" class="nav-link" data-scroll-target="#common-distributions"><span class="header-section-number">57.10</span> Common Distributions</a></li>
  <li><a href="#joint-marginal-and-conditional-distributions" id="toc-joint-marginal-and-conditional-distributions" class="nav-link" data-scroll-target="#joint-marginal-and-conditional-distributions"><span class="header-section-number">57.11</span> Joint, Marginal, and Conditional Distributions</a>
  <ul class="collapse">
  <li><a href="#joint-distributions" id="toc-joint-distributions" class="nav-link" data-scroll-target="#joint-distributions"><span class="header-section-number">57.11.1</span> Joint Distributions</a></li>
  <li><a href="#marginal-distribution" id="toc-marginal-distribution" class="nav-link" data-scroll-target="#marginal-distribution"><span class="header-section-number">57.11.2</span> Marginal Distribution</a></li>
  <li><a href="#conditional-distribution" id="toc-conditional-distribution" class="nav-link" data-scroll-target="#conditional-distribution"><span class="header-section-number">57.11.3</span> Conditional Distribution</a></li>
  <li><a href="#covariance" id="toc-covariance" class="nav-link" data-scroll-target="#covariance"><span class="header-section-number">57.11.4</span> Covariance</a></li>
  <li><a href="#correlation" id="toc-correlation" class="nav-link" data-scroll-target="#correlation"><span class="header-section-number">57.11.5</span> Correlation</a></li>
  </ul></li>
  <li><a href="#various-sampling-techniques-in-statistics-and-machine-learning" id="toc-various-sampling-techniques-in-statistics-and-machine-learning" class="nav-link" data-scroll-target="#various-sampling-techniques-in-statistics-and-machine-learning"><span class="header-section-number">57.12</span> Various Sampling Techniques in Statistics and Machine Learning</a>
  <ul class="collapse">
  <li><a href="#simple-random-sampling" id="toc-simple-random-sampling" class="nav-link" data-scroll-target="#simple-random-sampling"><span class="header-section-number">57.12.1</span> Simple Random Sampling</a></li>
  <li><a href="#stratified-sampling" id="toc-stratified-sampling" class="nav-link" data-scroll-target="#stratified-sampling"><span class="header-section-number">57.12.2</span> Stratified Sampling</a></li>
  <li><a href="#cluster-sampling" id="toc-cluster-sampling" class="nav-link" data-scroll-target="#cluster-sampling"><span class="header-section-number">57.12.3</span> Cluster Sampling</a></li>
  <li><a href="#systematic-sampling" id="toc-systematic-sampling" class="nav-link" data-scroll-target="#systematic-sampling"><span class="header-section-number">57.12.4</span> Systematic Sampling</a></li>
  </ul></li>
  <li><a href="#sampling-from-distributions" id="toc-sampling-from-distributions" class="nav-link" data-scroll-target="#sampling-from-distributions"><span class="header-section-number">57.13</span> Sampling from Distributions</a>
  <ul class="collapse">
  <li><a href="#inverse-transform-sampling" id="toc-inverse-transform-sampling" class="nav-link" data-scroll-target="#inverse-transform-sampling"><span class="header-section-number">57.13.1</span> Inverse Transform Sampling</a></li>
  <li><a href="#rejection-sampling" id="toc-rejection-sampling" class="nav-link" data-scroll-target="#rejection-sampling"><span class="header-section-number">57.13.2</span> Rejection Sampling</a></li>
  <li><a href="#importance-sampling" id="toc-importance-sampling" class="nav-link" data-scroll-target="#importance-sampling"><span class="header-section-number">57.13.3</span> Importance Sampling</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#target-distribution-exponential" id="toc-target-distribution-exponential" class="nav-link" data-scroll-target="#target-distribution-exponential"><span class="header-section-number">58</span> Target distribution: Exponential</a></li>
  <li><a href="#proposal-distribution-uniform" id="toc-proposal-distribution-uniform" class="nav-link" data-scroll-target="#proposal-distribution-uniform"><span class="header-section-number">59</span> Proposal distribution: Uniform</a></li>
  <li><a href="#number-of-samples" id="toc-number-of-samples" class="nav-link" data-scroll-target="#number-of-samples"><span class="header-section-number">60</span> Number of samples</a></li>
  <li><a href="#draw-samples-from-the-proposal-distribution" id="toc-draw-samples-from-the-proposal-distribution" class="nav-link" data-scroll-target="#draw-samples-from-the-proposal-distribution"><span class="header-section-number">61</span> Draw samples from the proposal distribution</a></li>
  <li><a href="#compute-weights" id="toc-compute-weights" class="nav-link" data-scroll-target="#compute-weights"><span class="header-section-number">62</span> Compute weights</a></li>
  <li><a href="#estimate-the-mean-of-the-target-distribution" id="toc-estimate-the-mean-of-the-target-distribution" class="nav-link" data-scroll-target="#estimate-the-mean-of-the-target-distribution"><span class="header-section-number">63</span> Estimate the mean of the target distribution</a>
  <ul class="collapse">
  <li><a href="#distance-between-distributions." id="toc-distance-between-distributions." class="nav-link" data-scroll-target="#distance-between-distributions."><span class="header-section-number">63.1</span> Distance between Distributions.</a>
  <ul class="collapse">
  <li><a href="#kullback-leibler-divergence-kl-divergence" id="toc-kullback-leibler-divergence-kl-divergence" class="nav-link" data-scroll-target="#kullback-leibler-divergence-kl-divergence"><span class="header-section-number">63.1.1</span> Kullback-Leibler Divergence (KL Divergence)</a></li>
  <li><a href="#jensen-shannon-divergence-jsd" id="toc-jensen-shannon-divergence-jsd" class="nav-link" data-scroll-target="#jensen-shannon-divergence-jsd"><span class="header-section-number">63.1.2</span> Jensen-Shannon Divergence (JSD)</a></li>
  <li><a href="#earth-movers-distance-emd-or-wasserstein-distance" id="toc-earth-movers-distance-emd-or-wasserstein-distance" class="nav-link" data-scroll-target="#earth-movers-distance-emd-or-wasserstein-distance"><span class="header-section-number">63.1.3</span> Earth Mover’s Distance (EMD) or Wasserstein Distance</a></li>
  <li><a href="#hellinger-distance" id="toc-hellinger-distance" class="nav-link" data-scroll-target="#hellinger-distance"><span class="header-section-number">63.1.4</span> Hellinger Distance</a></li>
  <li><a href="#total-variation-distance-tvd" id="toc-total-variation-distance-tvd" class="nav-link" data-scroll-target="#total-variation-distance-tvd"><span class="header-section-number">63.1.5</span> Total Variation Distance (TVD)</a></li>
  </ul></li>
  <li><a href="#functions-of-random-variables" id="toc-functions-of-random-variables" class="nav-link" data-scroll-target="#functions-of-random-variables"><span class="header-section-number">63.2</span> Functions of Random Variables</a>
  <ul class="collapse">
  <li><a href="#calculating-the-distribution-of-y-gx" id="toc-calculating-the-distribution-of-y-gx" class="nav-link" data-scroll-target="#calculating-the-distribution-of-y-gx"><span class="header-section-number">63.2.1</span> Calculating the Distribution of <span class="math inline">\(Y = g(X)\)</span></a></li>
  <li><a href="#expectation-and-variance" id="toc-expectation-and-variance" class="nav-link" data-scroll-target="#expectation-and-variance"><span class="header-section-number">63.2.2</span> Expectation and Variance</a></li>
  </ul></li>
  <li><a href="#probabilistic-programming-languages" id="toc-probabilistic-programming-languages" class="nav-link" data-scroll-target="#probabilistic-programming-languages"><span class="header-section-number">63.3</span> Probabilistic Programming Languages</a></li>
  </ul></li>
  <li><a href="#define-a-standard-normal-distribution-for-x" id="toc-define-a-standard-normal-distribution-for-x" class="nav-link" data-scroll-target="#define-a-standard-normal-distribution-for-x"><span class="header-section-number">64</span> Define a standard normal distribution for X</a></li>
  <li><a href="#define-a-sample-size" id="toc-define-a-sample-size" class="nav-link" data-scroll-target="#define-a-sample-size"><span class="header-section-number">65</span> Define a sample size</a></li>
  <li><a href="#sample-from-x" id="toc-sample-from-x" class="nav-link" data-scroll-target="#sample-from-x"><span class="header-section-number">66</span> Sample from X</a></li>
  <li><a href="#apply-the-transformation-gx-expx-to-get-y" id="toc-apply-the-transformation-gx-expx-to-get-y" class="nav-link" data-scroll-target="#apply-the-transformation-gx-expx-to-get-y"><span class="header-section-number">67</span> Apply the transformation g(X) = exp(X) to get Y</a></li>
  <li><a href="#convert-to-numpy-for-plotting" id="toc-convert-to-numpy-for-plotting" class="nav-link" data-scroll-target="#convert-to-numpy-for-plotting"><span class="header-section-number">68</span> # Convert to numpy for plotting</a></li>
  <li><a href="#samples_y_np-samples_y.numpy" id="toc-samples_y_np-samples_y.numpy" class="nav-link" data-scroll-target="#samples_y_np-samples_y.numpy"><span class="header-section-number">69</span> samples_Y_np = samples_Y.numpy()</a></li>
  <li><a href="#plot-the-histogram-of-x-and-y" id="toc-plot-the-histogram-of-x-and-y" class="nav-link" data-scroll-target="#plot-the-histogram-of-x-and-y"><span class="header-section-number">70</span> Plot the histogram of X and Y</a>
  <ul class="collapse">
  <li><a href="#confusion-matrix" id="toc-confusion-matrix" class="nav-link" data-scroll-target="#confusion-matrix"><span class="header-section-number">70.0.1</span> Confusion Matrix</a></li>
  <li><a href="#example-classification" id="toc-example-classification" class="nav-link" data-scroll-target="#example-classification"><span class="header-section-number">70.0.2</span> Example: Classification</a></li>
  <li><a href="#example-regression" id="toc-example-regression" class="nav-link" data-scroll-target="#example-regression"><span class="header-section-number">70.0.3</span> Example: Regression</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">70.1</span> Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#the-mle-approach" id="toc-the-mle-approach" class="nav-link" data-scroll-target="#the-mle-approach"><span class="header-section-number">70.1.1</span> The MLE Approach</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions"><span class="header-section-number">70.1.2</span> Making Predictions</a></li>
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1"><span class="header-section-number">70.1.3</span> Example</a></li>
  </ul></li>
  <li><a href="#support-vector-machines" id="toc-support-vector-machines" class="nav-link" data-scroll-target="#support-vector-machines"><span class="header-section-number">70.2</span> Support Vector Machines</a>
  <ul class="collapse">
  <li><a href="#linear-classification" id="toc-linear-classification" class="nav-link" data-scroll-target="#linear-classification"><span class="header-section-number">70.2.1</span> Linear Classification</a></li>
  <li><a href="#non-linear-classification-with-kernel-trick" id="toc-non-linear-classification-with-kernel-trick" class="nav-link" data-scroll-target="#non-linear-classification-with-kernel-trick"><span class="header-section-number">70.2.2</span> Non-linear Classification with Kernel Trick</a></li>
  <li><a href="#data-normalization" id="toc-data-normalization" class="nav-link" data-scroll-target="#data-normalization"><span class="header-section-number">70.2.3</span> Data Normalization</a></li>
  <li><a href="#vapnik-chervonenkis-vc-dimension" id="toc-vapnik-chervonenkis-vc-dimension" class="nav-link" data-scroll-target="#vapnik-chervonenkis-vc-dimension"><span class="header-section-number">70.2.4</span> Vapnik-Chervonenkis (VC) Dimension</a></li>
  <li><a href="#kernel-selection" id="toc-kernel-selection" class="nav-link" data-scroll-target="#kernel-selection"><span class="header-section-number">70.2.5</span> Kernel Selection</a></li>
  <li><a href="#svm-for-regression-svr" id="toc-svm-for-regression-svr" class="nav-link" data-scroll-target="#svm-for-regression-svr"><span class="header-section-number">70.2.6</span> SVM for Regression (SVR)</a></li>
  <li><a href="#example-fault-diagnostics-using-classification" id="toc-example-fault-diagnostics-using-classification" class="nav-link" data-scroll-target="#example-fault-diagnostics-using-classification"><span class="header-section-number">70.2.7</span> Example: Fault Diagnostics Using Classification</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#unsupervised-learning" id="toc-unsupervised-learning" class="nav-link" data-scroll-target="#unsupervised-learning"><span class="header-section-number">71</span> Unsupervised Learning</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Unsupervised Learning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>



<section id="machine-learning-algorithms" class="level1" data-number="13">
<h1 data-number="13"><span class="header-section-number">13</span> Machine Learning Algorithms</h1>
</section>
<section id="aerospace-application-1" class="level1" data-number="14">
<h1 data-number="14"><span class="header-section-number">14</span> Aerospace Application 1</h1>
<section id="surrogate-modeling" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="surrogate-modeling"><span class="header-section-number">14.1</span> Surrogate Modeling</h2>
<p>Function approximation is intricately connected to the concept of <em>surrogate modeling</em>, a technique widely used in various scientific and engineering disciplines. Surrogate modeling involves creating a simpler, computationally efficient model (the surrogate) that approximates a more complex or computationally expensive simulation or function. Essentially, it’s about using function approximation methods to build a model that can predict the outputs of a complex system based on its inputs, without having to run the full, detailed simulation or process every time. This approach is particularly valuable in scenarios where the actual computational model is too time-consuming or resource-intensive. By approximating the function of interest, surrogate models enable quicker evaluations, facilitating tasks like optimization, sensitivity analysis, and uncertainty quantification. These models often employ polynomial approximations, piecewise functions, or advanced machine learning techniques, including neural networks, to capture the behavior of the complex system with sufficient accuracy. This not only saves significant computational resources but also opens up new possibilities for analyzing and understanding the underlying system, making surrogate modeling an essential tool in fields ranging from aerospace engineering to financial modeling.</p>
<p>In aerospace engineering, surrogate modeling plays a pivotal role in streamlining the design and analysis processes, where high-fidelity simulations are often computationally expensive and time-consuming. These surrogate models, also known as metamodels, are used extensively for aerodynamic shape optimization, where they approximate complex fluid dynamics simulations to quickly evaluate the performance of various aircraft designs under different flight conditions. This allows for rapid exploration of a large design space, identifying optimal shapes that balance factors like lift, drag, and stability with computational efficiency. Surrogate models are also crucial in structural optimization, helping engineers to predict the strength, weight, and durability of aircraft components while minimizing the need for costly physical prototypes. Additionally, they are employed in the analysis of combustion processes in jet engines, where the models approximate the behavior of fuel burn and emissions, enabling more efficient and environmentally friendly engine designs. By leveraging these models, aerospace engineers can significantly reduce development cycles, optimize performance, and ensure safety, all while managing the immense computational costs associated with simulating complex aerospace systems.</p>
</section>
</section>
<section id="aerospace-application-2" class="level1" data-number="15">
<h1 data-number="15"><span class="header-section-number">15</span> Aerospace Application 2</h1>
<section id="bayesian-inference" class="level3" data-number="15.0.1">
<h3 data-number="15.0.1" class="anchored" data-anchor-id="bayesian-inference"><span class="header-section-number">15.0.1</span> Bayesian Inference</h3>
<ul>
<li><strong>Bayesian vs.&nbsp;Frequentist Approach</strong>: Philosophical differences and their implications in machine learning.</li>
<li><strong>Bayesian Inference and Prior Distributions</strong>: How to update beliefs with evidence, essential in many machine learning models.</li>
<li><strong>Python Code</strong>: Implement simple Bayesian inference examples.</li>
</ul>
</section>
</section>
<section id="data-pre-processing" class="level1" data-number="16">
<h1 data-number="16"><span class="header-section-number">16</span> Data Pre-Processing</h1>
<p>Data preprocessing is a critical step in the machine learning pipeline. It involves preparing and cleaning the raw data before feeding it into a model. The aim is to make the data more suitable for modeling, improve its quality, and increase the overall efficiency of the learning process. Here are some of the common data preprocessing techniques in machine learning:</p>
<section id="data-cleaning" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="data-cleaning"><span class="header-section-number">16.1</span> Data Cleaning</h2>
<p>Data cleaning is a crucial step in the data preprocessing phase, involving the correction or removal of incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. We next describe some of various data cleaning techniques, with real-world examples and Python code for each.</p>
<section id="handling-missing-values" class="level3" data-number="16.1.1">
<h3 data-number="16.1.1" class="anchored" data-anchor-id="handling-missing-values"><span class="header-section-number">16.1.1</span> Handling Missing Values</h3>
<p>Handling missing values is a fundamental aspect of data cleaning in machine learning and data analysis. Missing data can arise due to various reasons like errors during data collection, processing, or transmission. Proper handling of these missing values is crucial as they can lead to biased or incorrect results. Some of the common techniques are summarized next.</p>
<p>Note that the choice of method depends on the nature of the data, the extent of missingness, and the analysis or modeling task. It is important to consider the potential biases introduced by missing data and the chosen method to handle it. In some cases, it might be informative to create an additional binary feature indicating whether data was missing.</p>
<section id="meanmedianmode-imputation" class="level4" data-number="16.1.1.1">
<h4 data-number="16.1.1.1" class="anchored" data-anchor-id="meanmedianmode-imputation"><span class="header-section-number">16.1.1.1</span> Mean/Median/Mode Imputation</h4>
<p>Replacing missing values with the mean, median, or mode of the column. This method is simple and works well with numerical data. The mean is typically used for normal distributions, while the median is better for skewed distributions. The mode is used for categorical data.</p>
<p>Let us consider an engineering application involving sensor data from a manufacturing process. In such settings, sensors might record various parameters like temperature, pressure, and operational speed of machinery. It is common to have missing values due to sensor malfunctions or transmission errors. We’ll create a Python example where we handle missing values in such a dataset using mean and median imputation:</p>
<pre class="{python}"><code>import pandas as pd
import numpy as np

# Example DataFrame representing sensor data in a manufacturing process
# Columns: Temperature (°C), Pressure (kPa), Speed (RPM)
data = {
    'Temperature': [200, 205, np.nan, 210, 208, np.nan, 207],
    'Pressure': [30, 35, 34, np.nan, 36, 37, np.nan],
    'Speed': [1500, 1495, 1500, 1502, np.nan, 1498, 1501]
}

df = pd.DataFrame(data)

# Mean Imputation for Temperature
# Assuming temperature readings are relatively stable and normally distributed
df['Temperature'].fillna(df['Temperature'].mean(), inplace=True)

# Median Imputation for Pressure
# Pressure might have occasional spikes; median is more robust to outliers
df['Pressure'].fillna(df['Pressure'].median(), inplace=True)

# Speed is a crucial operational parameter, so we might decide not to impute and keep it as is

print(df)</code></pre>
<p>In this example, we have a DataFrame <code>df</code> representing sensor data. We use:</p>
<ol type="1">
<li><p><strong>Mean Imputation</strong> for the ‘Temperature’ column. We assume temperature readings are normally distributed and don’t vary wildly in short periods, which is often the case in controlled industrial environments with Gaussian sensor noise.</p></li>
<li><p><strong>Median Imputation</strong> for the ‘Pressure’ column, as pressure can have occasional spikes due to sudden changes in the manufacturing process. Median imputation helps to mitigate the impact of such outliers.</p></li>
<li><p>We choose not to impute values for the ‘Speed’ column, considering that missing values in this crucial operational parameter might need special attention or indicate critical issues that imputation might mask.</p></li>
</ol>
<p>This example illustrates how different imputation methods can be applied in an engineering context, considering the nature and criticality of the data being handled.</p>
</section>
<section id="custom-value" class="level4" data-number="16.1.1.2">
<h4 data-number="16.1.1.2" class="anchored" data-anchor-id="custom-value"><span class="header-section-number">16.1.1.2</span> Custom Value</h4>
<p>Custom value imputation is a technique where missing data in a dataset is filled with a predefined or custom value. This approach is particularly useful in scenarios where it is important to distinctly identify or separate the missing data from the naturally occurring data. In engineering contexts, this method can be used to maintain consistency, signal a specific condition, or ensure that the processing algorithms function correctly without introducing biases.</p>
<p>Consider a dataset from an industrial monitoring system that tracks the operational status of various machines in a factory. The dataset contains readings such as temperature, pressure, and a status code indicating the machine’s condition (e.g., 0 for normal, 1 for maintenance required, etc.). Sometimes, the status code might be missing due to communication issues or sensor errors.</p>
<p>In such a case, we might choose to fill the missing status codes with a custom value that does not overlap with existing status codes. For example, we could use -1 to indicate missing or unknown status. This approach makes it clear that the data was missing and not a part of the normal operational readings.</p>
<pre class="{python}"><code>import pandas as pd
import numpy as np

# Sample DataFrame representing machine operational data
data = {
    'Temperature': [200, 205, 210, 208, 207],
    'Pressure': [30, 35, 34, 36, 37],
    'StatusCode': [0, 1, np.nan, 0, np.nan]  # 0: normal, 1: maintenance required
}

df = pd.DataFrame(data)

# Custom Value Imputation for StatusCode
# -1 will indicate missing or unknown status
df['StatusCode'].fillna(-1, inplace=True)

print(df)</code></pre>
<p>In this code we create a DataFrame <code>df</code> with ‘Temperature’, ‘Pressure’, and ‘StatusCode’ columns. Missing values in the ‘StatusCode’ column are replaced with -1, a custom value indicating unknown status. By using a custom value for imputation, engineers and analysts can easily distinguish between normal data and data that was missing or not recorded, which is essential for accurate monitoring and decision-making in industrial settings.</p>
</section>
<section id="predictive-imputation" class="level4" data-number="16.1.1.3">
<h4 data-number="16.1.1.3" class="anchored" data-anchor-id="predictive-imputation"><span class="header-section-number">16.1.1.3</span> Predictive Imputation</h4>
<p>Predictive imputation is a sophisticated method of handling missing data in a dataset. Unlike simpler methods like mean or median imputation, predictive imputation uses the relationships found in the non-missing parts of the data to predict and fill in the missing values. This approach often leads to more accurate and realistic data imputation, especially when the missing data is not random and depends on other variables in the dataset.</p>
<p>The process begins with the development of a model using the portion of the dataset that does not contain any missing values. This model is tailored to predict the missing variable using other related variables in the dataset as inputs. Essentially, it learns the relationships and patterns present in the complete data to estimate the missing values.</p>
<p>Once the model is established and trained, it is then employed to predict the missing values. For each instance in the dataset where the target variable is missing, the model uses the available, non-missing data to make a prediction. It applies the relationships it has learned to estimate the missing value as accurately as possible.</p>
<p>The final step in predictive imputation is the substitution of these predicted values back into the dataset. The missing values are replaced with these newly predicted values, thus filling the gaps in the dataset. This method ensures a more informed and potentially accurate way of dealing with missing data, as it takes into account the underlying patterns and correlations in the data, unlike simpler methods such as mean or median imputation. However, the success of predictive imputation largely depends on the accuracy and appropriateness of the predictive model used, making it crucial to choose and tune the model carefully based on the specific characteristics of the data. If physics-based models are available, it can be also be used predict missing data.</p>
<p>Predictive imputation stands out for its ability to yield more accurate results compared to simpler imputation methods, largely because it takes into account the correlations and relationships between different variables in the dataset. This makes it especially advantageous in situations where the missingness of data is not random but instead is influenced by other variables, allowing for a more context-aware approach to filling in missing values. However, this technique comes with its own set of challenges. The process of building and validating an appropriate predictive model can be complex and time-consuming, requiring careful consideration and expertise. There’s also a risk of overfitting, particularly if the model is too complex, which can result in imputations that are less reliable as they may overly conform to the specifics of the available data rather than general patterns. Furthermore, the effectiveness of predictive imputation hinges significantly on the assumptions underpinning the chosen model and its alignment with the data’s characteristics. Any misalignment or incorrect assumptions can adversely affect the accuracy of the imputed values.</p>
<p>Let’s consider an engineering scenario where we have a dataset from a wind turbine farm. This dataset includes variables like wind speed, turbine rotation speed, and power output. Sometimes, due to sensor errors or data transmission issues, we might have missing values in the power output, which we want to predict based on the other variables.</p>
<p>We’ll use a simple linear regression model for predictive imputation. This example assumes that a linear relationship exists between the wind speed, turbine rotation speed, and the power output of the turbines. Here’s the Python code demonstrating this:</p>
<pre class="{python}"><code>import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
import numpy as np

# Sample DataFrame representing wind turbine data
# Columns: WindSpeed (m/s), TurbineSpeed (RPM), PowerOutput (kW)
data = {
    'WindSpeed': [5.2, 7.4, 6.5, np.nan, 7.0, 5.5, 6.8],
    'TurbineSpeed': [1200, 1400, 1300, 1250, 1350, 1280, np.nan],
    'PowerOutput': [300, 400, 350, np.nan, 390, 320, 360]
}

df = pd.DataFrame(data)

# Imputing missing values in WindSpeed and TurbineSpeed with their means
imputer = SimpleImputer(strategy='mean')
df['WindSpeed'] = imputer.fit_transform(df[['WindSpeed']])
df['TurbineSpeed'] = imputer.fit_transform(df[['TurbineSpeed']])

# Separating the dataset into two - one where PowerOutput is missing and one where It is available
df_missing_power = df[df['PowerOutput'].isna()]
df_complete = df.dropna(subset=['PowerOutput'])

# Training a linear regression model to predict PowerOutput
model = LinearRegression()
model.fit(df_complete[['WindSpeed', 'TurbineSpeed']], df_complete['PowerOutput'])

# Predicting the missing PowerOutput values
predicted_power = model.predict(df_missing_power[['WindSpeed', 'TurbineSpeed']])
df_missing_power.loc[:,'PowerOutput'] = predicted_power

# Combining the data back together
df_imputed = pd.concat([df_complete, df_missing_power])

print(df_imputed)</code></pre>
<p>In this code, we create a DataFrame <code>df</code> with simulated data for wind speed, turbine speed, and power output. Missing values in ‘WindSpeed’ and ‘TurbineSpeed’ are imputed using the mean of their respective columns. Next, we train a linear regression model on the part of the dataset where ‘PowerOutput’ is not missing, using ‘WindSpeed’ and ‘TurbineSpeed’ as predictors. The model is then used to predict missing ‘PowerOutput’ values. Finally, we combine the imputed data back into a single DataFrame.</p>
<p><em>Note:</em> This approach assumes a linear relationship between the variables, which might be an oversimplification in real-world scenarios. In practice, more complex models and validation methods would likely be necessary to accurately predict missing values in an engineering context.</p>
</section>
<section id="dropping-rows-or-columns" class="level4" data-number="16.1.1.4">
<h4 data-number="16.1.1.4" class="anchored" data-anchor-id="dropping-rows-or-columns"><span class="header-section-number">16.1.1.4</span> Dropping Rows or Columns</h4>
<p>In engineering and similar data-intensive fields, dropping rows and columns from a dataset is a common data cleaning practice, particularly essential when dealing with issues such as missing data, irrelevant information, or erroneous entries.</p>
<p>The decision to drop rows is often made when they contain a high proportion of missing values or outliers that could skew analysis results. This is particularly relevant in engineering scenarios like sensor data analysis, where missing or aberrant readings can significantly distort the interpretation.</p>
<p>Dropping columns is typically considered when the data they contain is irrelevant, highly correlated with other columns, or so predominantly missing as to render them uninformative. For instance, in an engineering dataset, a feature that remains constant across all data points (such as a specific component in a machine that doesn’t vary) may be dropped, as it doesn’t contribute to the variability or insights sought in the analysis.</p>
<p>While dropping rows and columns can simplify and improve the quality of the data, it must be done judiciously to avoid losing valuable information or introducing bias. The process should be well-documented and justified, considering the specific goals and context of the analysis. Python’s pandas library simplifies this task with functions like <code>dropna()</code> for rows and <code>drop()</code> for columns, facilitating efficient data cleaning in engineering data processing workflows.</p>
</section>
</section>
<section id="smoothing-noisy-data" class="level3" data-number="16.1.2">
<h3 data-number="16.1.2" class="anchored" data-anchor-id="smoothing-noisy-data"><span class="header-section-number">16.1.2</span> Smoothing Noisy Data</h3>
<p>Smoothing noisy data is an important step in data preprocessing, especially in fields like signal processing, finance, and engineering where data quality can significantly impact analysis and decision-making. Several techniques are commonly used to smooth out noise, each with its own advantages and disadvantages.</p>
<section id="binning" class="level4" data-number="16.1.2.1">
<h4 data-number="16.1.2.1" class="anchored" data-anchor-id="binning"><span class="header-section-number">16.1.2.1</span> Binning</h4>
<p>Binning is a data smoothing technique commonly used to reduce the effects of minor observation errors. The main idea is to transform continuous numeric variables into discrete categories, or “bins”. Each original data value is replaced by a value representative of its bin, often the bin’s mean or median.</p>
<p>Binning encompasses three main steps. First, the range of continuous data is segmented into a series of intervals, commonly referred to as bins. This segmentation is a crucial step as it determines the granularity of the analysis. In the next step, each individual data point from the dataset is allocated to one of these bins, based on where it falls within the range. The final step involves replacing the data points in each bin with a representative value of that bin, which is often the mean or median. This representative value stands in for all the data points in the bin, effectively smoothing out the data by reducing the impact of minor variations or outliers within each bin.</p>
<p>Suppose we have a dataset of temperature readings that we wish to bin. We can use Python’s pandas library to accomplish this, as shown next.</p>
<pre class="{python}"><code>import pandas as pd

# Sample data: temperature readings
temperature_data = [68, 71, 74, 69, 70, 73, 65, 67, 72, 70, 75]

# Convert to DataFrame
df = pd.DataFrame(temperature_data, columns=['Temperature'])

# Define the number of bins
num_bins = 3

# Create bins
df['Temp_Binned'] = pd.cut(df['Temperature'], bins=num_bins)

# Replace each value by the mean of its bin
df['Temp_Binned_Mean'] = df.groupby('Temp_Binned',observed=False)['Temperature'].transform('mean')

print(df)</code></pre>
<p>In the above example, a series of temperature readings is initially organized into a pandas DataFrame. To smooth these data, the readings are categorized into three distinct bins by employing the <code>pd.cut()</code> function. Following this, each temperature reading is replaced with the average value of its respective bin. This is achieved by segmenting the DataFrame based on the binned temperature categories and then computing the mean temperature for each category.</p>
<p>Binning offers notable advantages. It is straightforward to implement and effectively smooths the data, thereby minimizing the impact of minor variances. However, there are also drawbacks to consider. The size of the bins chosen can greatly influence the outcome, potentially leading to significant alterations in the results. Additionally, this method might result in a loss of detailed information and the original data’s variability, as it essentially generalizes the data points within each bin.</p>
<p>This method is particularly useful in situations where the data contains <em>minor</em> inaccuracies or when preparing data for certain types of categorical analysis. It helps to mitigate the impact of minor errors or variances in the measurements.</p>
</section>
<section id="regression" class="level4" data-number="16.1.2.2">
<h4 data-number="16.1.2.2" class="anchored" data-anchor-id="regression"><span class="header-section-number">16.1.2.2</span> Regression</h4>
<p>The regression-based technique for smoothing noisy data involves constructing a statistical model that best fits the observed data. Essentially, this method fits a regression model (such as linear, polynomial, or logistic regression, depending on the data nature) to the dataset. Once this model is established, it is used to predict the expected values for each data point. These predicted values, which are based on the underlying pattern identified by the regression model, are generally smoother and less noisy compared to the original data. There are many advantages to this approach. It is capable of handling complex relationships within the data, making it particularly valuable in scenarios where the underlying data patterns are intricate and not immediately apparent. Additionally, it is most effective when the noise present in the data is random, as the regression model focuses on capturing and representing the fundamental trend or pattern in the data.</p>
<p>However, there are some disadvantages to this technique. One significant risk is overfitting, especially if the chosen model is overly complex. Overfitting occurs when the model becomes too tailored to the specific idiosyncrasies of the training data, reducing its ability to generalize and perform well on new, unseen data. This issue is particularly pertinent when dealing with large datasets or datasets with many features, where the model may inadvertently learn noise as if it were a valid signal. Furthermore, regression models, especially complex ones, can demand substantial computational resources, making them less practical for very large datasets or scenarios where computational efficiency is a priority. These drawbacks necessitate a careful balance in model selection and complexity, ensuring that the model is sophisticated enough to capture the essential patterns in the data, but not so complex that it overfits or becomes computationally unmanageable.</p>
</section>
<section id="moving-average" class="level4" data-number="16.1.2.3">
<h4 data-number="16.1.2.3" class="anchored" data-anchor-id="moving-average"><span class="header-section-number">16.1.2.3</span> Moving Average</h4>
<p>The moving average technique is a widely used method for smoothing noisy data, particularly in time series analysis. It involves calculating the average of a specified number of data points within a moving window that progresses through the dataset. For example, in a simple moving average, each data point is replaced with the average of itself and the surrounding data points within the window. This method effectively dampens short-term fluctuations and highlights longer-term trends or cycles in the data.</p>
<p><strong>Advantage:</strong> One of the main advantages of the moving average technique is its simplicity and ease of implementation. It requires minimal computational resources, making it suitable for large datasets and real-time data analysis. Additionally, it’s intuitive and straightforward to interpret, which is particularly beneficial for quick insights and initial data exploration.</p>
<p>To demonstrate the advantage of moving average smoothing, let’s create a Python example where we apply this technique to a time series data set with some random noise. The goal is to see how the moving average can smooth out short-term fluctuations and reveal underlying trends in the data. We’ll start by generating a simple time series dataset with added random noise and then apply a moving average smoothing technique to it:</p>
<pre class="{python}"><code>#| fig-cap: Demonstration of moving average as a technique for data de-noising.
#| label: fig-moving_average
#| layout-ncol: 1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate sample time series data
np.random.seed(0)
time = np.arange(100)
data = np.sin(time / 10) + np.random.normal(scale=0.5, size=time.size)

# Convert to DataFrame for convenience
df = pd.DataFrame({'Time': time, 'Data': data})

# Apply moving average with a window of 10
window_size = 10
df['Moving_Average'] = df['Data'].rolling(window=window_size).mean()

# Plotting the original data and the smoothed data
plt.figure(figsize=(12, 6))
plt.plot(df['Time'], df['Data'], label='Original Data')
plt.plot(df['Time'], df['Moving_Average'], label='Moving Average', color='red')
plt.title('Effect of Moving Average Smoothing')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()</code></pre>
<p>In this code, we create a time series dataset (<code>data</code>) that represents some signal (in this case, a sine wave) with added Gaussian noise. The data is put into a pandas DataFrame for ease of manipulation. We then apply a moving average smoothing with a window size of 10. This means each point in the ‘Moving_Average’ column is the average of the current and the previous 9 points in the ‘Data’ column. Finally, we plot both the original and the smoothed data. <a href="data_pre_processing.html#fig-moving_average">Figure&nbsp;<span>10.1</span></a> shows how the moving average smooths out the fluctuations and reveals the underlying sine wave pattern more clearly.</p>
<p><strong>Disadvantage:</strong> However, the moving average technique also has its limitations. A significant disadvantage is its inability to handle rapid changes or non-linear trends in the data effectively. The method tends to “lag” behind the actual data, especially with larger window sizes, meaning that it might not be responsive enough to recent changes in the data. Furthermore, the choice of the window size is crucial; a window that is too small may not smooth the data sufficiently, while a window that is too large can oversmooth the data, potentially obscuring important details and patterns. Additionally, the moving average does not differentiate between older and more recent data points within the window, treating all points with equal importance, which might not always be appropriate depending on the data’s nature and the analysis’s goals.</p>
<p>The following Python code demonstrate a disadvantage of using the moving average for data denoising, especially in the context of rapidly changing or non-linear trends. The code generates a time series dataset that includes a sudden change in trend, highlighting a key disadvantage of the moving average smoothing method. Initially, random data is generated to simulate a scenario where the data abruptly shifts from one trend to another. Specifically, for the first half of the time series, the data follows a normal distribution with a mean of 0, and then it suddenly shifts to a different normal distribution with a mean of 5, representing a distinct change in trend.</p>
<pre class="{python}"><code>#| fig-cap: Limitations of moving average data denoising.
#| label: fig-moving_average_bad
#| layout-ncol: 1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate sample time series data with a sudden change
np.random.seed(0)
time = np.arange(100)
data = np.where(time &lt; 50, np.random.normal(0, 0.5, time.size), np.random.normal(5, 0.5, time.size))

# Convert to DataFrame
df = pd.DataFrame({'Time': time, 'Data': data})

# Apply moving average with a window of 10
window_size = 10
df['Moving_Average'] = df['Data'].rolling(window=window_size).mean()

# Plotting the original data and the smoothed data
plt.figure(figsize=(12, 6))
plt.plot(df['Time'], df['Data'], label='Original Data')
plt.plot(df['Time'], df['Moving_Average'], label='Moving Average', color='red')
plt.title('Disadvantage of Moving Average in Rapid Trend Changes')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()</code></pre>
<p>This dataset is converted into a pandas DataFrame for ease of manipulation. A moving average is then applied to this data with a window size of 10. This window size means that each point in the ‘Moving_Average’ column of the DataFrame represents the average of the current and the previous nine points in the ‘Data’ column.</p>
<p>The key point of interest is observed in <a href="data_pre_processing.html#fig-moving_average_bad">Figure&nbsp;<span>10.2</span></a>, where both the original and smoothed data are plotted. The plot clearly illustrates the disadvantage of the moving average in this scenario. Due to the moving average’s inherent nature, it <em>lags</em> in responding to the sudden change in trend. This lag is particularly noticeable at the point where the data shifts from one distribution to another. This lag can lead to misinterpretation or delay in recognizing significant changes in the data, which is a critical drawback in applications where timely and accurate response to data trends is essential. ​</p>
</section>
<section id="exponential-smoothing" class="level4" data-number="16.1.2.4">
<h4 data-number="16.1.2.4" class="anchored" data-anchor-id="exponential-smoothing"><span class="header-section-number">16.1.2.4</span> Exponential Smoothing</h4>
<p>Exponential smoothing is a popular data smoothing technique, particularly effective in time series forecasting. It operates by applying decreasing weights to past observations, with the most recent observations given more significance. The fundamental principle is that more recent data is a better reflector of the future, hence the exponential decrease in weight for older data. This weighting is achieved through a smoothing factor, typically denoted as <span class="math inline">\(\alpha\)</span>, which determines how rapidly the weights decrease; a higher alpha places more emphasis on recent observations.</p>
<p><strong>Advantages:</strong> One of the key advantages of exponential smoothing is its adaptability to changes in data trends and patterns. Unlike simple moving averages that treat all points in the window equally, exponential smoothing can quickly adjust to recent changes, making it more responsive to shifts in the underlying data pattern. This feature makes it particularly useful for forecasting in scenarios where data trends are dynamic and evolving.</p>
<p>Here is a Python code example demonstrating the use of exponential smoothing on a time series dataset. We use the same data as shown in <a href="data_pre_processing.html#fig-moving_average_bad">Figure&nbsp;<span>10.2</span></a>, where the simple moving average was lagging.</p>
<pre class="{python}"><code>#| fig-cap: Exponential smoothing of a noisy data with sudden jumps in trends.
#| label: fig-exp_smoothing_good
#| layout-ncol: 1
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Generate sample time series data with a linear trend
np.random.seed(0)
time = np.arange(100)
data = np.where(time &lt; 50, np.random.normal(0, 0.5, time.size), np.random.normal(5, 0.5, time.size))

# Convert to DataFrame
df = pd.DataFrame({'Time': time, 'Data': data})

# Apply exponential smoothing
alpha = 0.3  # Smoothing factor
df['Exponential_Smoothing'] = df['Data'].ewm(alpha=alpha).mean()

# Plotting the original data and the smoothed data
plt.figure(figsize=(12, 6))
plt.plot(df['Time'], df['Data'], label='Original Data')
plt.plot(df['Time'], df['Exponential_Smoothing'], label='Exponential Smoothing', color='red')
plt.title('Exponential Smoothing Demonstration')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()</code></pre>
<p><a href="data_pre_processing.html#fig-exp_smoothing_good">Figure&nbsp;<span>10.3</span></a> shows both the original noisy data and the smoothed data. It illustrates how exponential smoothing effectively smooths out the noise, and is more responsive to changes in trends in comparison with simple moving average shown in <a href="data_pre_processing.html#fig-moving_average_bad">Figure&nbsp;<span>10.2</span></a>.</p>
<p><strong>Limitations:</strong> However, the method also has its drawbacks. Choosing the right smoothing factor <span class="math inline">\(\alpha\)</span> can be challenging and may require trial and error or optimization techniques. If <span class="math inline">\(\alpha\)</span> is set too high, the smoothed data might become too sensitive to recent changes, capturing random fluctuations rather than the underlying trend. Conversely, if <span class="math inline">\(\alpha\)</span> is too low, the method becomes similar to a moving average, potentially lagging behind recent trends. Another limitation is its simplicity; exponential smoothing is best suited for univariate time series without seasonality or trend components. For more complex data structures, more sophisticated methods like Holt-Winters exponential smoothing, which extends the basic idea to capture seasonality and trends, are often required. We can change <span class="math inline">\(\alpha\)</span> in the above code to <span class="math inline">\(0.1\)</span> to see increased lag in the prediction, which is worse than the simple moving average example.</p>
<p>While exponential smoothing is a valuable tool in time series analysis, especially for its quick adaptation to recent data changes and its ease of implementation. However, its effectiveness is dependent on the appropriate setting of the smoothing factor and the complexity of the time series data. It works best for simpler, non-seasonal series and might not be suitable for more complex datasets with multiple influencing factors.</p>
</section>
<section id="filtering-e.g.-low-pass-filters" class="level4" data-number="16.1.2.5">
<h4 data-number="16.1.2.5" class="anchored" data-anchor-id="filtering-e.g.-low-pass-filters"><span class="header-section-number">16.1.2.5</span> Filtering (e.g., Low-pass filters)</h4>
<p>Low-pass filtering is a technique used in both signal processing and machine learning to reduce noise in data. It works by allowing signals with a frequency lower than a certain cutoff frequency to pass through and attenuating frequencies higher than this cutoff. The rationale and application in machine learning can be understood through the following points:</p>
<ol type="1">
<li><p><strong>Nature of Noise</strong>: In many real-world datasets, noise is often high-frequency. This means that the unwanted variations or fluctuations in the data occur at short intervals. These could be due to various factors like measurement errors, anomalies, or irrelevant variations.</p></li>
<li><p><strong>Smoothing Data</strong>: Low-pass filters help in smoothing the data by removing these high-frequency fluctuations. This can be particularly useful in time-series data or any data with a temporal or spatial dimension, where smoothness often corresponds to more meaningful trends and patterns.</p></li>
<li><p><strong>Preserving Relevant Information</strong>: By choosing an appropriate cutoff frequency, a low-pass filter can preserve the essential, low-frequency components of the data. These low-frequency components often represent the underlying trends or the ‘signal’ in the data that are of interest for analysis and model training.</p></li>
<li><p><strong>Improving Model Performance</strong>: In machine learning, models trained on noisy data can overfit, meaning they learn the noise as part of the signal, which reduces their ability to generalize to new, unseen data. By denoising the data, low-pass filtering can improve the generalization ability of these models.</p></li>
<li><p><strong>Implementation</strong>: Low-pass filtering can be implemented in several ways, such as using Fourier transforms, moving averages, or specific digital filter designs like Butterworth or Chebyshev filters. The choice of method depends on the nature of the data and the specific requirements of the task.</p></li>
</ol>
<p><strong>Advantages:</strong> Low-pass filtering is a valuable preprocessing step at reducing high-frequency noise, which is a common characteristic of many types of noise in data, especially in signal processing and time-series analysis. These filters can preserve the main signal or trend in the data if the cutoff frequency is chosen correctly. This makes them particularly useful for applications where the signal of interest is of lower frequency. Low-pass filters are generally simple to implement and computationally efficient. They don’t require extensive computational resources, making them suitable for real-time applications. By removing noise, they can improve the performance of machine learning models, reducing the chance of overfitting and enhancing the model’s ability to generalize from training to unseen data. There are various types of low-pass filters (e.g., Butterworth, Chebyshev), offering flexibility to choose one that best fits the specific characteristics of the data. Here is a Python code demonstrating advantages of using low-pass filtering.</p>
<pre class="{python}"><code>#| fig-cap: Low-pass filtering to denoise data.
#| label: fig-low_pass_filtering
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter

# Function to create a Butterworth low-pass filter
def butter_lowpass(cutoff, fs, order=5):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a

# Function to apply the low-pass filter
def lowpass_filter(data, cutoff, fs, order=5):
    b, a = butter_lowpass(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y

# Sample rate and desired cutoff frequency of the filter
fs = 1000  # Sample rate, Hz
cutoff = 3  # Desired cutoff frequency of the filter, Hz

# Generate a clean sinusoidal signal
T = 5.0     # seconds
n = int(T * fs)  # total number of samples
t = np.linspace(0, T, n, endpoint=False)
clean_signal = np.sin(1.2 * 2 * np.pi * t)

# Add noise to the signal
noise = np.random.normal(0, 0.5, clean_signal.shape)
noisy_signal = clean_signal + noise

# Filter the noisy signal
filtered_signal = lowpass_filter(noisy_signal, cutoff, fs)

# Plotting
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.plot(t, clean_signal)
plt.title('Clean Signal')
plt.grid()

plt.subplot(1, 3, 2)
plt.plot(t, noisy_signal)
plt.title('Noisy Signal')
plt.grid()

plt.subplot(1, 3, 3)
plt.plot(t, filtered_signal,label="Filtered Signal")
plt.plot(t, clean_signal,'--',label="Clean Signal")
plt.title('Filtered Signal (and Clean Signal)')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()</code></pre>
<p><a href="data_pre_processing.html#fig-low_pass_filtering">Figure&nbsp;<span>10.4</span></a> shows three plots: the original clean sinusoidal signal, the same signal with added noise, and the noisy signal after applying a low-pass filter. We see that the filtered signal is devoid of any noise, but has a magnitude and phase distortion when compared to the clean signal.</p>
<p><strong>Limitations:</strong> Low-pass filtering, while effective for reducing high-frequency noise, presents several challenges and limitations. A significant concern is the potential loss of valuable high-frequency information, which can be detrimental if the signal of interest includes crucial high-frequency components. Additionally, selecting the correct cutoff frequency is a critical yet often challenging task; an inappropriate cutoff may either inadequately filter noise or inadvertently eliminate important signal elements. Low-pass filters can also introduce phase shifts or distortions, which are problematic in scenarios where signal phase is key. There’s also a risk of oversmoothing with excessive use of these filters, leading to a loss of important data variations and details. Moreover, these filters are specifically tailored for high-frequency noise and may not be suitable for low-frequency or broadly distributed noise types. Finally, in the context of time-series or spatial data, low-pass filtering can cause edge effects at data boundaries, where the performance of the filter may be compromised due to insufficient surrounding data points. These limitations highlight the need for careful consideration and application of low-pass filtering in data processing and analysis.</p>
<p>The following Python code demonstrates the disadvantages of using low-pass filtering in denoising data. The example focuses on two primary concerns: the loss of significant high-frequency signal components and the possibility of phase shift. We first create a composite signal with both low and high-frequency elements, mimicking a real-world scenario where a signal carries multiple frequencies. Then we add a random noise to it, simulating typical real-life interference. The noisy data is then smoothened using a low-pass filter.</p>
<p>While the filter’s primary objective is to remove noise, it is important to observe its impact on the high-frequency components of the signal. Low-pass filtering, by design, attenuates high-frequency elements, and this effect becomes evident in this example. Finally, the example involves a comparison among the original signal, the noisy version, and the filtered output. This comparison is crucial as it highlights the loss of high-frequency details due to filtering, and it also reveals phase shift introduced by the filter. Such phase shifts can misalign signal components in time, which can be a critical issue in applications where the timing of signal events is important – for example detecting faults in safety critical systems.</p>
<pre class="{python}"><code>#| fig-cap: Loss of high frequency features, and phase and magnitude distortions, in low-pass filtering.
#| layout-ncol: 1
#| label: fig-low_pass_bad
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter

# Function to create a Butterworth low-pass filter
def butter_lowpass(cutoff, fs, order=5):
    nyq = 0.5 * fs
    normal_cutoff = cutoff / nyq
    b, a = butter(order, normal_cutoff, btype='low', analog=False)
    return b, a

# Function to apply the low-pass filter
def lowpass_filter(data, cutoff, fs, order=5):
    b, a = butter_lowpass(cutoff, fs, order=order)
    y = lfilter(b, a, data)
    return y

# Sample rate and desired cutoff frequency of the filter
fs = 1000  # Sample rate, Hz
cutoff = 10  # Desired cutoff frequency of the filter, Hz

# Generate a composite signal (low + high frequency)
T = 5.0     # seconds
n = int(T * fs)  # total number of samples
t = np.linspace(0, T, n, endpoint=False)
low_freq_signal = np.sin(2 * np.pi * 1.5 * t)
high_freq_signal = np.sin(2 * np.pi * 30 * t)
composite_signal = low_freq_signal + high_freq_signal

# Add noise to the signal
noise = np.random.normal(0, 0.3, composite_signal.shape)
noisy_signal = composite_signal + noise

# Filter the noisy signal
filtered_signal = lowpass_filter(noisy_signal, cutoff, fs)

# Plotting
plt.figure(figsize=(15, 7))
plt.subplot(2, 2, 1)
plt.plot(t, composite_signal)
plt.title('Original Composite Signal')
plt.grid()

plt.subplot(2, 2, 2)
plt.plot(t, noisy_signal)
plt.title('Noisy Signal')
plt.grid()

plt.subplot(2, 2, 3)
plt.plot(t, filtered_signal,label="Filtered signal")
plt.plot(t, low_freq_signal,'--', label="Original low frequency signal")
plt.title('Filtered Signal')
plt.legend()
plt.grid()

plt.subplot(2, 2, 4)
plt.plot(t, high_freq_signal)
plt.title('High Frequency Component')
plt.grid()

plt.tight_layout()
plt.show()
</code></pre>
<p>The code generates four plots (see <a href="data_pre_processing.html#fig-low_pass_bad">Figure&nbsp;<span>10.5</span></a>): the original composite signal with both low and high-frequency components, the same signal with added noise, the filtered signal, and the high-frequency component of the original signal. After applying the low-pass filter, we notice that the high-frequency component is lost in the filtered signal, illustrating the disadvantage of losing important high-frequency details. Additionally, there are magnitude and phase distortions in the predicted (or reconstructed) low frequency signal. This example underscores the need for careful consideration when applying low-pass filters, as they can potentially alter the desired characteristics of the original signal.</p>
</section>
<section id="wavelet-transformation" class="level4" data-number="16.1.2.6">
<h4 data-number="16.1.2.6" class="anchored" data-anchor-id="wavelet-transformation"><span class="header-section-number">16.1.2.6</span> Wavelet Transformation</h4>
<p>Wavelet transformation is a powerful tool in denoising data, particularly in the field of machine learning. It involves decomposing a signal into a set of wavelets, which are small waves that vary in frequency and duration. This technique is advantageous because it provides a multi-resolution analysis of the data, allowing for the observation and modification of the signal at various scales. The process typically involves transforming the noisy data into the wavelet domain, where noise and signal components can be distinctly identified and separated. Denoising is then achieved by applying a thresholding technique to remove the noise components while retaining the essential features of the original signal.</p>
<p>Wavelet Transform (WT) is a mathematical technique used for signal processing, which involves decomposing a signal into components at various scales using wavelets. Wavelets are functions that are localized in both time and frequency, unlike the sinusoids used in Fourier Transform, which are localized only in frequency. The mathematical rigor of wavelet transform can be understood through its two main types: Continuous Wavelet Transform (CWT) and Discrete Wavelet Transform (DWT).</p>
<p>CWT of a continuous signal <span class="math inline">\(x(t)\)</span> is defined by the integral: <span class="math display">\[ W_x(a, b) = \frac{1}{\sqrt{|a|}} \int_{-\infty}^{\infty} x(t) \psi^*\left(\frac{t-b}{a}\right) dt,\]</span> where <span class="math inline">\(W_x(a, b)\)</span> is the wavelet coefficient at scale <span class="math inline">\(a\)</span> and position <span class="math inline">\(b\)</span>, <span class="math inline">\(\psi(t)\)</span> is the mother wavelet – a function localized in time, <span class="math inline">\(\psi^*(t)\)</span> is the complex conjugate of <span class="math inline">\(\psi(t)\)</span>, <span class="math inline">\(a\)</span> is the scale parameter, and <span class="math inline">\(b\)</span> is the translation parameter.</p>
<p>The signal is convolved with a family of wavelets, which are scaled and translated versions of the mother wavelet. The scale parameter <span class="math inline">\(a\)</span> compresses or stretches the wavelet, allowing analysis at different frequency bands, while the translation parameter <span class="math inline">\(b\)</span> moves the wavelet along the time axis, enabling time localization.</p>
<p>DWT provides a discrete and computationally efficient version of the wavelet transform. It is defined using dyadic scales and positions: <span class="math display">\[DWT_{jk} = \sum_{n} x[n] \psi_{jk}[n],\]</span> where <span class="math inline">\(\psi_{jk}[n] = 2^{-j/2} \psi(2^{-j}n - k)\)</span> are the discrete wavelets, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> are integers that control the scale and translation, respectively, and <span class="math inline">\(x[n]\)</span> is the discrete signal.</p>
<p>DWT is typically implemented using a filter bank consisting of a high-pass and a low-pass filter, corresponding to the decomposition of the signal into approximation and detail coefficients at each level. The process is iterative, where the approximation coefficients at each level are further decomposed in subsequent levels.</p>
<p><strong>Advantages:</strong> One of the main advantages of wavelet transformation is its ability to handle non-stationary signals, where the statistical properties of the signal change over time. This makes it particularly useful in real-world scenarios where data often exhibits such characteristics. The wavelet transform is also efficient in localizing both time and frequency features, making it superior to traditional Fourier methods for many applications. This localization allows for more precise denoising, as it can differentiate between noise and signal characteristics at different scales. The following Python code demonstrates denoising data (or signals) with wavelet transformation.</p>
<pre class="{python}"><code>#| fig-cap: Denoising data using wavelet transformations.
#| layout-ncol: 1
#| label: fig-wavelet_denoising
import numpy as np
import matplotlib.pyplot as plt
import pywt

# Create a sample signal with noise
t = np.linspace(0, 1, 1000, endpoint=False)
original_signal = np.sin(2 * np.pi * 5 * t)  # 5 Hz sine wave
noise = np.random.normal(0, 0.5, 1000)
noisy_signal = original_signal + noise

# Wavelet denoising
def wavelet_denoise(data, wavelet, level):
    coeff = pywt.wavedec(data, wavelet, mode="per")
    threshold = np.sqrt(2 * np.log(len(data))) * np.median(np.abs(coeff[-level]) / 0.6745)
    coeff[1:] = (pywt.threshold(i, value=threshold, mode='soft') for i in coeff[1:])
    reconstructed_signal = pywt.waverec(coeff, wavelet, mode="per")
    return reconstructed_signal

denoised_signal = wavelet_denoise(noisy_signal, 'db10', 1)

# Plotting the results
plt.figure(figsize=(12, 8))
plt.subplot(311)
plt.plot(t, original_signal)
plt.title("Original Signal")
plt.subplot(312)
plt.plot(t, noisy_signal)
plt.title("Noisy Signal")
plt.subplot(313)
plt.plot(t, denoised_signal,label="denoised")
plt.plot(t, original_signal,'--',label="original")
plt.title("Denoised Signal")
plt.legend()
plt.tight_layout()
plt.show()</code></pre>
<p>In the above Python code, we demonstrate the use of wavelet transformation for effective denoising of a signal.We frist generate a 5 Hz sine wave, with random Gaussian noise added to create a noisy signal. The key to the denoising process is the <code>wavelet_denoise</code> function, which leverages the Discrete Wavelet Transform (DWT). This function decomposes the noisy signal into wavelet coefficients, then applies a noise-reduction technique by thresholding these coefficients. The thresholding method employed is the universal threshold with soft thresholding, a common approach in denoising tasks. This method calculates the threshold value based on the median of the coefficients at the highest decomposition level, adjusted by a factor related to the length of the data. This calculation is critical in determining an optimal threshold for effectively removing noise.</p>
<p>For simplicity and demonstration purposes, the Daubechies wavelet (‘db10’) is used, though the choice of wavelet can vary depending on the specific characteristics of the signal being processed. <a href="data_pre_processing.html#fig-wavelet_denoising">Figure&nbsp;<span>10.6</span></a> shows the original signal, the noisy version, and the denoised signal. We observe that the wavelet transform is an effective techniques to denoise signals. However, it introduces distortions in the reconstructed signal, but are less prone to introducing phase distortions compared to some other signal processing techniques.</p>
<p><strong>Limitations:</strong> However, there are also disadvantages to using wavelet transformation. The selection of an appropriate wavelet function and the determination of the correct level of decomposition are not straightforward and often require expert knowledge and experimentation. This can make the technique less accessible to non-experts. Additionally, the effectiveness of wavelet denoising can be highly dependent on the characteristics of the noise and signal in the data. In cases where noise and signal characteristics overlap significantly in the wavelet domain, denoising becomes more challenging and can lead to the loss of important signal information.</p>
</section>
</section>
<section id="identifying-and-removing-outliers" class="level3" data-number="16.1.3">
<h3 data-number="16.1.3" class="anchored" data-anchor-id="identifying-and-removing-outliers"><span class="header-section-number">16.1.3</span> Identifying and Removing Outliers</h3>
<p>Detecting and removing outliers is an important step in preparing data for machine learning, as outliers can significantly skew the results of from models. There are several methods for detecting and dealing with outliers. We briefly explain some of the commonly used methods.</p>
<section id="the-z-score" class="level4" data-number="16.1.3.1">
<h4 data-number="16.1.3.1" class="anchored" data-anchor-id="the-z-score"><span class="header-section-number">16.1.3.1</span> The Z-score</h4>
<p>The Z-score is a statistical measurement that describes a data point’s relation to the mean of a group of values, measured in terms of standard deviations from the mean. It’s used in outlier detection to identify data points that are unusually far from the mean.</p>
<p>The formula for calculating the Z-score of a data point is: <span class="math display">\[ Z = \frac{(X - \mu)}{\sigma},\]</span> where <span class="math inline">\(X\)</span> is the data point, <span class="math inline">\(\mu\)</span> is the mean of the data <span class="math inline">\(\sigma\)</span> is the standard deviation of the data.</p>
<p><strong>Example:</strong> Let’s say we have a dataset of test scores: <span class="math display">\[100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150.\]</span></p>
<p>In this dataset, most scores are around <span class="math inline">\(70-100\)</span>, but there’s a score of <span class="math inline">\(150\)</span>, which seems unusually high. We suspect it might be an outlier. We next compute the Z-score from this data, by first computing the mean and the standard deviation. The Z-score is then computed for each score using the above formula. Outliers are often determined by considering data points with Z-scores beyond +/- 2.5 or 3 as outliers, as they are significantly far from the mean.</p>
<p>A step-by-step process to compute Z-scores is presented next:</p>
<ol type="1">
<li><p><strong>Find the Mean</strong>: Mean <span class="math inline">\(\mu = \frac{\sum X}{n}\)</span></p></li>
<li><p><strong>Find the Standard Deviation</strong>: Standard Deviation <span class="math inline">\(\sigma = \sqrt{\frac{\sum (X - \mu)^2}{n}}\)</span></p></li>
<li><p><strong>Calculate Z-Scores for Each Data Point</strong>: <span class="math inline">\(Z = \frac{(X - \mu)}{\sigma}\)</span></p></li>
<li><p><strong>Identify Outliers</strong>: Any score with <span class="math inline">\(|Z| &gt; 3\)</span> (or another chosen threshold) is considered an outlier.</p></li>
</ol>
<p>Let’s perform these calculations for our example dataset. Based on our calculations, here are the test scores with their corresponding Z-scores:</p>
<table class="table">
<thead>
<tr class="header">
<th>Test Score</th>
<th>Z-Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>100</td>
<td>0.638</td>
</tr>
<tr class="even">
<td>95</td>
<td>0.437</td>
</tr>
<tr class="odd">
<td>90</td>
<td>0.237</td>
</tr>
<tr class="even">
<td>85</td>
<td>0.036</td>
</tr>
<tr class="odd">
<td>80</td>
<td>-0.164</td>
</tr>
<tr class="even">
<td>75</td>
<td>-0.365</td>
</tr>
<tr class="odd">
<td>70</td>
<td>-0.565</td>
</tr>
<tr class="even">
<td>65</td>
<td>-0.766</td>
</tr>
<tr class="odd">
<td>60</td>
<td>-0.966</td>
</tr>
<tr class="even">
<td>55</td>
<td>-1.167</td>
</tr>
<tr class="odd">
<td>150</td>
<td>2.643</td>
</tr>
</tbody>
</table>
<p>From this table, we can see that most scores have Z-scores within the range of -1 to 1, indicating that they are close to the mean. However, the test score of 150 has a Z-score of approximately 2.643, which is significantly higher than our outlier threshold of 2.5 or 3.</p>
<p>Therefore, according to this method, the test score of 150 can be considered an outlier. To remove it, we would simply exclude this data point from our dataset before proceeding with further analysis or machine learning modeling. This helps in ensuring that our model is not unduly influenced by this extreme value.</p>
<p>Python implementation is provided next:</p>
<pre class="{python}"><code>import numpy as np

# Dataset of test scores
data = np.array([100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150])

# Calculate the mean and standard deviation
mean = np.mean(data)
std_dev = np.std(data)

# Calculate Z-scores
z_scores = (data - mean) / std_dev

# Combine data, Z-scores for easier interpretation
data_with_z_scores = np.column_stack((data, z_scores))

# Display the data with their corresponding Z-scores
data_with_z_scores</code></pre>
</section>
<section id="interquartile-range-iqr" class="level4" data-number="16.1.3.2">
<h4 data-number="16.1.3.2" class="anchored" data-anchor-id="interquartile-range-iqr"><span class="header-section-number">16.1.3.2</span> Interquartile Range (IQR)</h4>
<p>The Interquartile Range (IQR) is another statistical method used to detect and remove outliers. It is particularly useful because it is less affected by extremes than methods relying on the mean and standard deviation, like the Z-score. IQR is the range between the first quartile (Q1, the 25th percentile) and the third quartile (Q3, the 75th percentile) in a dataset. It represents the middle 50% of the data. The IQR is calculated as: <span class="math display">\[ \text{IQR} = Q3 - Q1.\]</span></p>
<p>Outliers are typically defined as observations that fall below <span class="math inline">\(Q1 - 1.5 \times \text{IQR}\)</span> or above <span class="math inline">\(Q3 + 1.5 \times \text{IQR}\)</span>. This rule of thumb is widely used but can be adjusted based on specific data characteristics or domain knowledge.</p>
<p>A step-by-step process to detect and remove outliers using IQR:</p>
<ol type="1">
<li><strong>Calculate Q1 and Q3</strong>: These are the 25th and 75th percentiles of the data, respectively.</li>
<li><strong>Calculate IQR</strong>: Subtract Q1 from Q3.</li>
<li><strong>Determine Outlier Thresholds</strong>:
<ul>
<li>Lower Bound = <span class="math inline">\(Q1 - 1.5 \times \text{IQR}\)</span></li>
<li>Upper Bound = <span class="math inline">\(Q3 + 1.5 \times \text{IQR}\)</span></li>
</ul></li>
<li><strong>Identify Outliers</strong>: Data points that fall outside of these bounds are considered outliers.</li>
<li><strong>Remove or Adjust Outliers</strong>: Once identified, outliers can be removed, capped, or adjusted based on the analytical requirements.</li>
</ol>
<p><strong>Example:</strong> Let us use the same dataset of test scores: <span class="math display">\[100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150.\]</span></p>
<p>We will calculate the IQR and use it to determine the outliers in this dataset. Based on the IQR method, here are the results for our test score dataset:</p>
<ul>
<li><strong>Lower Bound for Outliers</strong>: 30.0</li>
<li><strong>Upper Bound for Outliers</strong>: 130.0</li>
<li><strong>Identified Outliers</strong>: [150]</li>
<li><strong>Data Without Outliers</strong>: [100, 95, 90, 85, 80, 75, 70, 65, 60, 55]</li>
</ul>
<p>In this case, the score of 150 is identified as an outlier, as it is above the upper bound of 130.0. According to the IQR method, this data point can be considered for removal or adjustment.</p>
<p>The process of removing the outlier involves excluding the score of 150 from the dataset. This leaves us with the scores [100, 95, 90, 85, 80, 75, 70, 65, 60, 55], which are more representative of the typical range of scores in this dataset. By doing this, we reduce the potential impact of extreme values on further analysis or machine learning modeling.</p>
<p>The following Python code shows the computations involved.</p>
<pre class="{python}"><code>import numpy as np

# Re-defining the dataset of test scores
data = np.array([100, 95, 90, 85, 80, 75, 70, 65, 60, 55, 150])

# Calculate Q1 and Q3
Q1 = np.percentile(data, 25)
Q3 = np.percentile(data, 75)

# Calculate IQR
IQR = Q3 - Q1

# Determine outlier thresholds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Identifying outliers
outliers = data[(data &lt; lower_bound) | (data &gt; upper_bound)]

# Data without outliers
data_without_outliers = data[(data &gt;= lower_bound) &amp; (data &lt;= upper_bound)]

lower_bound, upper_bound, outliers, data_without_outliers
</code></pre>
</section>
<section id="isolation-forest-algorithm" class="level4" data-number="16.1.3.3">
<h4 data-number="16.1.3.3" class="anchored" data-anchor-id="isolation-forest-algorithm"><span class="header-section-number">16.1.3.3</span> Isolation Forest Algorithm</h4>
<p>The Isolation Forest algorithm is an effective method for detecting outliers, especially in large datasets. It’s particularly useful in unsupervised learning scenarios where you don’t have labeled data to train on.</p>
<p>The key concept behind Isolation Forest is that outliers are few and different, and hence, they are ‘easier’ to isolate compared to non-outlier points. In other words, it requires fewer random partitions to isolate an outlier than to isolate a regular data point.</p>
<p>It involves the following steps:</p>
<ol type="1">
<li><p><strong>Random Sampling:</strong> The algorithm randomly samples a subset of the data. This makes the method efficient for large datasets.</p></li>
<li><p><strong>Building Isolation Trees (iTrees):</strong> For each sample, it randomly selects a feature and then randomly selects a split value between the maximum and minimum values of the selected feature. This process partitions the dataset, and it’s repeated recursively, leading to the formation of an ‘isolation tree’. Each node in the tree isolates data points from the rest of the sample.</p></li>
<li><p><strong>Path Lengths:</strong> The number of splits required to isolate a sample is recorded as the ‘path length’. Since outliers are ‘easier’ to isolate, they tend to have shorter path lengths.</p></li>
<li><p><strong>Ensemble of iTrees:</strong> Multiple isolation trees are created to form a random forest. The path lengths across these trees are then averaged to get a final score.</p></li>
<li><p><strong>Scoring Anomalies:</strong> The average path length is used to calculate an anomaly score for each data point. A shorter path length results in a higher anomaly score, indicating a higher likelihood of the point being an outlier.</p></li>
</ol>
<p>Here is a Python code demonstrating detection of outliers using the algorithm.</p>
<pre class="{python}"><code>#| fig-cap: Detection of outliers using the Isolation Forest algorithm. 
#| layout-ncol: 1
#| label: fig-ifa
from sklearn.ensemble import IsolationForest
import numpy as np
import matplotlib.pyplot as plt

# Generating a synthetic dataset
np.random.seed(42)
X = 0.3 * np.random.randn(100, 2)
X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
X = np.r_[X + 2, X - 2, X_outliers]

# Fitting the model
clf = IsolationForest(max_samples=100, random_state=42)
clf.fit(X)

# Predictions
y_pred = clf.predict(X)

# Visualization
plt.figure(figsize=(10, 6))
plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='coolwarm', edgecolor='k', s=50)
plt.title("Isolation Forest Outlier Detection")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()</code></pre>
<p>The above Python code demonstrates the use of the Isolation Forest algorithm for outlier detection in a synthetic dataset. We first create a synthetic dataset by generating random data points using a normal distribution (<code>np.random.randn</code>) and then scaling them by a factor of 0.3. To introduce some variability, the generated points are shifted to create two clusters (<code>X + 2</code> and <code>X - 2</code>). Additionally, a set of outlier points is created using a uniform distribution over a wider range (<code>np.random.uniform</code>). These outlier points are then combined with the original data to form the complete dataset (<code>X</code>). An Isolation Forest model (<code>IsolationForest</code>) is instantiated with a specified <code>max_samples</code> parameter (100 in this case, which is the number of samples to draw from the dataset to train each base estimator in the forest). The model is then fitted to the dataset using the <code>fit</code> method. After fitting the model, it’s used to predict the nature of each data point in the dataset (<code>y_pred</code>). The prediction results in a label for each point, where <code>1</code> indicates a normal point (inlier), and <code>-1</code> indicates an outlier.</p>
<p><a href="data_pre_processing.html#fig-ifa">Figure&nbsp;<span>10.7</span></a> demonstrates how the Isolation Forest algorithm separates outliers from normal observations. The outliers, which are fewer and more scattered, are identified and marked distinctly from the denser clusters of normal data points. This demonstration provides a practical insight into the algorithm’s capability in handling unsupervised outlier detection in a dataset.</p>
<p><strong>Advantages:</strong> The Isolation Forest algorithm offers several distinct advantages, making it a popular choice for outlier detection, especially in large and complex datasets. One of its primary strengths is efficiency: it can handle large datasets and high-dimensional data effectively due to its sampling strategy and the simplicity of isolation trees. Unlike many traditional statistical methods, the Isolation Forest does not assume a normal distribution of data, making it versatile and robust in handling various types of datasets. This characteristic is particularly beneficial in real-world scenarios where data often deviates from theoretical distributions.</p>
<p>Another advantage of the Isolation Forest is its resilience to the swamping and masking effects. Traditional outlier detection methods can sometimes misclassify normal observations as outliers (swamping) or fail to detect actual outliers (masking). The Isolation Forest algorithm, with its unique approach of isolating anomalies, tends to be less prone to these issues, leading to more accurate detection.</p>
<p>Furthermore, the algorithm’s random forest approach contributes to its effectiveness. By creating multiple isolation trees and averaging their results, the algorithm can achieve a more reliable and stable outlier detection, mitigating the randomness that might arise from any single tree. This ensemble technique enhances the overall accuracy of the model. Lastly, the Isolation Forest is relatively straightforward to implement and does not require extensive parameter tuning, which adds to its practicality in various applications, from fraud detection to anomaly detection in network traffic or sensor data monitoring.</p>
<p><strong>Limitations:</strong> The Isolation Forest algorithm, though effective for outlier detection, has several disadvantages. Firstly, its reliance on random selection for creating isolation trees introduces a degree of unpredictability, potentially leading to inconsistent results across different runs. This randomness can affect the stability and reproducibility of the model, which is a significant concern in applications requiring consistent performance. Secondly, the algorithm’s performance is highly sensitive to its parameters, such as the number of trees in the forest and the sample size. Selecting these parameters appropriately requires a deep understanding of the dataset and can involve a trial-and-error approach, which may not always be feasible.</p>
<p>Additionally, while Isolation Forest is generally efficient, its performance can degrade with extremely large datasets, particularly those with a high number of features. This can lead to increased computational demands, making the algorithm less suitable for very large-scale applications. Another limitation is the interpretability of the results; understanding why a specific data point is identified as an outlier is not straightforward with Isolation Forest, which can be a drawback in scenarios where explanation and transparency are crucial.</p>
<p>Moreover, the algorithm’s effectiveness is predominantly in numerical data contexts and may not perform as well with categorical data or mixed data types without adequate preprocessing. Also, it operates purely on statistical properties, lacking contextual sensitivity, which means it might not align with domain-specific definitions of outliers. In summary, while the Isolation Forest algorithm is a valuable tool for detecting outliers, its application requires careful consideration of its limitations regarding randomness, parameter sensitivity, scalability, interpretability, and data type compatibility.</p>
<p><strong>Note on Removing Outliers:</strong> It’s important to note that Isolation Forest is primarily a method for detecting outliers, not necessarily removing them. The decision to remove outliers identified by the algorithm should be made based on the context and the impact of these outliers on the subsequent analysis or machine learning models.</p>
</section>
<section id="dbscan-density-based-spatial-clustering-of-applications-with-noise" class="level4" data-number="16.1.3.4">
<h4 data-number="16.1.3.4" class="anchored" data-anchor-id="dbscan-density-based-spatial-clustering-of-applications-with-noise"><span class="header-section-number">16.1.3.4</span> DBSCAN (Density-Based Spatial Clustering of Applications with Noise)</h4>
<p>DBSCAN, which stands for Density-Based Spatial Clustering of Applications with Noise, is a widely used clustering algorithm in the realms of machine learning and data analysis. The cornerstone of DBSCAN is the concept of clustering, which involves grouping data points in such a way that points in the same cluster are more similar to each other than to those in other clusters. This method is particularly effective for discovering patterns and groupings within data.</p>
<p>A key feature distinguishing DBSCAN from other clustering methods like K-Means is its focus on density-based clustering. Rather than grouping points based on distance from a central point, as in K-Means, DBSCAN forms clusters based on the density of data points. This approach enables the identification of clusters with arbitrary shapes and even the detection of nested clusters.</p>
<p>At the heart of DBSCAN are three core concepts:</p>
<ol type="1">
<li><strong>Core Points</strong>: These are points that have a minimum number of other points (specified as MinPts) within a certain radius (<span class="math inline">\(\epsilon\)</span>, or epsilon).</li>
<li><strong>Border Points</strong>: These are points that fall within the radius of a core point but have fewer than MinPts within their own radius.</li>
<li><strong>Noise Points</strong>: Points that do not qualify as either core or border points are categorized as noise or outliers.</li>
</ol>
<p>The DBSCAN algorithm operates by starting with an arbitrary point and identifying all points that are density-reachable from this point, based on the <span class="math inline">\(\epsilon\)</span> and MinPts parameters. If the starting point is a core point, a cluster is formed. If it’s a border point, with no density-reachable points, the algorithm proceeds to the next point. This process continues until each point in the dataset is classified as either a core point, border point, or noise. This method’s ability to handle noise and identify clusters of varying shapes and densities makes it a versatile and powerful tool in data analysis.</p>
<p><strong>Advantages</strong>: DBSCAN offers several significant advantages as a clustering algorithm. First and foremost, its ability to identify clusters of arbitrary shapes is a major strength, especially when dealing with complex spatial data. This contrasts with algorithms like K-Means, which tend to identify only spherical clusters. Secondly, DBSCAN is particularly adept at handling noise and outliers in the data set. It can effectively differentiate between core points (dense areas of the dataset), border points (edges of clusters), and noise, providing a more accurate clustering result in noisy environments. Another advantage of DBSCAN is that it does not require the number of clusters to be defined a priori, unlike K-Means. This makes it particularly useful in exploratory data analysis where the number of clusters is not known beforehand. Additionally, DBSCAN’s minimal reliance on domain knowledge for setting its two main parameters (radius and minimum number of points) simplifies its application across various domains. Lastly, DBSCAN’s capability to discover clusters within clusters allows for a deeper and more nuanced understanding of the data structure, making it a versatile tool for a wide range of data analysis tasks. These advantages make DBSCAN a popular choice for many real-world applications, including image processing, genetics, and spatial data analysis.</p>
<p><strong>Limitations:</strong> DBSCAN, while versatile, has certain limitations that can affect its performance and applicability. One of the primary challenges lies in determining the appropriate values for its two main parameters: epsilon (<span class="math inline">\(\epsilon\)</span>), which defines the radius around a point to search for neighboring points, and MinPts, the minimum number of points required to form a dense region. The choice of these parameters can significantly influence the clustering outcome, and finding the right balance often requires domain knowledge or trial-and-error, which can be impractical in many situations. Another limitation is DBSCAN’s sensitivity to varying densities within the same dataset. It may struggle to identify clusters correctly if there are significant differences in the density of clusters, leading to either fragmented clusters or over-merging of clusters. Additionally, DBSCAN’s performance can degrade with high-dimensional data due to the curse of dimensionality, as the notion of density becomes less meaningful in high-dimensional spaces. Finally, the computational complexity of DBSCAN, particularly with large datasets, can be a drawback. It requires calculating the distance between points, which can be computationally intensive for large datasets, although optimized implementations can mitigate this to some extent. Despite these limitations, DBSCAN remains a popular and effective clustering method for datasets where its unique advantages can be fully leveraged.</p>
<p>Below is an example of how to use the DBSCAN algorithm for clustering in Python using the <code>scikit-learn</code> library.</p>
<pre class="{python}"><code>#| fig-cap: Clustering of complicated data using DBSCAN.
#| layout-ncol: 1
#| label: fig-dbscan
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn import datasets

# Step 1: Generate sample data
X, _ = datasets.make_moons(n_samples=300, noise=0.05, random_state=0)

# Step 2: Apply DBSCAN
# epsilon (eps) is the maximum distance between two samples for them to be considered in the same neighborhood
# min_samples is the number of samples in a neighborhood for a point to be considered a core point
dbscan = DBSCAN(eps=0.3, min_samples=10)
clusters = dbscan.fit_predict(X)

# Step 3: Visualize the results
plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='Paired', marker='o', s=30, edgecolor='k')
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
<p>In this script, we first generate a two-dimensional dataset using <code>make_moons</code> from <code>sklearn.datasets</code>. We then apply the DBSCAN algorithm to this data, where <code>eps</code> and <code>min_samples</code> are key parameters. <a href="data_pre_processing.html#fig-dbscan">Figure&nbsp;<span>10.8</span></a> shows the results where each cluster is colored differently, and outliers (points not belonging to any cluster) are usually shown in a different color.</p>
<p>The next code shows outlier detection using DBSCAN.</p>
<pre class="{python}"><code>#| fig-cap: Outlier detection using DBSCAN.
#| layout-ncol: 1
#| label: fig-dbscan_outlier
#| 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_blobs

# Generate sample data
centers = [[1, 1], [-1, -1], [1, -1]]
X, labels_true = make_blobs(n_samples=300, centers=centers, cluster_std=0.4, random_state=0)

# Add some noise/outlier points
np.random.seed(42)
outliers = np.random.uniform(low=-3, high=3, size=(20, 2))
X = np.concatenate([X, outliers])

# Apply DBSCAN
dbscan = DBSCAN(eps=0.3, min_samples=10)
labels = dbscan.fit_predict(X)

# Identify core and noise points
core_samples_mask = np.zeros_like(labels, dtype=bool)
core_samples_mask[dbscan.core_sample_indices_] = True
unique_labels = set(labels)

# Plot the results
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black is used for noise/outliers
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    # Plot core points
    xy = X[class_member_mask &amp; core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)

    # Plot outliers
    xy = X[class_member_mask &amp; ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)

plt.title('DBSCAN: Outlier Detection')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()</code></pre>
<p>In the above Python code, the process of outlier detection using the DBSCAN algorithm is illustrated through a series of steps. Initially, a sample dataset is created using the <code>make_blobs</code> function, which generates clusters of data points. To simulate a real-world scenario where data might contain anomalies, additional random points are introduced into this dataset, serving as potential outliers. Following the creation of this dataset, the DBSCAN algorithm is applied. DBSCAN operates by identifying clusters based on the density of data points, and it labels points that do not belong to any cluster as -1, effectively categorizing them as outliers. After the application of DBSCAN, the results are shown in <a href="data_pre_processing.html#fig-dbscan_outlier">Figure&nbsp;<span>10.9</span></a>. This plot distinctly shows the core points (the dense central parts of clusters), border points (points on the edges of clusters), and outliers. The outliers, which are points identified as not belonging to any cluster, are typically highlighted in black. This visual representation aids in understanding how DBSCAN segregates outliers from the main clusters in the dataset, showcasing its utility in outlier detection.</p>
</section>
<section id="outliers-and-rare-events" class="level4" data-number="16.1.3.5">
<h4 data-number="16.1.3.5" class="anchored" data-anchor-id="outliers-and-rare-events"><span class="header-section-number">16.1.3.5</span> Outliers and Rare Events</h4>
<p>Outlier detection, a critical task in data analysis, faces several challenges, particularly in identifying and correctly interpreting rare events. An outlier is typically a data point that significantly deviates from the majority of data, suggesting a different underlying mechanism. Rare events are similar but are specifically those events that occur infrequently in the dataset. For instance, fraud in financial transactions or the failure of a critical component in a manufacturing process are rare events – they don’t happen often, but when they do, their impact can be substantial.</p>
<p><strong>Challenges in Outlier Detection:</strong> Outlier detection in data analysis encounters several notable challenges, particularly in specific contexts. One such challenge is high dimensionality. In spaces with many dimensions, the task of distinguishing between mere noise and genuine outliers becomes increasingly complex. This complexity arises from the “curse of dimensionality,” a phenomenon where the conventional measure of distance between data points loses its effectiveness as the number of dimensions increases.</p>
<p>Another challenge is the presence of varying densities within datasets. In such cases, a data point that is an outlier in a less dense cluster might be considered normal in a denser cluster. This variability complicates the establishment of a universal threshold for identifying outliers, as what qualifies as an outlier can differ significantly from one region of the dataset to another.</p>
<p>Additionally, the phenomena of masking and swamping add to the complexity. Masking occurs when the presence of certain outliers obscures or hides other outliers, while swamping refers to the misidentification of normal points as outliers because they are in close proximity to actual outliers.</p>
<p>Lastly, the challenge of evolving data is significant, especially in dynamic datasets where patterns and trends change over time, a concept known as concept drift. In such environments, the definition of what constitutes an outlier is not static but changes as the underlying data patterns evolve. This necessitates a continuous adaptation of the outlier detection models to remain effective in identifying true anomalies over time. These challenges underscore the need for sophisticated and adaptable methods in outlier detection to accurately discern genuine anomalies in complex datasets.</p>
<p><strong>Missing Rare Events:</strong> The challenges in detecting rare events in data analysis and machine learning are multifaceted, primarily due to their inherent characteristics. One of the most prominent challenges is their low frequency of occurrence. By nature, rare events occur infrequently within datasets, leading to a tendency for them to be overlooked or misclassified as mere anomalies. This infrequency poses a significant challenge in accurately identifying and assessing the impact of these events.</p>
<p>Another critical issue is the lack of representation in training data for machine learning models. In many scenarios, the training datasets do not adequately capture these rare events, leading to models that are not well-equipped to recognize them when they do occur. The scarcity of examples of rare events in the training data results in a learning process that is heavily biased towards more frequent occurrences, thereby diminishing the model’s ability to identify rare events accurately.</p>
<p>Furthermore, the challenge of balancing sensitivity and specificity in the context of rare events is a delicate task. Sensitivity refers to the model’s ability to correctly detect most of the rare events, whereas specificity relates to its ability not to mislabel normal events as rare. Striking the right balance is crucial; overly sensitive models might flag too many normal occurrences as rare events, causing false alarms, while models with excessive specificity might miss genuine rare events. This balancing act is particularly crucial in fields where the implications of missing a rare event or falsely identifying one are significant, such as in medical diagnostics, financial fraud detection, or security systems.</p>
<p>These challenges highlight the need for specialized approaches in data analysis and model training that can accommodate the unique nature of rare events, ensuring that these critical but infrequent occurrences are neither missed nor wrongly classified.</p>
<p><strong>Examples of Rare Events:</strong> Rare events in various engineering fields, particularly in aerospace, mechanical, and other disciplines, often involve scenarios where the low probability of occurrence belies the potential for severe consequences. Here are some examples, along with explanations of their nature and the catastrophes that ensued from missed rare events:</p>
<ol type="1">
<li><strong>Civil Engineering - Tacoma Narrows Bridge Collapse (1940)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: The bridge collapsed due to aeroelastic flutter caused by wind.</li>
<li><strong>Catastrophe</strong>: The bridge, nicknamed “Galloping Gertie,” was vulnerable to this rare phenomenon, which was not well-understood at the time. The collapse of the bridge was a significant event in engineering, leading to changes in the design and construction of suspension bridges to prevent similar incidents.</li>
</ul></li>
<li><strong>Chemical Engineering - Bhopal Gas Tragedy (1984)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: A leak of methyl isocyanate gas and other chemicals from a pesticide plant.</li>
<li><strong>Catastrophe</strong>: Widely considered the world’s worst industrial disaster, it resulted in thousands of deaths and long-term health effects for many more. This tragedy underscored the catastrophic potential of rare chemical leaks and the need for stringent safety measures.</li>
</ul></li>
<li><strong>Electrical Engineering - Northeast Blackout (2003)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: A large-scale power outage occurred due to a software bug in an alarm system, coupled with operational errors.</li>
<li><strong>Catastrophe</strong>: This event led to a widespread electricity blackout across parts of the Northeastern and Midwestern United States and Ontario, Canada, affecting around 55 million people. It exposed vulnerabilities in the electrical grid and the cascading effect of small failures.</li>
</ul></li>
<li><strong>Mechanical Engineering - Fukushima Daiichi Nuclear Disaster (2011)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: This disaster was triggered by a massive tsunami following a strong earthquake.</li>
<li><strong>Catastrophe</strong>: The tsunami led to the failure of nuclear reactors’ cooling systems, causing meltdowns, hydrogen-air explosions, and the release of radioactive material. The rarity of such a large-scale natural disaster, coupled with the unpreparedness for such an event, resulted in one of the worst nuclear disasters in history.</li>
</ul></li>
</ol>
<p>These examples from different engineering disciplines demonstrate how rare events, often overlooked due to their low probability, can lead to significant disasters. They underscore the importance of considering even unlikely scenarios in engineering design and risk management to prevent catastrophic outcomes.</p>
<p>Aerospace engineering, with its complex systems and high stakes, has witnessed several rare events that had significant consequences. These events, often considered low-probability, underscore the critical importance of meticulous design, rigorous testing, and comprehensive risk management in the field. Here are some examples from aerospace engineering:</p>
<ol type="1">
<li><strong>Apollo 13 Oxygen Tank Explosion (1970)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: An oxygen tank in the service module of Apollo 13 failed due to an electrical fault, causing an explosion.</li>
<li><strong>Catastrophe</strong>: This incident crippled the spacecraft during its mission to the Moon, leading to a critical situation for the astronauts aboard. The mission was aborted, and a safe return to Earth became the primary objective. The event highlighted the need for redundant systems and rigorous pre-flight testing in space missions.</li>
</ul></li>
<li><strong>Space Shuttle Challenger Disaster (1986)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: The disaster was primarily caused by the failure of an O-ring seal in its right solid rocket booster.</li>
<li><strong>Catastrophe</strong>: The O-ring failure, exacerbated by unusually cold weather, led to the breakup of the space shuttle shortly after launch, resulting in the death of all seven crew members. This event highlighted the catastrophic consequences of underestimating the likelihood and impact of what was considered a rare component failure.</li>
</ul></li>
<li><strong>Soyuz 11 Crew Loss (1971)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: The Soyuz 11 spacecraft depressurized during its return to Earth.</li>
<li><strong>Catastrophe</strong>: The three crew members aboard died due to asphyxiation, marking the only human deaths in space (above the Kármán line). The tragedy led to redesigned safety features and protocols in Soviet and later Russian crewed space missions.</li>
</ul></li>
<li><strong>Mars Climate Orbiter Loss (1999)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: The spacecraft was lost due to a navigation error caused by a failure to convert units from English to metric.</li>
<li><strong>Catastrophe</strong>: The orbiter approached Mars at a lower altitude than planned and disintegrated due to atmospheric stresses. This incident underscored the critical importance of thorough cross-checks and consistent units in engineering calculations, especially in international collaborations.</li>
</ul></li>
<li><strong>Space Shuttle Columbia Disaster (2003)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: Damage to the thermal protection system of the Space Shuttle Columbia during launch led to its disintegration upon re-entry.</li>
<li><strong>Catastrophe</strong>: All seven crew members were lost. The investigation following the disaster led to a 29-month suspension of the Space Shuttle program and a reevaluation of safety protocols and shuttle integrity inspections.</li>
</ul></li>
<li><strong>Air France Flight 447 Crash (2009)</strong>:
<ul>
<li><strong>Nature of Rare Event</strong>: The crash was primarily caused by the aircraft’s speed sensors being obstructed by ice crystals, leading to a series of erroneous readings and pilot errors.</li>
<li><strong>Catastrophe</strong>: The aircraft crashed into the Atlantic Ocean, resulting in the deaths of all passengers and crew. This event brought attention to the importance of pilot training in handling high-altitude stalls and the need for more reliable airspeed sensors.</li>
</ul></li>
</ol>
<p>These incidents in aerospace engineering, though rare, had profound impacts on safety protocols, design considerations, and operational procedures. Each event prompted a reevaluation of existing practices, leading to advancements in technology and an increased emphasis on safety in aerospace engineering.</p>
<p>In the realm of autonomy, particularly in autonomous vehicles and systems, recent times have seen several rare events. These events are significant as they highlight the challenges and potential risks associated with the integration of autonomous technology in various sectors. Some of these rare events include:</p>
<ol type="1">
<li><strong>Autonomous Vehicle Accidents</strong>:
<ul>
<li><strong>Event Description</strong>: There have been instances where autonomous or semi-autonomous vehicles were involved in accidents, some of which were fatal.</li>
<li><strong>Significance</strong>: These incidents have raised questions about the readiness of autonomous driving technology for widespread use, the reliability of sensors and algorithms in complex, real-world environments, and the need for regulatory frameworks.</li>
</ul></li>
<li><strong>Drone Mishaps in Controlled Airspace</strong>:
<ul>
<li><strong>Event Description</strong>: Drones entering controlled or restricted airspace have caused disruptions, including near-miss incidents with manned aircraft and temporary shutdowns of major airports.</li>
<li><strong>Significance</strong>: These events underscore the challenges in integrating unmanned aerial systems into existing airspace, particularly regarding safety and coordination with manned aircraft operations.</li>
</ul></li>
<li><strong>Robotic System Failures in Healthcare</strong>:
<ul>
<li><strong>Event Description</strong>: There have been rare but notable instances where autonomous or robotic systems in healthcare settings malfunctioned, leading to incorrect diagnoses or complications in surgeries.</li>
<li><strong>Significance</strong>: These cases highlight the importance of stringent testing, validation, and oversight in the deployment of autonomous systems in sensitive and high-stakes fields like healthcare.</li>
</ul></li>
<li><strong>Security Breaches in Autonomous Systems</strong>:
<ul>
<li><strong>Event Description</strong>: Cybersecurity incidents involving autonomous systems, including hacking or manipulation of autonomous vehicles or industrial automation systems.</li>
<li><strong>Significance</strong>: These breaches point to the critical need for robust cybersecurity measures in autonomous systems, given their potential impact on safety and privacy.</li>
</ul></li>
<li><strong>AI Ethics and Bias Incidents</strong>:
<ul>
<li><strong>Event Description</strong>: Instances where AI systems, including autonomous decision-making algorithms, have demonstrated biases or ethical issues, such as in hiring practices or law enforcement.</li>
<li><strong>Significance</strong>: These events raise concerns about the ethical implications of autonomous systems, the need for unbiased data, and the importance of incorporating ethical considerations into AI development.</li>
</ul></li>
<li><strong>Failures in Autonomous Trading Algorithms</strong>:
<ul>
<li><strong>Event Description</strong>: Rare cases where autonomous trading algorithms caused significant market disruptions or losses due to unexpected behaviors under unusual market conditions.</li>
<li><strong>Significance</strong>: Such events draw attention to the risks associated with high-frequency, autonomous trading in financial markets and the need for comprehensive risk management strategies.</li>
</ul></li>
</ol>
<p>These rare events in autonomy highlight the complexities and potential risks associated with advancing autonomous technologies. They emphasize the need for ongoing research, development, and regulation to ensure safety, reliability, and ethical considerations are adequately addressed as these technologies become more integrated into daily life.</p>
<p>In summary, the primary challenges in outlier detection and the identification of rare events lie in the definition, variability of data, and limitations inherent to data analysis methods. Understanding these challenges is crucial for designing effective detection systems, especially in fields where rare events, while infrequent, can have significant consequences.</p>
</section>
</section>
</section>
<section id="survivorship-bias" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="survivorship-bias"><span class="header-section-number">16.2</span> Survivorship Bias</h2>
<p>Survivorship bias is a logical fallacy that occurs when a person focuses on the people or things that “survived” some process and inadvertently overlooks those that did not because of their lack of visibility. This bias can lead to false conclusions because the failures, which are not seen, are not considered. Here are some real-world examples:</p>
<ol type="1">
<li><p><strong>World War II Aircraft Analysis</strong>: During World War II, military analysts examined returning aircraft for bullet holes to determine which areas needed additional armor. However, the analysis initially suffered from survivorship bias. They were only looking at planes that made it back and not considering the ones that were shot down. The areas where the returning planes were hit were actually the strongest parts, as evidenced by their ability to return. The planes that didn’t return were likely hit in different places, which were the areas that actually needed reinforcing.</p></li>
<li><p><strong>Stock Market Investment Strategies</strong>: In the financial world, survivorship bias can be seen when analyzing investment funds. If one only considers funds that are currently successful or still in existence, they might conclude that investing in funds is generally a profitable venture. This ignores all the funds that failed and closed, which could show a more risky and volatile market.</p></li>
<li><p><strong>Success Stories in Entrepreneurship</strong>: The media often highlights successful entrepreneurs, leading to a perception that entrepreneurial endeavors are more likely to succeed than they actually are. This ignores the vast majority of startups that fail, and it can lead aspiring entrepreneurs to underestimate the risks involved.</p></li>
<li><p><strong>Self-Help and Motivational Literature</strong>: Books and articles often focus on successful individuals and their routines or habits, implying that emulating these will lead to similar success. This ignores the many people who may have had the same habits but did not achieve the same level of success, often due to factors outside their control.</p></li>
<li><p><strong>Educational Institution Alumni Success</strong>: Universities and colleges often showcase their most successful alumni as representations of the potential outcomes of their education. However, this can create a skewed perception, as it doesn’t account for the majority of graduates who may not achieve the same level of fame or success.</p></li>
<li><p><strong>Success Stories of College Dropouts</strong>: The media and popular culture often highlight the success stories of famous dropouts like Bill Gates, Steve Jobs, and Mark Zuckerberg, who achieved extraordinary success despite not completing their formal education. While these narratives are inspiring, they represent a very small fraction of dropouts and contribute to survivorship bias. The reality is that most people who leave school early do not achieve such high levels of success; instead, they are more likely to face economic and social challenges, including lower incomes and higher unemployment rates. This skewed portrayal can lead to a misconception that formal education is not essential for success, potentially influencing young people to undervalue education and overlook the risks associated with dropping out. It’s important to balance these exceptional stories with the more common and less visible outcomes of leaving school early.</p></li>
</ol>
<p>Survivorship bias leads to a distorted view of reality by emphasizing the winners and ignoring the losers. It’s important in decision-making and analysis to consider the full picture, including those who didn’t “survive” the process being examined.</p>
<p>Survivorship bias can significantly impact data-driven machine learning in several ways, primarily by skewing the training data and leading to models that are not representative of the real world. Here’s how this bias can affect machine learning:</p>
<ol type="1">
<li><p><strong>Incomplete Training Data</strong>: Survivorship bias in machine learning occurs when the training data includes only successful cases or ‘survivors’, while failing to account for failures or ‘non-survivors’. For instance, if a model is trained only on successful retail companies, it might not learn the patterns that lead to the failure of retail businesses.</p></li>
<li><p><strong>Misleading Model Performance</strong>: Models trained on data affected by survivorship bias might show deceptively high performance during training and testing but fail in real-world applications. This happens because the model has not learned from the complete range of scenarios, especially those that led to failures or negative outcomes.</p></li>
<li><p><strong>Biased Predictions and Decisions</strong>: The bias can lead to models that are biased towards the characteristics of the ‘survivors’. For example, in credit scoring, if the model is trained only on data from borrowers who haven’t defaulted, it might underestimate the risk of default.</p></li>
<li><p><strong>Overlooking Crucial Factors</strong>: Survivorship bias can cause important variables and factors leading to failure to be overlooked. In healthcare, for instance, if a model is trained only on patients who have survived a certain disease, it might miss critical symptoms or factors that are common among those who did not survive.</p></li>
<li><p><strong>Difficulty in Generalization</strong>: The models become less generalizable to wider, more diverse scenarios. They become overfitted to the successful cases, which could be a small, non-representative sample of the overall population.</p></li>
<li><p><strong>Ethical and Fairness Concerns</strong>: This bias can also raise ethical concerns, particularly in applications where it’s important to represent and serve a diverse range of individuals fairly, such as in hiring or loan approval processes.</p></li>
</ol>
<p>To mitigate the impact of survivorship bias in machine learning, it’s crucial to use comprehensive and representative datasets that include both successes and failures. Additionally, continuous monitoring and validation of the model’s performance in real-world scenarios are essential to ensure that it remains accurate and fair over time.</p>
</section>
<section id="data-transformation" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="data-transformation"><span class="header-section-number">16.3</span> Data Transformation</h2>
<pre><code>- **Normalization and Standardization**: Rescaling the data to a specific range (like 0 to 1 for normalization) or changing the distribution to have a mean of 0 and a standard deviation of 1 (standardization).
- **Scaling to Unit Length**: Rescaling data so that the length of each data point (considered as a vector) is 1.
- **Power Transforms**: Applying transformations like logarithmic, square root, or exponential to stabilize variance and make the data more 'normal' (Gaussian).</code></pre>
</section>
<section id="data-reduction" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="data-reduction"><span class="header-section-number">16.4</span> Data Reduction</h2>
<pre><code>- **Dimensionality Reduction**: Techniques like Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or t-SNE are used to reduce the number of features while retaining most of the information.
- **Feature Selection**: Selecting the most significant features based on various statistical criteria. Methods include filter, wrapper, and embedded methods.
- **Binning**: Grouping a set of numerical values into a smaller number of bins (or intervals).</code></pre>
</section>
<section id="data-encoding" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="data-encoding"><span class="header-section-number">16.5</span> Data Encoding</h2>
<pre><code>- **Encoding Categorical Data**: Converting categorical data into numerical format using methods like one-hot encoding, label encoding, or binary encoding.
- **Text Encoding**: Transforming text data into numerical format using techniques like Bag of Words, TF-IDF, or word embeddings (like Word2Vec).</code></pre>
</section>
<section id="handling-imbalanced-data" class="level2" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="handling-imbalanced-data"><span class="header-section-number">16.6</span> Handling Imbalanced Data</h2>
<pre><code>- Applying techniques like oversampling the minority class, undersampling the majority class, or using synthetic data generation methods like SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.</code></pre>
</section>
<section id="feature-engineering" class="level2" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="feature-engineering"><span class="header-section-number">16.7</span> Feature Engineering</h2>
<pre><code>- Creating new features or modifying existing ones to improve model performance. This can involve extracting information from dates, creating interaction features, or aggregating data to create summary statistics.</code></pre>
</section>
<section id="time-series-specific-techniques" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="time-series-specific-techniques"><span class="header-section-number">16.8</span> Time Series Specific Techniques</h2>
<pre><code>- If dealing with time series data, techniques like windowing, lag features, and handling seasonality might be necessary.</code></pre>
</section>
<section id="data-integration" class="level2" data-number="16.9">
<h2 data-number="16.9" class="anchored" data-anchor-id="data-integration"><span class="header-section-number">16.9</span> Data Integration</h2>
<pre><code>- Combining data from multiple sources, which might involve format alignment, resolving data conflicts, and schema integration.</code></pre>
<p>Each of these techniques must be chosen and applied based on the specific requirements of the dataset and the problem at hand. The goal is always to produce a clean, reliable dataset that can be effectively used for machine learning modeling.</p>
</section>
</section>
<section id="deep-learning" class="level1" data-number="17">
<h1 data-number="17"><span class="header-section-number">17</span> Deep Learning</h1>
</section>
<section id="dimensionality-reduction" class="level1" data-number="18">
<h1 data-number="18"><span class="header-section-number">18</span> Dimensionality Reduction</h1>
<section id="pca" class="level2" data-number="18.1">
<h2 data-number="18.1" class="anchored" data-anchor-id="pca"><span class="header-section-number">18.1</span> PCA</h2>
<section id="examples" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="examples">Examples</h3>
<p>We next consider an image approximation (or compression) example where matrix basis are used. Consider the following image:</p>
<p>```{python,results=‘hide’} #| echo: false import cv2 import numpy as np</p>
<p>img = np.zeros([100,100,1],dtype=np.uint8) img.fill(0)</p>
<p>image = cv2.circle(img, (50,50), 20, (255,255,255), 2) cv2.imwrite(“images/circle.png”,img)</p>
<pre><code>![An 100x100 image of a circle.](images/circle.png){#fig-circle}

@fig-circle shows an image of a circle, which is $100 \times 100$ in size. While the image is in 2D, the pixel space is $\real^{100\times 100}$, which needs $100\times 100=10000$ basis matrices! 

However, looking at the image, we can see that most of the image is empty (black pixels) and the circle can be represented with much smaller number of basis functions. Thus, by inspection, we can infer that the image can be represented in a lower dimensional space. We can apply singular value decomposition to determine lower dimensional (or *reduced order*) representation of the image. This is also known as *image compression*.

The following Python code demonstrates it.

```{python, results='hide'}
#| fig-cap: "Singular values of the image matrix. Most of the singular values are zero, which means the image can be represented in lower dimensional space."
#| layout-ncol: 1
#| label: fig-svd_of_image
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import svd
from PIL import Image

# Load the image into a numpy array.
img = Image.open('images/circle.png')

# Perform SVD
U, Sigma, Vt = svd(img)

markerline, stemline, baseline = plt.stem(range(len(Sigma)),Sigma)
plt.setp(markerline, markersize = 3)
plt.title("Singular values of the image matrix.")
plt.xlabel("Index k.")
plt.ylabel("Singular Value")</code></pre>
<p><a href="dimensionality_reduction.html#fig-svd_of_image">Figure&nbsp;<span>13.2</span></a> shows the singular values of the image matrix. We observe that most of the singular values are zero, indicating the image can be represented in a much lower dimensional space. The next Python code shows how it can be done.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Various representations of original image in lower dimensional spaces."

def compress_image(U,Vt,Sigma,k):
  U_k = U[:, :k]
  Sigma_k = np.diag(Sigma[:k])
  Vt_k = Vt[:k, :]
  A_k = np.dot(U_k, np.dot(Sigma_k, Vt_k))
  print(U_k.shape)
  return A_k

img1 = compress_image(U,Vt,Sigma,20)
img2 = compress_image(U,Vt,Sigma,10)
img3 = compress_image(U,Vt,Sigma,5)


# Plot the original and the compressed image
plt.figure()
plt.subplot(2, 2, 1)
plt.imshow(img, cmap='gray')
plt.title('Original Image')

plt.subplot(2, 2, 2)
plt.imshow(img1, cmap='gray')
plt.title('Compressed Image with k=20')

plt.subplot(2, 2, 3)
plt.imshow(img2, cmap='gray')
plt.title('Compressed Image with k=10')

plt.subplot(2, 2, 4)
plt.imshow(img3, cmap='gray')
plt.title('Compressed Image with k=5')

plt.tight_layout()
plt.show()
</code></pre>
</section>
</section>
</section>
<section id="function-approximation" class="level1" data-number="19">
<h1 data-number="19"><span class="header-section-number">19</span> Function Approximation</h1>
<p>Function approximation is a core concept in mathematics, statistics, and machine learning, involving the estimation of a function using a simpler or more tractable form. This process is essential when the exact form of a function is unknown, too complex, or when an analytical expression is not available.</p>
<p>Key aspects of function approximation include:</p>
<ol type="1">
<li><p><strong>Simplifying Complex Functions</strong>: Function approximation involves representing a complicated function with a simpler one, which is easier to analyze, compute, or understand. The approximating function typically belongs to a well-defined class of functions, like polynomials, trigonometric functions, or piecewise linear functions.</p></li>
<li><p><strong>Polynomial Approximation</strong>: One common approach is using polynomial functions, such as in Taylor series or Fourier series approximations. For example, a complex function can be approximated by a polynomial of a certain degree, capturing the essential behavior of the function within a specific range.</p></li>
<li><p><strong>Piecewise Approximation</strong>: Another method is to approximate a function using a series of simple functions over different intervals. For instance, a complex curve could be approximated by a series of straight lines (linear functions) in different segments.</p></li>
<li><p><strong>Least Squares and Regression</strong>: In statistics and machine learning, regression techniques, such as linear regression or polynomial regression, are used to approximate the relationship between variables. The least squares method is often used to find the best-fit line or curve that minimizes the differences (residuals) between the observed values and the values predicted by the model.</p></li>
<li><p><strong>Neural Networks</strong>: In modern machine learning, neural networks, particularly deep learning models, are powerful tools for function approximation. They can model complex, non-linear relationships in data, effectively approximating functions with many variables. We will learn about neural networks later in the book.</p></li>
</ol>
<p>Function approximation is an effective strategy for addressing real-world challenges where direct calculation or analysis is difficult. It facilitates the extraction of vital insights from intricate phenomena and plays a pivotal role in various domains, including engineering, economics, physics, and artificial intelligence. The selection of the approximation technique and the level of approximation rely on the specific needs for accuracy and computational efficiency.</p>
<section id="linear-spaces-with-basis-elements" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="linear-spaces-with-basis-elements"><span class="header-section-number">19.1</span> Linear Spaces with Basis Elements</h2>
<p>This chapter emphasizes function approximation through a linear combination of known functions, creating a linear approximation space. In this realm, the notions of “basis” and “basis functions” are essential. A basis in a linear approximation space consists of a set of basis elements (vectors or functions) that are linearly independent and encompass the entire space. This implies that any vector or function in this space can be represented as a linear combination of these basis elements.</p>
<p>These basis vectors or functions need to be linearly independent, ensuring that none of them can be expressed as a linear combination of the others. This guarantees the minimality of the basis, meaning that there are no superfluous elements. Additionally, this implies that the basis elements are orthogonal.</p>
<p>The collection of basis vectors or functions must cover the entire space, signifying that any element within the space can be precisely depicted using a linear combination of the basis components.</p>
<p>Some important linear approximation spaces are discussed next.</p>
</section>
<section id="eulidean-space" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="eulidean-space"><span class="header-section-number">19.2</span> Eulidean Space</h2>
<p>A Euclidean space, often denoted as <span class="math inline">\(\real^n\)</span>, is a mathematical space where each point is represented by a vector <span class="math inline">\(\x:=\begin{bmatrix} x_1 &amp; \cdots &amp; x_n \end{bmatrix}^T\)</span>. Each element <span class="math inline">\(x_i\)</span> represents the coordinate of the point along a specific axis or dimension.</p>
<p>A <strong>basis</strong> in a Euclidean space consists of a set of linearly independent vectors that <em>span</em> the entire space. In other words, any vector in the Euclidean space can be uniquely expressed as a linear combination of the basis vectors.</p>
<section id="minimal-basis" class="level3" data-number="19.2.1">
<h3 data-number="19.2.1" class="anchored" data-anchor-id="minimal-basis"><span class="header-section-number">19.2.1</span> Minimal Basis</h3>
<p>In Euclidean spaces, the most commonly used basis is the <strong>standard</strong> or <strong>canonical basis</strong>. In <span class="math inline">\(\real^n\)</span>, the standard basis consists of n unit vectors, each having a single component equal to 1 and all other components equal to 0. For example, in 3D (<span class="math inline">\(\real^3\)</span>), the standard basis vectors are <span class="math display">\[
\e_1 = \begin{bmatrix}1\\ 0\\ 0\end{bmatrix}, \e_2 = \begin{bmatrix}0\\ 1\\ 0\end{bmatrix}, \e_3 = \begin{bmatrix}0\\ 0\\ 1\end{bmatrix}.
\]</span></p>
<p>Euclidean space can be represented as a linear combination of these basis vectors. For example, in <span class="math inline">\(\real^3\)</span>, we can express any vector <span class="math inline">\(\x:=\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 \end{bmatrix}^T\)</span> as <span class="math display">\[
\x =  x_1\begin{bmatrix}1\\ 0\\ 0\end{bmatrix} + x_2 \begin{bmatrix}0\\ 1\\ 0\end{bmatrix} + x_3 \begin{bmatrix}0\\ 0\\ 1\end{bmatrix},
\]</span> or more compactly as <span class="math inline">\(\x = x_1\e_1 + x_2\e_2 + x_3\e_3.\)</span></p>
<p>Euclidean spaces are equipped with an inner product, denoted as <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span>, which defines the dot product or scalar product between two vectors. Mathematically, it is defined as <span class="math display">\[\langle \x, \y\rangle = \sum_{i=1}^n x_i y_i.\]</span></p>
<p>The basis <span class="math inline">\(\e_1, \e_2\)</span>, and <span class="math inline">\(\e_3\)</span> are orthogonal, i.e.&nbsp;<span class="math display">\[\begin{align*}
\dotp{\e_i,\e_j} &amp;= 0, \text{ for } i \neq j; \; i,j = {1,2,3},\\
\dotp{\e_i,\e_i} &amp;= 1, \text{ for } i = {1,2,3}.
\end{align*}\]</span> This orthogonality condition generalizes to <span class="math inline">\(\real^n\)</span>.</p>
<p>In general, the basis need not be canonical, but a set of any <span class="math inline">\(n\)</span> orthogonal vectors will also <em>span</em> <span class="math inline">\(\real^n\)</span>. Think of them as different <em>coordinate system</em> in <span class="math inline">\(\real^n\)</span>. That is, we can express <span class="math inline">\(\x\)</span> a linear combination of two sets of basis: <span class="math display">\[
\x = \sum_{i=1}^n a_i \vo{a}_i = \sum_{i=1}^n b_i \vo{b}_i,
\]</span> where <span class="math inline">\(\vo{a}_i\)</span> and <span class="math inline">\(\vo{a}_j\)</span> are orthogonal, and <span class="math inline">\(\vo{b}_i\)</span> and <span class="math inline">\(\vo{b}_j\)</span> are orthogonal. The corresponding components are <span class="math inline">\(a_i\)</span> and <span class="math inline">\(b_i\)</span> respectively. This is illustrated in the figure below:</p>
<p>```{r, engine = ‘tikz’,out.width = “300px”} #| fig-cap: “Different sets of orthogonal basis to represent the same 2D Euclidean space.” #| echo: false</p>
<pre><code>
*Note:* Therefore, a Euclidean space can be represented by a non-unique set of orthogonal basis.

*Note:* In $\real^n$, we need a minimum $n$ orthogonal basis to span it.

We can recover the *components* of the vector $\x$ by *projecting* $\x$ on each of the basis $\e_i$. The projection is defined as the inner-product between $\x$ and $\e_i$. Therefore, the component $x_i$ is the projection of $\x$ on $\e_i$, i.e., $$
x_i = \dotp{\x,\e_i}.
$$

### Nonminimal Basis

The basis in $\real^n$ need not be orthogonal. If they are then we have a *minimal set*.

In general it is possible to *exactly* represent a vector $\x\in\real^n$ with basis $\{\e_i\}_{i=1}^m$ where $\vo{e}_i \in \real^n$ and $m &gt; n$. However, the basis must have $n$ linear independent components, otherwise $\{\e_i\}_{i=1}^m$ doesn't span $\real^n$ and we will not be able to represent $\x\in\real^n$ exactly using linear combinations of $\{\e_i\}_{i=1}^m$.

If we define a matrix $\vo{E} \in \real^{n\times m}$ with $\{\e_i\}_{i=1}^m$, i.e., $$
\vo{E} = \begin{bmatrix}\e_1 &amp; \e_2 &amp; \cdots &amp; \e_m \end{bmatrix},
$$ then the vector $\x\in\real^n$ can be expressed as $$
\x \approx  \sum_{i=1}^m {y_i\e_i} = \vo{E}\y,$$ where $\y = \begin{bmatrix}y_1 &amp; y_2 &amp; \cdots &amp; y_m\end{bmatrix}^T$ are the components of $\x$ in $\{\e_i\}_{i=1}^m$. We can think of $\vo{E}$ as a matrix that linearly maps vectors from $\real^m$ to $\real^n$.

We use $\approx$ in the above equation because $\{\e_i\}_{i=1}^m$ may not span $\real^n$. In that, case our approximation space will be a lower dimensional space, even when we have $m&gt;n$.

The dimension of the space spanned by $\{\e_i\}_{i=1}^m$ is given by the rank of the matrix $\vo{E}$. Recall that a matrix is said to have *full rank* if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be *rank-deficient* if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank. Therefore, in this case the rank of $\vo{E}$ is always less than or equal to $n$.

We solve for $\y$ in an optimization framework. We first define the *residual* or error $$\vo{r} = \x - \vo{E}\y$$ and the objective is to minimize $\vo{r}$ in some sense. For this problem, $\vo{r}$ is a vector and we want to minimize the length of $\vo{r}$. For simplicity, we minimize the square of the length of $\vo{r}$, which is simply $\vo{r}^T\vo{r}$. Therefore, the best representation of $\x$ in the space spanned by $\{\e_i\}_{i=1}^m$ is the one that minimizes the residual.

Mathematically, it can be written as 
$$
\min_{\y} \vo{r}^T\vo{r},
$$ which defines the *least-squares* problem in Euclidean space.

The cost function $\vo{r}^T\vo{r}$ can be written as \begin{align*}
\vo{r}^T\vo{r} &amp;= (\x - \vo{E}\y)^T(\x - \vo{E}\y),\\
&amp;= \y^T\vo{E}^T\vo{E}\y - \y^T\vo{E}^T\x - \x^T\vo{E}\y + \x^T\x,\\
&amp;= \y^T\vo{E}^T\vo{E}\y -2\y^T\vo{E}^T\x + \x^T\x.
\end{align*}

This is a quadratic equation in the unknown $\y$. To solve for $\y$ we first apply the first order condition of optimality \begin{align*}
&amp; \frac{\partial \left(\y^T(\vo{E}^T\vo{E})\y -2\y^T\vo{E}^T\x + \x^T\x\right)}{\partial \y} = 0 \\
\implies &amp; \y^T(\vo{E}^T\vo{E}) - 2\vo{E}^T\x = 0,\\
\implies &amp; 2(\vo{E}^T\vo{E})\y = 2\vo{E}^T\x,\\
\implies &amp; (\vo{E}^T\vo{E})\y = \vo{E}^T\x,\\
\text{or } &amp; \y =  (\vo{E}^T\vo{E})^\dagger\vo{E}^T\x.
\end{align*}

Therefore, the optimal representation of $\x$ in the space spanned by $\{\e_i\}_{i=1}^m$ is given by the components 
$$ 
 \y^\ast = (\vo{E}^T\vo{E})^\dagger\vo{E}^T\x.
 $$ {#eq-optmal-ls-euclidean}

We put $\ast$ to indicate optimality.

Note that:

-   If $\vo{E}$ is full rank, i.e. $\rank{\vo{E}} = n$, then the optimal residual $\vo{r}^\ast := \x - \vo{E}\y^\ast = \vo{0}$, i.e., we are able to exactly represent $\x$ with linear combinations $\{\e_i\}_{i=1}^m$.
-   If $\vo{E}$ is rank-deficient rank, i.e. $\rank{\vo{E}} &lt; n$, then the optimal residual $\vo{r}^\ast := \x - \vo{E}\y^\ast \neq \vo{0}$, i.e., we are *not* able to exactly represent $\x$ with linear combinations $\{\e_i\}_{i=1}^m$. We can only obtain the *best* approximation of $\x$.

### Examples

Here are some examples demonstrating approximation in Euclidean space.

#### $\vo{E}$ Has Full Rank

Consider $\real^3$ with basis $\e_i := \begin{bmatrix}\cos(\theta_i) &amp; \sin(\theta_i) &amp; 1\end{bmatrix}^T$, for $\theta_i \in \{0^\circ, 30^\circ, 45^\circ, 60^\circ, 90^\circ\}$.

```{python}
#| jupyter: python3
import numpy as np 
d2r = np.pi/180;
TH = np.array([0,30,45,60,90])
e = lambda th: np.array([np.cos(th*d2r), np.sin(th*d2r), 1])
E = np.array([e(th) for th in TH]).T
print(E)</code></pre>
<p>This results in <span class="math display">\[
  \vo{E} = \begin{bmatrix}
  1.0 &amp; 0.866 &amp; 0.707  &amp;0.5&amp;       0.0\\
0.0  &amp;0.5       &amp;0.707  &amp;0.866 &amp; 1.0\\
1.0  &amp;1.0       &amp;1.0       &amp;1.0      &amp; 1.0\\
  \end{bmatrix}.
\]</span> We can verify that <span class="math inline">\(\rank{\vo{E}} = 3\)</span>, therefore <span class="math inline">\(\vo{E}\)</span> has full-rank.</p>
<pre class="{python}"><code>print(np.linalg.matrix_rank(E))</code></pre>
<p>Let us now represent a vector <span class="math inline">\(\x:=\begin{bmatrix} 1 &amp; 1 &amp; 1\end{bmatrix}^T\)</span> with the basis provided. Using (<a href="function_approximation.html#eq-optmal-ls-euclidean">Equation&nbsp;<span>9.1</span></a>), the optimal components of <span class="math inline">\(\x\)</span> in <span class="math inline">\(\{\e_i\}_{i=1}^5\)</span> is given by:</p>
<pre class="{python}"><code>x = np.array([1,1,1])
yopt = np.linalg.pinv(E.T@E)@E.T@x
print("Optimal components:", yopt)</code></pre>
<p>Since <span class="math inline">\(\vo{E}\)</span> is full rank, the optimal residual <span class="math inline">\(\vo{r}^\ast\)</span> should be (numerically) zero.</p>
<pre class="{python}"><code>print("Optimal residual:", x - E@yopt)</code></pre>
<section id="obtain-minimal-spanning-set-from-a-given-basis" class="level4" data-number="19.2.1.1">
<h4 data-number="19.2.1.1" class="anchored" data-anchor-id="obtain-minimal-spanning-set-from-a-given-basis"><span class="header-section-number">19.2.1.1</span> Obtain Minimal Spanning Set From a Given Basis</h4>
<p>Given <span class="math inline">\(\vo{E}\)</span> for the previous example, we are interested in determining the minimal basis from it. Since we are representing vectors in <span class="math inline">\(\real^3\)</span>, the minimal set should include three orthogonal basis. We can obtain them using singular value decomposition of <span class="math inline">\(\vo{E}\)</span>. This is shown in the following Python code.</p>
<pre class="{python}"><code>U, singular_values, Vt = np.linalg.svd(E) # Get svd of E
print("The reduced basis are given by the columns of U:\n", U)</code></pre>
<p>These columns are orthogonal.</p>
<pre class="{python}"><code>print("dot(U[:,0],U[:,1]):", np.dot(U[:,0],U[:,1]))
print("dot(U[:,0],U[:,2]):", np.dot(U[:,0],U[:,2]))
print("dot(U[:,1],U[:,2]):", np.dot(U[:,1],U[:,2]))</code></pre>
<p>The components of <span class="math inline">\(\x\)</span> in the new basis are given by:</p>
<pre class="{python}"><code>yred = np.linalg.pinv(U.T@U)@(U.T)@x
print("The components in reduced basis:\n", yred);</code></pre>
<p>The residual should also be zero, since <span class="math inline">\(\rank{\vo{U}} = 3\)</span>.</p>
<pre class="{python}"><code>print("Rank(U):", np.linalg.matrix_rank(U))
print("Optimal residue with reduced basis:\n", x-U@yred)</code></pre>
</section>
<section id="voe-spans-lower-dimensional-space" class="level4" data-number="19.2.1.2">
<h4 data-number="19.2.1.2" class="anchored" data-anchor-id="voe-spans-lower-dimensional-space"><span class="header-section-number">19.2.1.2</span> <span class="math inline">\(\vo{E}\)</span> Spans Lower Dimensional Space</h4>
<p>Now consider basis <span class="math inline">\(\e_i := \begin{bmatrix}\cos(\theta_i) &amp; \sin(\theta_i) &amp; 0 \end{bmatrix}^T\)</span>, for <span class="math inline">\(\theta_i \in \{0^\circ, 30^\circ, 45^\circ, 60^\circ, 90^\circ\}\)</span>. The corresponding <span class="math inline">\(\vo{E}\)</span> matrix is given by <span class="math display">\[
  \vo{E} = \begin{bmatrix}
  1.0  &amp; 0.866     &amp; 0.707  &amp; 0.5   &amp;  0.0\\
  0.0  &amp; 0.5       &amp; 0.707  &amp; 0.866 &amp; 1.0\\
  0.0  &amp; 0.0       &amp; 0.0    &amp; 0.0   &amp; 0.0\\
  \end{bmatrix}.
\]</span></p>
<pre class="{python}"><code>import numpy as np 
d2r = np.pi/180;
TH = np.array([0,30,45,60,90])
e = lambda th: np.array([np.cos(th*d2r), np.sin(th*d2r), 0])
E = np.array([e(th) for th in TH]).T
print(E)
print("Matrix rank: ", np.linalg.matrix_rank(E))</code></pre>
<p>Since the rank of this matrix is 2, the associated basis doesn’t span <span class="math inline">\(\real^3\)</span>. Therefore, we will not be able to exactly represent vectors in <span class="math inline">\(\real^3\)</span>. We can get the <em>best</em> approximation in <span class="math inline">\(\real^2\)</span>.</p>
<p>For example, <span class="math inline">\(\x:=\begin{bmatrix} 1 &amp; 1 &amp; 1\end{bmatrix}^T\)</span> cannot be exactly represented, i.e., we will have a non-zero residual. This is demonstrated in the following Python code.</p>
<pre class="{python}"><code>y = np.linalg.pinv(E.T@E)@(E.T)@x
print("Optimal residue with given basis:\n", x-E@y)</code></pre>
<p>We see that there is zero residual in the first two coordinates, which is in the space spanned by the given basis. The residual vector is thus in the space orthogonal to the space spanned by the given basis.</p>
<p>In general, the residual will be orthogonal to the space spanned by the basis. This will always be the case whenever we try to approximate <em>objects</em> in higher dimensional space in lower dimensional space.</p>
</section>
<section id="a-projection-perspective" class="level4" data-number="19.2.1.3">
<h4 data-number="19.2.1.3" class="anchored" data-anchor-id="a-projection-perspective"><span class="header-section-number">19.2.1.3</span> A Projection Perspective</h4>
<p>We next show that the best approximation in a lower dimensional space can be eqiuvalently achieved by projecting the residual on each of the basis function. We assume we have <span class="math inline">\(m\)</span> basis in <span class="math inline">\(\real^n\)</span>, i.e.&nbsp;<span class="math inline">\(\{\e_i\}_{i=1}^m\)</span> for <span class="math inline">\(\e_i\in\real^n\)</span>.</p>
<p>The optimal representation of <span class="math inline">\(\x\in\real\)</span> in that case satisfies <span id="eq-projection-theorem"><span class="math display">\[
\dotp{\vo{r},\e_i} = 0, \text{ for } i = 1,\cdots,m.
\tag{19.1}\]</span></span></p>
<p>Recalling that <span class="math inline">\(\vo{r}:=\x - \vo{E}\y\)</span> and <span class="math inline">\(\dotp{\vo{r},\e_i}\)</span> for vectors is simply <span class="math inline">\(\vo{r}^T \e_i = \e_i^T\vo{r}\)</span>, <a href="references.html#eq-projection-theorem">Equation&nbsp;<span>7.1</span></a> can be written as <span class="math display">\[\begin{align*}
&amp; \e_i^T\x - \e_i^T\vo{E}\y = 0,\\
\text{ or }&amp; \e_i^T\vo{E}\y = \e_i^T\x, \text{ for } i = 1,\cdots,m,
\end{align*}\]</span> or more compactly as <span class="math display">\[
\vo{E}^T\vo{E}\y = \vo{E}^T\x.
\]</span></p>
<p>Therefore, the coefficients <span class="math inline">\(\y\)</span> can be solved as <span class="math display">\[
\y = (\vo{E}^T\vo{E})^\dagger\vo{E}^T\x,
\]</span> which is the same as <a href="function_approximation.html#eq-optmal-ls-euclidean">Equation&nbsp;<span>9.1</span></a>. Therefore, projecting the residual on each basis results in the optimal approximation.</p>
</section>
</section>
<section id="matrix-spaces" class="level3" data-number="19.2.2">
<h3 data-number="19.2.2" class="anchored" data-anchor-id="matrix-spaces"><span class="header-section-number">19.2.2</span> Matrix Spaces</h3>
<p>We next look at linear matrix spaces. Let us consider a matrix in <span class="math inline">\(\real^{2\times 2}\)</span> given by <span class="math display">\[
\A = \begin{bmatrix}a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{bmatrix}.
\]</span> We can represent it as a linear combination of basis matrices in the following manner: <span class="math display">\[
\A = a_{11}\begin{bmatrix}1 &amp; 0\\0 &amp; 0\end{bmatrix} +  a_{12}\begin{bmatrix}0 &amp; 1\\0 &amp; 0\end{bmatrix} + a_{21}\begin{bmatrix}0 &amp; 0\\1 &amp; 0\end{bmatrix} + a_{22}\begin{bmatrix}0 &amp; 0\\0 &amp; 1\end{bmatrix},
\]</span> where <span class="math display">\[
\begin{bmatrix}1 &amp; 0\\0 &amp; 0\end{bmatrix}, \begin{bmatrix}0 &amp; 1\\0 &amp; 0\end{bmatrix}, \begin{bmatrix}0 &amp; 0\\1 &amp; 0\end{bmatrix}, \text{ and }\begin{bmatrix}0 &amp; 0\\0 &amp; 1\end{bmatrix},
\]</span> are basis matrices that span <span class="math inline">\(\real^{2\times 2}\)</span>.</p>
<p>Similarly, space of symmetric matrices in <span class="math inline">\(\real^{2\times 2}\)</span> can be written as <span class="math display">\[
\A = a_{11}\begin{bmatrix}1 &amp; 0\\0 &amp; 0\end{bmatrix} +  a_{12}\begin{bmatrix}0 &amp; 1\\1 &amp; 0\end{bmatrix} + a_{22}\begin{bmatrix}0 &amp; 0\\0 &amp; 1\end{bmatrix} =  \begin{bmatrix}a_{11} &amp; a_{12} \\ a_{12} &amp; a_{22} \end{bmatrix}.
\]</span></p>
<p>Recall that inner product of two matrices <span class="math inline">\(\A,\B\)</span> is defined as: <span class="math display">\[
\dotp{\A,\B} := \tr{\A^T\B},
\]</span> which can be applied to the basis matrices shown above to confirm that they are <em>orthogonal</em>.</p>
</section>
</section>
<section id="function-space" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="function-space"><span class="header-section-number">19.3</span> Function Space</h2>
<p>Let us consider an example of function approximation in the <span class="math inline">\(\set{L}^2\)</span> Hilbert space, which consists of all square-integrable functions. Recall that a function <span class="math inline">\(f(x)\)</span> is in <span class="math inline">\(\set{L}^2\)</span> if <span class="math inline">\(\int |f(x)|^2 dx &lt; \infty\)</span>. In this space, the inner product of two functions <span class="math inline">\(f(x)\)</span> and <span class="math inline">\(g(x)\)</span> which maps <span class="math inline">\(\real\)</span> to <span class="math inline">\(\real\)</span> is defined as <span class="math inline">\(\langle f(x), g(x) \rangle = \int f(x)g(x) dx\)</span>. If the functions are vectors or matrices with vector arguments, then we have the following definition of inner products: <span class="math display">\[\begin{align*}
\text{For vector functions: } &amp; \langle\vo{f}(\x),\vo{g}(\x)\rangle := \int_\set{D} \vo{f}^T(x)\vo{g}(\x) d\x, \\
\text{For matrix functions: } &amp; \langle\vo{F}(\x),\vo{G}(\x)\rangle := \int_\set{D} \tr{\vo{F}^T(x)\vo{G}(\x)} d\x,
\end{align*}\]</span> where <span class="math inline">\(\set{D}\)</span> is the set over which the functions are defined.</p>
<section id="basis-functions" class="level3" data-number="19.3.1">
<h3 data-number="19.3.1" class="anchored" data-anchor-id="basis-functions"><span class="header-section-number">19.3.1</span> Basis Functions</h3>
<p>A Hilbert space is defined by a set of basis functions (or vectors for <span class="math inline">\(l_2\)</span> spaces), which are denoted by <span class="math inline">\(\phi_i(\x)\)</span> for <span class="math inline">\(\x\in\real^n\)</span>. The set of basis functions are denoted <span id="eq-basis-functions"><span class="math display">\[
\Phib(\x) := \begin{bmatrix}\phi_0(\x)\\\phi_1(\x) \\ \vdots \\\phi_N(\x) \end{bmatrix}.
\tag{19.2}\]</span></span></p>
<p>Scalar functions in this space can be represented as linear combination of these basis functions as, <span class="math display">\[
\hat{f}(\x) = \sum_{i=0}^N \phi_i(\x)\alpha_i = \Phib^T(\x)\alphab = \alphab^T\Phib(\x),
\]</span> where <span class="math inline">\(\alphab := \begin{bmatrix}\alpha_0 &amp; \alpha_1 &amp; \cdots &amp;\alpha_N\end{bmatrix}^T\)</span> is the vector of coefficients. Vector functions <span class="math inline">\(\vo{F}(\x):\real^n \mapsto \real^m\)</span> can be represented as <span class="math display">\[
\hat{\vo{f}}(\x) = \A\Phib(\x),
\]</span> where <span class="math inline">\(\A\in\real^{m\times (N+1)}\)</span> is the coeffient matrix. We put a <span class="math inline">\(\hat{}\)</span> on the functions to indicate that we will be approximating the true functions in a finite-dimensional Hilbert space.</p>
<p>When <span class="math inline">\(\phi_i(\x)\)</span> are orthogonal, i.e., <span class="math inline">\(\langle\phi_i(\x),\phi_j(\x)\rangle = 0\)</span> for <span class="math inline">\(i\neq j\)</span>, the dimension of the Hilbert space is given by the number of basis functions. However, these basis functions need not be orthogonal, resulting in non-minimal set of basis functions. In that case, the dimension of the Hilbert space is given by the rank of the matrix <span class="math inline">\(\langle \Phib^T(\x),\Phib^T(\x) \rangle\)</span>.</p>
<p>Let us next consider approximation of a function <span class="math inline">\(f(\x):\real^n \mapsto \real\)</span> in the space defined by <span class="math inline">\(\Phib(\x)\)</span>. Similar to approximation in the Euclidean space, we will define the error (or residual) as <span class="math display">\[
e(\x) := f(\x) - \Phib^T(\x)\alphab.
\]</span> The objective is to determine the coefficient vector <span class="math inline">\(\alphab\)</span> which minimizes <span class="math inline">\(e(\x)\)</span> in some sense. We will minimize <span class="math inline">\(\|e(\x)\|^2_2\)</span>, which is defined as <span class="math display">\[\begin{align*}
\|e(\x)\|^2_2 &amp;:= \langle e(\x),e(\x))\rangle^2, \\
&amp;=  \langle f(\x) - \Phib^T(\x)\alphab,  f(\x) - \Phib^T(\x)\alphab \rangle,\\
&amp;= \int_\set{D} (f(\x) - \Phib^T(\x)\alphab)^T (f(\x) - \Phib^T(\x)\alphab) d\x,\\
&amp; = \int_\set{D} \left(f^2(\x) -  2f(\x)\Phib^T(\x)\alphab + \alphab^T\Phib(\x)\Phib^T(\x)\alphab \right)d\x,\\
&amp; = \underbrace{\left(\int_\set{D} f^2(\x)d\x\right)}_{=: s} - 2\underbrace{\left(\int_\set{D} f(\x)\Phib^T(\x) \right)}_{=:\vo{r}^T}\alphab + \alphab^T \underbrace{\left(\int_\set{D} \Phib(\x)\Phib^T(\x) d\x \right)}_{=:\vo{Q \ge \vo{0}}}\alphab, \\
&amp;= \alphab\top\vo{Q}\alphab -  2\vo{r}^T\alphab + s,
\end{align*}\]</span> which is quadratic in <span class="math inline">\(\alphab\)</span>. The minimum value of a quadratic cost function is obtained by first setting the gradient w.r.t <span class="math inline">\(\alphab\)</span> to zero and then checking the Hessian for positive (semi-)definiteness. Therefore, <span class="math display">\[\begin{align*}
&amp; \frac{\partial}{\partial \alphab}\left(\alphab\top\vo{Q}\alphab -  2\vo{r}^T\alphab + s\right) = 2\vo{Q}\alphab - 2\vo{r} = 0.\\
\end{align*}\]</span> Therefore, the solution is given by <span id="eq-optimal-L2-error"><span class="math display">\[
\implies  \alphab^\ast := \vo{Q}^\dagger\vo{r}.
\tag{19.3}\]</span></span></p>
<p>The Hessian of the cost function is <span class="math inline">\(\vo{Q}\)</span> which is positive (semi-) definite. Therefore, the candidate solution(s) obtained by setting the gradient to zero is (are) the minima of the function.</p>
<p>If <span class="math inline">\(\vo{Q}\)</span> is positive definite then it is invertible and <span class="math inline">\(\alphab^\ast\)</span> is unique. If <span class="math inline">\(\vo{Q}\)</span> is positive semi-definite then <span class="math inline">\(\vo{Q}\)</span> is not invertible and there are many solutions of <span class="math inline">\(\alphab\)</span>. The pseudo-inverse gives the <span class="math inline">\(\alphab\)</span> which minimizes <span class="math inline">\(\|\alphab\|_2\)</span>.</p>
<p>The Hessian of the cost function is <span class="math inline">\(\vo{Q}\)</span>, which is positive (semi-)definite. Consequently, the potential solution(s) identified by equating the gradient to zero represent the minima of the function. When <span class="math inline">\(\vo{Q}\)</span> is positive definite, it also becomes invertible, leading to a unique solution denoted as <span class="math inline">\(\alphab^\ast\)</span>. In contrast, if <span class="math inline">\(\vo{Q}\)</span> is positive semi-definite, it lacks invertibility, resulting in multiple solutions for <span class="math inline">\(\alphab\)</span>. In such cases, the pseudo-inverse method is employed to determine the value of <span class="math inline">\(\alphab\)</span> that minimizes the <span class="math inline">\(\ell_2\)</span> norm, <span class="math inline">\(\|\alphab\|_2\)</span>.</p>
<p><em>Note:</em> If the function <span class="math inline">\(f(\x)\)</span> is in the space spanned by <span class="math inline">\(\Phib(x)\)</span>, then the optimal <span class="math inline">\(\|e(\x)\|^2_2\)</span> will be zero. Otherwise, the optimal residual will be non-zero.</p>
</section>
<section id="a-projection-perspective-1" class="level3" data-number="19.3.2">
<h3 data-number="19.3.2" class="anchored" data-anchor-id="a-projection-perspective-1"><span class="header-section-number">19.3.2</span> A Projection Perspective</h3>
<p>We can show that projecting the error <span class="math inline">\(e(\x)\)</span> on the basis function also results in the optimal solution. We can write <span class="math inline">\(\langle e(\x),\phi_i(\x) \rangle = 0\)</span> for <span class="math inline">\(i = 0,\cdots,N\)</span> compactly as <span class="math display">\[
\begin{bmatrix}\langle e(\x),\phi_0(\x) \rangle\\
\langle e(\x),\phi_1(\x) \rangle \\
\vdots \\
\langle e(\x),\phi_N(\x) \rangle
\end{bmatrix} = \vo{0} \\
\implies
\underbrace{\begin{bmatrix}
\langle\Phib^T(\x)\phi_0(\x)\rangle\\
\langle\Phib^T(\x)\phi_1(\x)\rangle\\
\vdots \\
\langle\Phib^T(\x)\phi_N(\x)\rangle
\end{bmatrix}}_{\text{This is equal to } \langle \Phib(\x)\Phib^T(\x)\rangle = \vo{Q}.} \alphab = \underbrace{\begin{bmatrix}\langle f(\x)\phi_0(\x)\rangle\\
\langle f(\x)\phi_1(\x)\rangle\\
\vdots \\
\langle f(\x)\phi_N(\x)\rangle
\end{bmatrix}}_{\text{This is equal to } \langle \Phib(\x)f(\x)\rangle = \vo{r}.}.
\]</span> Therefore, the system of linear equations has a solution <span class="math inline">\(\alphab^\ast = \vo{Q}^\dagger\vo{r}\)</span>, which is the same solution as <a href="references.html#eq-optimal-L2-error">Equation&nbsp;<span>7.3</span></a>.</p>
<section id="example-exact-polynomial-representation" class="level4" data-number="19.3.2.1">
<h4 data-number="19.3.2.1" class="anchored" data-anchor-id="example-exact-polynomial-representation"><span class="header-section-number">19.3.2.1</span> Example: Exact Polynomial Representation</h4>
<p>Here we consider approximation of <span class="math inline">\(f(x) = 1+x^2 - x^3\)</span> with monomial basis <span class="math inline">\(\Phib(x) = \begin{bmatrix}1 &amp; x &amp; x^2 &amp; x^3 &amp; x^4 \end{bmatrix}^T\)</span>. This is a simple problem, where <span class="math inline">\(f(x)\)</span> is in the space spanned by <span class="math inline">\(\Phib(x)\)</span> and we should expect zero residual error.</p>
<p>To approximate the function <span class="math inline">\(f(x) = 1 + 2x^2 - x^3\)</span> using a polynomial basis, we can use the SymPy library in Python. We define the inner product in this context as <span class="math inline">\(\langle p(x), q(x) \rangle = \int_{-1}^{1} p(x)q(x) dx\)</span> for polynomials <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>.</p>
<p>Here’s the Python code to perform this approximation:</p>
<pre class="{python}"><code>import numpy as np
import sympy as sp

x = sp.symbols('x')

basis = [1,x,x**2,x**3,x**4]
nbasis = len(basis)
f = 1 + 2*x**2 - x**3

# Compute projections using Sympy.
Q = [];
r = [];
for p in basis:
   a = sp.integrate(p*f,(x,-1,1))
   r.append(a)
   for q in basis:
      b = sp.integrate(p*q,(x,-1,1))
      Q.append(b)

# Solve for the coefficients
QQ = np.reshape(Q, (nbasis,nbasis)).astype(float)
rr = np.array(r).astype(float)
alp = np.round(np.linalg.pinv(QQ)@rr,2)

fhat = sum(basis*alp)
print(' f(x):',f,'\n','fhat(x):', fhat)</code></pre>
<p>We see that the basis functions are able to exactly recover the true function. This is expected since <span class="math inline">\(f(x)\)</span> lies in the space of functions spanned by <span class="math inline">\(\Phib(\x)\)</span>.</p>
</section>
<section id="example-approximation-of-ex-with-monomials" class="level4" data-number="19.3.2.2">
<h4 data-number="19.3.2.2" class="anchored" data-anchor-id="example-approximation-of-ex-with-monomials"><span class="header-section-number">19.3.2.2</span> Example: Approximation of <span class="math inline">\(e^x\)</span> with Monomials</h4>
<p>Now consider <span class="math inline">\(f(x) = e^x\)</span>. From Taylor series expansion, we know that we need infinite terms to represent <span class="math inline">\(e^x\)</span>, i.e., <span class="math display">\[
e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!} + \frac{x^5}{5!} + \cdots
\]</span></p>
<p>Therefore, if we treat each monomial as a basis function, we will need infinite basis to exactly represent <span class="math inline">\(e^x\)</span>. Consequently, we can only <em>approximate</em> <span class="math inline">\(e^x\)</span> with a finite number of monomial basis functions. However, if the approximation interval is small, we may get satisfactorily high accuracy with a few basis functions, as demonstrated next. The following Python code approximates <span class="math inline">\(e^x\)</span> with increasing number of basis functions.</p>
<pre class="{python}"><code>#| fig-cap: "Approximation of $e^x$ with monomial basis functions in the interval $[-1,1]$. The plot shows coefficients associated with each basis function."
#| layout-ncol: 1
import numpy as np
import sympy as sp
import matplotlib.pyplot as plt

x = sp.symbols('x')

N = 10
ii = range(N)
basis = [x**i for i in ii]

nbasis = len(basis)
f = sp.exp(x)

# Compute projections using Sympy.
Q = [];
r = [];
for p in basis:
   a = sp.integrate(p*f,(x,-1,1))
   r.append(a)
   for q in basis:
      b = sp.integrate(p*q,(x,-1,1))
      Q.append(b)

# Solve for the coefficients
QQ = np.reshape(Q, (nbasis,nbasis)).astype(float)
rr = np.array(r).astype(float)
alp = np.round(np.linalg.pinv(QQ)@rr,2)

fhat = sum(basis*alp)
plt.plot(ii,alp,'o-'); 
plt.xlabel('Coefficient Index');
plt.ylabel('alpha');
plt.title('Coefficients of various basis functions.')
plt.grid()
plt.show()</code></pre>
<p>We see that the coefficient for basis number six on onwards are zero, implying we are able accurately represent <span class="math inline">\(e^x\)</span> in the interval <span class="math inline">\([-1,1]\)</span> with the first six basis functions. This is supported by the next set of plots where we plot approximations of <span class="math inline">\(e^x\)</span> with increasing number of basis functions.</p>
<pre class="{python}"><code>#| fig-cap: "Comparison of $e^x$ with its approximation with increasing number of basis functions in the interval $[-1,1]$."
#| layout-ncol: 1

k1=3; fhat1 = sp.lambdify(x,sum(basis[:k1]*alp[:k1]))
k2=4; fhat2 = sp.lambdify(x,sum(basis[:k2]*alp[:k2]))
k3=5; fhat3 = sp.lambdify(x,sum(basis[:k3]*alp[:k3]))

xx = np.linspace(-1,1,100)

def plot_approximations(k,fhat,xx):
   line1, = plt.plot(xx,fhat(xx), label='Approximation')
   line2, = plt.plot(xx,np.exp(xx),label='exp(x)'); 
   plt.title(f'With {k+1} basis functions'); 
   plt.xlabel('x')
   plt.legend(handles=[line1,line2])

plt.figure(1);
plt.subplot(1,3,1);  plot_approximations(k1,fhat1,xx);
plt.subplot(1,3,2);  plot_approximations(k2,fhat2,xx);
plt.subplot(1,3,3);  plot_approximations(k3,fhat3,xx);

plt.tight_layout()
plt.show()</code></pre>
<p><em>Note:</em> Orthogonal polynomials, such as Legendre, Chebyshev, and Hermite, are often favored over monomials for function approximation due to their distinct advantages in numerical stability and efficiency. These polynomials are mutually orthogonal with respect to a specific inner product, ensuring independent coefficients in the polynomial approximation and reducing common numerical problems like ill-conditioning. The approximation process involves projecting the function into a space defined by these polynomials, with coefficients computed via inner products, leading to stable and precise results, especially with higher-degree polynomials. Unique properties of certain orthogonal polynomials, like the minimax property of Chebyshev polynomials and the consistent behavior of Legendre polynomials, make them ideal for function approximation.</p>
</section>
<section id="example-approximation-with-trigonometric-basis-functions" class="level4" data-number="19.3.2.3">
<h4 data-number="19.3.2.3" class="anchored" data-anchor-id="example-approximation-with-trigonometric-basis-functions"><span class="header-section-number">19.3.2.3</span> Example: Approximation with Trigonometric Basis Functions</h4>
<p>Trigonometric basis functions are useful in approximating periodic functions. Let us consider basis functions <span class="math display">\[
\Phib(x):= \begin{bmatrix} 1, &amp; \cos(\frac{\pi}{L}x), &amp; \sin(\frac{\pi}{L}x), &amp;\cdots, &amp; \cos(\frac{N\pi}{L}x), &amp; \sin(\frac{N\pi}{L}x) \end{bmatrix}^T,
\]</span> over the interval <span class="math inline">\([-L,L]\)</span>. These basis functions are orthogonal.</p>
<p>The following Python code approximates <span class="math inline">\(f(x) = \sin(\frac{3\pi}{2}x)^3(1-x^2)\)</span> using trigonometric basis functions over the interval <span class="math inline">\([-1,1]\)</span></p>
<pre class="{python}"><code>#| fig-cap: "Approximation of a periodic function in the interval $[-1,1]$ using trigonometric basis functions."
#| layout-ncol: 1
#| label: fig-periodic_function
import numpy as np
import sympy as sp
import matplotlib.pyplot as plt

x = sp.symbols('x')
f = sp.cos(3*(sp.pi/2)*x)**3*(1-x**2)

basis = [1]
N = 7
ii = range(N)
for i in ii:
   basis.append(sp.cos(i*sp.pi*x))
   basis.append(sp.sin(i*sp.pi*x))

nBasis = len(basis)

# Compute projections using Sympy.
Q = [];
r = [];
for p in basis:
   a = sp.integrate(p*f,(x,-1,1)) # Going to be slow
   r.append(a)
   for q in basis:
      b = sp.integrate(p*q,(x,-1,1)) # Going to be slow
      Q.append(b)

# Solve for the coefficients
QQ = np.reshape(Q, (nBasis,nBasis)).astype(float)
rr = np.array(r).astype(float)
alp = np.round(np.linalg.pinv(QQ)@rr,2)

xx = np.linspace(-1,1,100)
F = sp.lambdify(x,f)
fhat = sum(alp*basis)
Fhat = sp.lambdify(x,fhat)

plt.figure(1)
plt.subplot(1,2,1)
plt.plot(xx,F(xx),xx,Fhat(xx),'--')
plt.legend(('True','Approximate'))
plt.xlabel('x')
plt.title('Approximation Using \n Trigonometric Basis Functions.')

plt.subplot(1,2,2)
plt.plot(xx,F(xx)-Fhat(xx))
plt.xlabel('x')
plt.title('Approximation Error.')

plt.tight_layout()</code></pre>
<p><a href="function_approximation.html#fig-periodic_function">Figure&nbsp;<span>9.1</span></a> shows that we are able to approximate <span class="math inline">\(f(x) = \sin(\frac{3\pi}{2}x)^3(1-x^2)\)</span> with <span class="math inline">\(15\)</span> basis functions with small errors. We can reduce the error further by increasing the number of basis functions.</p>
<pre class="{python}"><code>#| fig-cap: "Coefficients of basis functions."
#| layout-ncol: 1
#| label: fig-periodic_function_coeff
plt.stem(range(nBasis),alp)
plt.xlabel('Basis Index')
plt.title('Basis Coefficient')</code></pre>
<p><a href="function_approximation.html#fig-periodic_function_coeff">Figure&nbsp;<span>9.2</span></a> show the coefficients, which are projections of <span class="math inline">\(f(x) = \sin(\frac{3\pi}{2}x)^3(1-x^2)\)</span> on each of the <span class="math inline">\(15\)</span> basis functions. We see that most of the coefficients are zero indicating the function lies in a much smaller dimensional space than assumed.</p>
<p>The basis functions are essentially Fourier modes. Therefore, the approximation is nothing but Fourier series expansion of the function and the coefficients of the basis functions gives rise to the discrete Fourier transform of the signal. If chose wavelets as basis, then we would get wavelet transform of the function. Therefore, we can think of all these transforms as projections of a function on different basis functions.</p>
</section>
</section>
<section id="function-approximation-over-discrete-data" class="level3" data-number="19.3.3">
<h3 data-number="19.3.3" class="anchored" data-anchor-id="function-approximation-over-discrete-data"><span class="header-section-number">19.3.3</span> Function Approximation Over Discrete Data</h3>
<p>So far we have considered approximation of a function from its analytical form. It involves inner products which require integral of function products. The functions we have considered so far have analytical integrals. However, in more complicated scenarios, analytical integrals may not exist and we have to resort to numerical integrals. In more realistic scenarios, we may have to approximate a function from some data points. In both cases, we have to determine the optimal coefficients for the basis functions from discrete data points.</p>
<section id="quadrature-methods" class="level4" data-number="19.3.3.1">
<h4 data-number="19.3.3.1" class="anchored" data-anchor-id="quadrature-methods"><span class="header-section-number">19.3.3.1</span> Quadrature Methods</h4>
<p>Quadrature, in the context of numerical analysis, refers to the process of estimating the definite integral of a function. It is a fundamental technique used when an integral is too complex to be solved analytically, or when dealing with integrals of data points rather than known functions. The goal of quadrature is to approximate the area under a curve defined by a function over a specific interval.</p>
<p>The process typically involves approximating the function by a simpler one, often a polynomial, and then dividing the integration interval into smaller subintervals. The integral over each subinterval is estimated using this approximation, and the results are summed to approximate the total integral.</p>
<p>Common quadrature methods include:</p>
<ol type="1">
<li><p><strong>Trapezoidal Rule</strong>: This method approximates the area under the curve as a series of trapezoids and sums their areas. It is simple but less accurate for functions that are not approximately linear over the subintervals.</p></li>
<li><p><strong>Simpson’s Rule</strong>: Simpson’s Rule improves accuracy by approximating the function with second-degree (quadratic) polynomials. It’s more accurate than the trapezoidal rule for smooth functions.</p></li>
<li><p><strong>Gaussian Quadrature</strong>: This technique improves accuracy by strategically choosing the points within each interval where the function is evaluated. It can achieve higher accuracy with fewer evaluations than equally-spaced samples. We discuss it in more detail next.</p></li>
</ol>
<p>Gaussian quadrature is a sophisticated numerical method for approximating the integral of a function, particularly effective with polynomial and smooth functions. It is based on transforming the integral into a weighted sum of function values at strategically chosen points or abscissas.</p>
<p>These abscissas are the roots of orthogonal polynomials defined over the integration interval. For a standard Gaussian quadrature over the interval <span class="math inline">\([-1, 1]\)</span>, Legendre polynomials are utilized. These polynomials are orthogonal with respect to the weight function, typically set to <span class="math inline">\(1\)</span>, over the given interval. Mathematically, the orthogonality condition for Legendre polynomials <span class="math inline">\(P_n(x)\)</span> is expressed as <span class="math display">\[\int_{-1}^{1} P_m(x) P_n(x) \, dx = 0, \text{ for } m\neq n.\]</span></p>
<p>The abscissas are the roots of the Legendre polynomial of degree <span class="math inline">\(n\)</span>, providing <span class="math inline">\(n\)</span> distinct points within the interval. The integration process then hinges on these points, with each accompanied by a specific weight. These weights are derived from the properties of orthogonal polynomials and are calculated by integrating the corresponding Lagrange polynomial, which is constructed to be <span class="math inline">\(1\)</span> at the given abscissa and <span class="math inline">\(0\)</span> at others, over the interval. For Legendre polynomials, the weight formula simplifies to <span class="math display">\[w_i = \frac{2}{(1 - x_i^2) [P_n'(x_i)]^2},\]</span> where <span class="math inline">\(x_i\)</span> are the roots of <span class="math inline">\(P_n(x)\)</span> and <span class="math inline">\(P_n'(x_i)\)</span> is the derivative of <span class="math inline">\(P_n(x)\)</span> evaluated at <span class="math inline">\(x_i\)</span>.</p>
<p>The Gaussian quadrature approximates the integral by summing the products of function values at each abscissa and the corresponding weights: <span class="math display">\[
I \approx \sum_{i=1}^{n} \frac{2 f(x_i)}{(1 - x_i^2) [P_n'(x_i)]^2}.\]</span></p>
<p>This method is particularly efficient and accurate for polynomial functions, ensuring exact results for polynomials of degree up to <span class="math inline">\(2n-1\)</span> with just <span class="math inline">\(n\)</span> function evaluations.</p>
<p>We next use Gaussian quadrature to integrate polynomial functions and compare it with the analytical solution. Let us consider polynomial <span class="math inline">\(f(x) = x^3 - 3x^2 + 2x\)</span>. We will integrate this function over an interval <span class="math inline">\([0,5]\)</span>, both analytically and numerically using Gaussian quadrature, and then compare the results. The following Python code demonstrates it.</p>
<pre class="{python}"><code>#| layout-ncol: 1
import numpy as np
from scipy.integrate import quad, quadrature

# Define the polynomial function
def poly_function(x):
    return x**3 - 3*x**2 + 2*x

# Analytical integration of the polynomial
def analytical_integration(a, b):
    # Integral of the polynomial is x^4/4 - x^3 + x^2
    return (b**4)/4 - (b**3) + (b**2) - ((a**4)/4 - (a**3) + (a**2))

# Integration limits
a, b = 0, 5

# Perform the integration using Gaussian quadrature
result_gauss, _ = quadrature(poly_function, a, b)

# Calculate the analytical result
result_analytical = analytical_integration(a, b)

# Print the results
print(f"Gaussian Quadrature Result: {result_gauss}")
print(f"Analytical Result: {result_analytical}")
print(f"Error:{result_analytical - result_gauss}, which is machine precision.")</code></pre>
<p>This script uses the <code>quadrature</code> function from SciPy, which implements Gaussian quadrature for numerical integration. It also defines an <code>analytical_integration</code> function that computes the integral of the given polynomial analytically. We see that the numerical integration matches the analytical solution upto machine precision.</p>
<p>We next show the performance of Gaussian quadrature for integrating non-polynomial functions. We consider integration of <span class="math inline">\(f(x) = e^{-x^2}\)</span> over <span class="math inline">\([0,1]\)</span>, which analytically is <span class="math inline">\(\frac{\sqrt{\pi}}{2}\text{erf}(1)\)</span>, where <span class="math inline">\(\text{erf}(\cdot)\)</span> is the error function.</p>
<pre class="{python}"><code>#| layout-ncol: 1
import numpy as np
import math
from scipy.integrate import quad, quadrature

# Define the non-polynomial function
def non_poly_function(x):
    return np.exp(-x**2)

# Integration limits
a, b = 0, 1

# Perform the integration using Gaussian quadrature
result_gauss, _ = quadrature(non_poly_function, a, b)

# Calculate the "analytical" result using quad (numerical approximation)
result_analytical = math.sqrt(math.pi)*math.erf(1)/2

# Print the results
print(f"Gaussian Quadrature Result: {result_gauss}")
print(f"Analytical Result: {result_analytical}")
print(f"Error:{result_analytical - result_gauss}")</code></pre>
<p>The error is quite small, but not quite machine precision.</p>
</section>
<section id="least-square-approximation" class="level4" data-number="19.3.3.2">
<h4 data-number="19.3.3.2" class="anchored" data-anchor-id="least-square-approximation"><span class="header-section-number">19.3.3.2</span> Least-Square Approximation</h4>
<p>In this case, we assume we do not have analytical form of the function <span class="math inline">\(f(\x):\real^n\mapsto\real\)</span>, but only pairs of values <span class="math inline">\((\x_k,f(\x_k))\)</span> for <span class="math inline">\(k=1,\cdots, d\)</span>. Given a set of basis functions <span class="math inline">\(\Phib(\x)\)</span>, we want to determine <span class="math inline">\(\hat{f}(\x) := \Phib^T(\x)\alphab\)</span> such that the errors <span class="math inline">\(f(\x_k) - \hat{f}(\x_k)\)</span> for <span class="math inline">\(k=1,\cdots, d\)</span>; is minimized in some sense. If the error is zero, we say that <span class="math inline">\(\hat{f}(\x)\)</span> <em>interpolates</em> the data, else it <em>approximates</em> the data.</p>
<p>We define the error vector <span class="math inline">\(\vo{e}\)</span> as <span class="math display">\[
\vo{e} := \begin{bmatrix}e_1 \\ \vdots \\ e_d \end{bmatrix} =  \begin{bmatrix} f(\x_1) - \hat{f}(\x_1) \\ \vdots \\ f(\x_d) - \hat{f}(\x_d) \end{bmatrix} =\underbrace{\begin{bmatrix} f(\x_1)\\ \vdots \\ f(\x_d) \end{bmatrix}}_{=: \vo{b}} - \underbrace{\begin{bmatrix}\Phib^T(\x_1) \\ \vdots \\ \Phib^T(\x_d)\end{bmatrix}}_{=:\A}\alphab.
\]</span></p>
<p>The two norm of the error vector is given by, <span class="math display">\[\begin{align*}
\|\vo{e}\|_2^2 &amp;= \|\A\alphab - \vo{b} \|_2^2, \\
&amp;= (\A\alphab - \vo{b})^T(\A\alphab - \vo{b}),\\
&amp;= \alphab^T(\A^T\A)\alphab -2\alphab^T\A^T\vo{b} + \vo{b}^T\vo{b},
\end{align*}\]</span> which is quadratic in <span class="math inline">\(\alphab\)</span> and the optimal solution is given by <span id="eq-lsq_scattered_data"><span class="math display">\[
\alphab^\ast := (\A^T\A)^\dagger \A^T\vo{b}.
\tag{19.4}\]</span></span></p>
<p><em>Note</em>: If <span class="math inline">\(\A\)</span> is full rank, then <span class="math inline">\(\|\vo{e}\|_2^2=0\)</span> and we interpolate the data, i.e., the <span class="math inline">\(\hat{f}(\x)\)</span> passes through <span class="math inline">\((\x_k,f(\x_k))\)</span>.</p>
</section>
<section id="least-squares-approximation-using-legendre-polynomials." class="level4" data-number="19.3.3.3">
<h4 data-number="19.3.3.3" class="anchored" data-anchor-id="least-squares-approximation-using-legendre-polynomials."><span class="header-section-number">19.3.3.3</span> Least-Squares Approximation Using Legendre Polynomials.</h4>
<p>The following Python code illustrates a method for approximating a function using Legendre polynomials on scattered 1D data. Initially, a set of scattered data points is generated. These data points (x) are randomly selected and associated with values (y) derived from a noisy sine wave. The core of this approach lies in constructing a Vandermonde-like matrix for Legendre polynomials. Each column of this matrix, denoted as <span class="math inline">\(\A\)</span>, corresponds to a Legendre polynomial of a specific degree evaluated at the data points. To approximate the function, a least squares problem is formulated and solved using <code>np.linalg.lstsq</code>, which essentially returns the optimal solution given by <a href="references.html#eq-lsq_scattered_data">Equation&nbsp;<span>7.4</span></a>.</p>
<p>```{python, results=‘hide’} #| fig-cap: “Function approximation using Legendre polynomials.” #| layout-ncol: 1 import numpy as np import matplotlib.pyplot as plt</p>
</section>
</section>
</section>
</section>
<section id="generate-some-scattered-data" class="level1" data-number="20">
<h1 data-number="20"><span class="header-section-number">20</span> Generate some scattered data</h1>
<p>np.random.seed(0) x = np.sort(np.random.rand(100)) y = np.sin(2 * np.pi * x) + np.random.normal(0, 0.1, x.size)</p>
</section>
<section id="function-to-approximate-the-data-using-legendre-polynomials-with-linear-algebra" class="level1" data-number="21">
<h1 data-number="21"><span class="header-section-number">21</span> Function to approximate the data using Legendre polynomials with linear algebra</h1>
<p>def legendre_approximation(x, y, degree): # Construct the Vandermonde matrix for Legendre polynomials A = np.zeros((len(x), degree + 1)) for i in range(degree + 1): A[:, i] = np.polynomial.legendre.legval(x, [0]*i + [1])</p>
<pre><code># Solve the least squares problem
coeffs, _, _, _ = np.linalg.lstsq(A, y, rcond=None)
return np.polynomial.legendre.Legendre(coeffs)</code></pre>
</section>
<section id="approximating-the-data" class="level1" data-number="22">
<h1 data-number="22"><span class="header-section-number">22</span> Approximating the data</h1>
<p>degree = 5 # Degree of the Legendre polynomial p = legendre_approximation(x, y, degree)</p>
</section>
<section id="plotting-the-original-data-and-the-approximation" class="level1" data-number="23">
<h1 data-number="23"><span class="header-section-number">23</span> Plotting the original data and the approximation</h1>
<p>plt.figure(figsize=(12, 6)) plt.scatter(x, y, label=‘Original data’) plt.plot(x, p(x), label=f’Approximation with Legendre basis’, color=‘red’) plt.legend() plt.xlabel(‘x’) plt.ylabel(‘y’) plt.title(f’Function Approximation with Legendre Polynomials Up to Degree {degree}’) plt.show()</p>
<pre><code>

#### Runge Phenomenon
The Runge phenomenon is a notable issue in function approximation using polynomial interpolation. It occurs when high-degree polynomial interpolants are used to approximate certain types of functions, leading to counterintuitive and problematic results.

This behavior is most prominently seen when using equidistant interpolation points. As the degree of the interpolating polynomial increases, the approximation becomes excellent near the center of the interval but worsens towards the ends. The polynomial starts to exhibit large oscillations, particularly near the endpoints, which severely affects the accuracy of the interpolation. This counterintuitive result is contrary to the expectation that higher-degree polynomials should provide better approximations.

The Runge phenomenon highlights a fundamental limitation of polynomial interpolation, particularly with equidistant points. It can be mitigated by choosing points that are not evenly spaced, like Chebyshev nodes. Another solution is to use methods like spline interpolation, which involve breaking the range into smaller pieces and using lower-degree polynomials over each piece. This approach avoids the big swings and inaccuracies seen with high-degree polynomials. 


The Runge phenomenon is demonstrated with the approximation of the functon $f(x) = \frac{1}{1 + 25x^2}$ using high-degree polynomials with both equidistant points (which leads to the Runge phenomenon) and Chebyshev nodes (to mitigate the phenomenon). It is demonstrated in the following Python code.

```{python, results='hide'}
#| fig-cap: "Demonstration of Runge phenomenon and its mitigation."
#| layout-ncol: 1
#| label: fig-runge_phenomenon
import numpy as np
import matplotlib.pyplot as plt
from numpy.polynomial.polynomial import Polynomial

# Define Runge's function
def runge_function(x):
    return 1 / (1 + 25 * x**2)

# Create equidistant and Chebyshev nodes
def create_nodes(n, type='equidistant'):
    if type == 'equidistant':
        return np.linspace(-1, 1, n)
    elif type == 'chebyshev':
        return np.cos((2 * np.arange(1, n + 1) - 1) / (2 * n) * np.pi)

# Interpolate the function
def interpolate_runge(n, node_type):
    x_nodes = create_nodes(n, node_type)
    y_nodes = runge_function(x_nodes)
    p = Polynomial.fit(x_nodes, y_nodes, deg=n-1)
    return p

# Number of nodes
n = 15

# Interpolate using equidistant nodes and Chebyshev nodes
p_equidistant = interpolate_runge(n, 'equidistant')
p_chebyshev = interpolate_runge(n, 'chebyshev')

# Plotting
x = np.linspace(-1, 1, 1000)
plt.figure(figsize=(12, 6))

plt.plot(x, runge_function(x), label='Runge Function', color='black')
plt.plot(x, p_equidistant(x), label='Equidistant Nodes', linestyle='--')
plt.plot(x, p_chebyshev(x), label='Chebyshev Nodes', linestyle='-.')
plt.scatter(create_nodes(n, 'equidistant'), runge_function(create_nodes(n, 'equidistant')), color='blue')
plt.scatter(create_nodes(n, 'chebyshev'), runge_function(create_nodes(n, 'chebyshev')), color='red')

plt.legend()
plt.title('Demonstration of Runge Phenomenon and its Mitigation')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.grid(True)
plt.show()</code></pre>
<section id="local-vs-global-basis-functions" class="level3" data-number="23.0.1">
<h3 data-number="23.0.1" class="anchored" data-anchor-id="local-vs-global-basis-functions"><span class="header-section-number">23.0.1</span> Local vs Global Basis Functions</h3>
<p>In general, the basis functions could be globally or locally supported. Globally supported basis functions, such as polynomials and trigonometric functions, are defined over the entire domain of interest. They offer smoothness and continuity, making them ideal for approximating smooth, globally defined functions with high accuracy. However, they are sensitive to local changes and can be computationally intensive due to dense matrix systems, limiting their practicality for large-scale problems.</p>
<p>In contrast, local basis functions, like piecewise polynomials in spline interpolation and shape functions in finite element methods, are defined over small subdomains. They provide efficient computation and local control, effectively handling functions with local irregularities or discontinuities. Despite these advantages, ensuring continuity between elements can be challenging, and their piecewise nature adds complexity to their formulation. The choice between these two types of basis functions depends on the problem’s requirements: global basis functions are preferred for smooth, entire-domain applications, while local basis functions are favored for large-scale, locally varying problems.</p>
<p>Locally supported functions are a class of functions in mathematics that are non-zero only over a specific, limited range and zero everywhere else. This property makes them highly valuable in various applications like finite element analysis, wavelet transforms, and spline interpolation. Below are some notable examples of locally supported functions, along with their mathematical details:</p>
<section id="splines" class="level4" data-number="23.0.1.1">
<h4 data-number="23.0.1.1" class="anchored" data-anchor-id="splines"><span class="header-section-number">23.0.1.1</span> Splines</h4>
<p>A spline is a piecewise polynomial function used in interpolation and approximation. In simple terms, a spline is a series of polynomial segments strung together, with each polynomial defined over a small subinterval.</p>
<p>For a spline of degree <span class="math inline">\(n\)</span>, each piece is a polynomial of degree <span class="math inline">\(n\)</span> defined on a subinterval <span class="math inline">\([x_i, x_{i+1}]\)</span>. For example, a cubic spline (<span class="math inline">\(n = 3\)</span>) is a piecewise-defined function where each piece is a cubic polynomial.</p>
<p>Splines have continuity properties at the points where the polynomial pieces meet (called knots). For instance, cubic splines are often constructed to have continuous first and second derivatives across these knots.</p>
<p>Here is a Python code that demonstrates cubic spline interpolation.</p>
<p>```{python, results=‘hide’} #| layout-ncol: 1 import numpy as np import matplotlib.pyplot as plt from scipy.interpolate import interp1d</p>
</section>
</section>
</section>
<section id="sample-data-points" class="level1" data-number="24">
<h1 data-number="24"><span class="header-section-number">24</span> Sample data points</h1>
<p>x = np.array([0, 1, 2, 3, 4, 5]) y = np.array([0, 0.8, 0.9, 0.1, -0.8, -1.0])</p>
</section>
<section id="create-a-cubic-spline-interpolation-of-the-data" class="level1" data-number="25">
<h1 data-number="25"><span class="header-section-number">25</span> Create a cubic spline interpolation of the data</h1>
<p>cubic_spline = interp1d(x, y, kind=‘cubic’)</p>
</section>
<section id="generate-more-points-to-evaluate-the-spline" class="level1" data-number="26">
<h1 data-number="26"><span class="header-section-number">26</span> Generate more points to evaluate the spline</h1>
<p>x_fine = np.linspace(0, 5, 500) y_fine = cubic_spline(x_fine)</p>
</section>
<section id="plot-the-original-data-points-and-the-interpolated-values" class="level1" data-number="27">
<h1 data-number="27"><span class="header-section-number">27</span> Plot the original data points and the interpolated values</h1>
<p>plt.figure(figsize=(10, 6)) plt.plot(x, y, ‘o’, label=‘Data points’) plt.plot(x_fine, y_fine, ‘-’, label=‘Cubic spline interpolation’) plt.title(‘Spline Interpolation Example’) plt.xlabel(‘x’) plt.ylabel(‘y’) plt.legend() plt.show()</p>
<pre><code>
#### B-Splines
B-splines or Basis splines are a generalization of the spline concept. They provide a basis for the spline space and are defined by a degree and a set of knot points. A B-spline of degree $n$ is defined piecewise by polynomial segments of degree $n$. Its shape is influenced by a set of control points, and it is non-zero only in a range defined by $n+1$ consecutive knots. B-splines have local control (adjusting one control point affects the spline shape only in a local region) and are widely used in computer graphics and data fitting due to their stability and flexibility.

The following code shows the B-spline basis functions.

```{python,results='hide'}
#| fig-cap: "B-Spline basis functions satisfying partition of unity, i.e., the sum of all the basis function is equal to one at every point. Note that each basis function is locally supported, i.e., it goes to zero far away from the location of its peak value."
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import BSpline

# Define the degree of the B-spline
degree = 3

# Define a set of knots
# Note: For partition of unity, we need to add degree+1 equal knots at both ends
knots = np.array([0, 0, 0, 0, 1, 2, 3, 4, 4, 4, 4])

# Generate B-spline basis functions
x = np.linspace(knots[degree], knots[-degree-1], 100)
basis_functions = []
for i in range(len(knots) - degree - 1):
    coeffs = np.zeros(len(knots) - degree - 1)
    coeffs[i] = 1
    basis_function = BSpline(knots, coeffs, degree)
    basis_functions.append(basis_function(x))

# Plot the B-spline basis functions
plt.figure(figsize=(10, 6))
for i, basis_function in enumerate(basis_functions):
    plt.plot(x, basis_function, label=f'B-spline basis {i+1}')

# Plot the sum of all basis functions to demonstrate partition of unity
plt.plot(x, np.sum(basis_functions, axis=0), 'k--', label='Sum of all basis functions');

plt.title('B-spline Basis Functions and Partition of Unity')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()</code></pre>
<p>We next show how to interpolate data using B-Splines.</p>
<p>```{python,results=‘hide’} #| layout-ncol: 1 import numpy as np import matplotlib.pyplot as plt from scipy.interpolate import splrep, splev</p>
</section>
<section id="sample-data-points-1" class="level1" data-number="28">
<h1 data-number="28"><span class="header-section-number">28</span> Sample data points</h1>
<p>x = np.linspace(0, 10, 10) y = np.sin(x)</p>
</section>
<section id="create-a-b-spline-representation-of-the-data" class="level1" data-number="29">
<h1 data-number="29"><span class="header-section-number">29</span> Create a B-Spline representation of the data</h1>
<p>tck = splrep(x, y, s=0)</p>
</section>
<section id="generate-more-points-to-evaluate-the-spline-1" class="level1" data-number="30">
<h1 data-number="30"><span class="header-section-number">30</span> Generate more points to evaluate the spline</h1>
<p>x_fine = np.linspace(0, 10, 200) y_fine = splev(x_fine, tck)</p>
</section>
<section id="plot-the-original-data-points-and-the-b-spline-interpolation" class="level1" data-number="31">
<h1 data-number="31"><span class="header-section-number">31</span> Plot the original data points and the B-Spline interpolation</h1>
<p>plt.figure(figsize=(10, 6)); plt.plot(x, y, ‘o’, label=‘Data points’); plt.plot(x_fine, y_fine, ‘-’, label=‘B-Spline interpolation’); plt.title(‘B-Spline Interpolation Example’); plt.xlabel(‘x’); plt.ylabel(‘y’); plt.legend(); plt.show()</p>
<pre><code>
#### Wavelets
Wavelets are functions used to divide a given function or continuous-time signal into different scale components. They have localized support in both time and frequency domains. A wavelet function $\psi(t)$ is typically defined over a finite interval and is used to generate a family of functions through scaling and translation: $\psi_{a,b}(t) = \frac{1}{\sqrt{a}} \psi\left(\frac{t - b}{a}\right)$, where $a$ and $b$ are the scaling and translation parameters. Wavelets are useful in signal processing and image compression, as they can represent data at different levels of resolution and are particularly effective in analyzing transient or high-frequency features.

Here is a Python code that applies Daubechies wavelets to approximate a function. We first show the Daubechies wavelets followed by approximation of a sine function with it.

```{python,results='hide'}
#| fig-cap: "Daubechies wavelets with increasing complexity."
#| layout-ncol: 1
import pywt
import matplotlib.pyplot as plt
import numpy as np

# Function to plot a wavelet
def plot_wavelet(wavelet, ax, title):
    wavelet_function, scaling_function, x_values = wavelet
    ax.plot(x_values, wavelet_function, label="Wavelet Function")
    ax.plot(x_values, scaling_function, label="Scaling Function")
    ax.set_title(title)
    ax.legend()

# Create a figure with subplots
fig, axs = plt.subplots(2, 2, figsize=(12, 8))

# List of Daubechies wavelets to plot
wavelets = ['db1', 'db2', 'db3', 'db4']

# Plot each wavelet
for i, wavelet_name in enumerate(wavelets):
    wavelet = pywt.Wavelet(wavelet_name)
    phi, psi, x = pywt.Wavelet(wavelet_name).wavefun(level=5)
    plot_wavelet((psi, phi, x), axs[i // 2, i % 2], f'Daubechies {wavelet_name.upper()}')

# Display the plot
plt.tight_layout()
plt.show()</code></pre>
<p>```{python, results=‘hide’} #| fig-cap: “Approximation of <span class="math inline">\(\\sin(x)\)</span> with Daubechies wavelets.” #| layout-ncol: 1 import numpy as np import matplotlib.pyplot as plt import pywt</p>
</section>
<section id="define-the-function-to-approximate" class="level1" data-number="32">
<h1 data-number="32"><span class="header-section-number">32</span> Define the function to approximate</h1>
<p>def f(x): return np.sin(x)</p>
</section>
<section id="generate-sample-data" class="level1" data-number="33">
<h1 data-number="33"><span class="header-section-number">33</span> Generate sample data</h1>
<p>x = np.linspace(0, 4 * np.pi, 400) y = f(x)</p>
</section>
<section id="choose-a-wavelet-type-and-level-of-decomposition" class="level1" data-number="34">
<h1 data-number="34"><span class="header-section-number">34</span> Choose a wavelet type and level of decomposition</h1>
<p>wavelet_type = ‘db1’ # Daubechies wavelet level = 3</p>
</section>
<section id="compute-the-wavelet-coefficients" class="level1" data-number="35">
<h1 data-number="35"><span class="header-section-number">35</span> Compute the wavelet coefficients</h1>
<p>coeffs = pywt.wavedec(y, wavelet_type, level=level)</p>
</section>
<section id="reconstruct-the-signal-from-the-coefficients" class="level1" data-number="36">
<h1 data-number="36"><span class="header-section-number">36</span> Reconstruct the signal from the coefficients</h1>
<p>y_approx = pywt.waverec(coeffs, wavelet_type)</p>
</section>
<section id="plot-the-original-and-approximated-function" class="level1" data-number="37">
<h1 data-number="37"><span class="header-section-number">37</span> Plot the original and approximated function</h1>
<p>plt.figure(figsize=(10, 6)) plt.plot(x, y, label=‘Original Function’) plt.plot(x, y_approx, label=‘Wavelet Approximation’, linestyle=‘–’) plt.title(‘Function Approximation Using Wavelets’) plt.xlabel(‘x’) plt.ylabel(‘f(x)’) plt.legend() plt.show()</p>
<pre><code>
#### Radial Basis Functions
Radial Basis Functions (RBFs) are a class of functions with radial symmetry. The value of the function depends only on the distance from a central point, termed the center of the RBF. This distance is typically measured using the Euclidean norm, making the function spherically symmetric in multidimensional space.

Mathematically, an RBF $\phi$ is a function such that $\phi(\vo{x}) = \phi(\|\vo{x} - \vo{c}\|)$, where $\vo{x}$ is a point in space, $\vo{c}$ is the center of the RBF, and $\|\vo{x} - \vo{c}\|$ is the distance from $\vo{x}$ to $\vo{c}$.

Common Types of Radial Basis Functions:

1. **Gaussian**: $\phi(r) = e^{-(\varepsilon r)^2}$, where $\varepsilon$ is a scaling parameter. Gaussian RBFs are widely used due to their smooth and rapidly decaying nature.

2. **Multiquadric**: $\phi(r) = \sqrt{1 + (\varepsilon r)^2}$. These RBFs are smooth and have been used effectively in various interpolation tasks.

3. **Inverse Multiquadric**: $\phi(r) = \frac{1}{\sqrt{1 + (\varepsilon r)^2}}$. They have a similar form to multiquadric RBFs but behave differently as the distance increases.

4. **Polyharmonic Splines**: $\phi(r) = r^k$, where $k$ is typically an integer. These functions are particularly useful for their smoothness properties in higher dimensions.

Radial Basis Functions stand out due to their flexibility and effectiveness in handling multidimensional data, their ability to create smooth interpolants, and their applicability in a wide range of scientific and engineering disciplines. Their unique radial property ensures that the influence of a point diminishes with distance, making them suitable for localized approximations in high-dimensional spaces.

The following code uses multiquadric radial basis functions to approximate $\sin(x)$.

```{python, results='hide'}
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import Rbf

# Define the function to approximate
def f(x):
    return np.sin(x)

# Generate sample data (you can change this to any other function or data points)
x_sample = np.linspace(0, 10, 10)
y_sample = f(x_sample)

# Create the Radial Basis Function interpolator
rbf_interpolator = Rbf(x_sample, y_sample, function='multiquadric')

# Generate more points to evaluate the RBF interpolator
x_fine = np.linspace(0, 10, 100)
y_fine = rbf_interpolator(x_fine)

# Plot the original function, sample points, and RBF approximation
plt.figure(figsize=(10, 6))
plt.plot(x_sample, y_sample, 'o', label='Sample points')
plt.plot(x_fine, f(x_fine), label='Original function')
plt.plot(x_fine, y_fine, label='RBF approximation', linestyle='--')
plt.title('Function Approximation using Radial Basis Functions')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.show()</code></pre>
<p>RBF interpolation is especially useful for scattered data in multiple dimensions, but this example shows its application in a simple 1D case for simplicity.</p>
<p>We next show how multiquadric RBFs can be used to approximate <span class="math inline">\(\sin(x)\cos(y)\)</span> from scattered data.</p>
<p>```{python, results=‘hide’} #| fig-cap: “Function approximation in 2D with radial basis functions.” #| layout-ncol: 1 import numpy as np import matplotlib.pyplot as plt from scipy.interpolate import Rbf</p>
</section>
<section id="define-the-2d-function-to-approximate" class="level1" data-number="38">
<h1 data-number="38"><span class="header-section-number">38</span> Define the 2D function to approximate</h1>
<p>def f(x, y): return np.sin(x) * np.cos(y)</p>
</section>
<section id="create-scattered-data-points" class="level1" data-number="39">
<h1 data-number="39"><span class="header-section-number">39</span> Create scattered data points</h1>
<p>nSamples = 200; x_sample = np.random.uniform(0, 5, nSamples) y_sample = np.random.uniform(0, 5, nSamples) z_sample = f(x_sample, y_sample)</p>
</section>
<section id="create-the-radial-basis-function-interpolator" class="level1" data-number="40">
<h1 data-number="40"><span class="header-section-number">40</span> Create the Radial Basis Function interpolator</h1>
<p>rbf_interpolator = Rbf(x_sample, y_sample, z_sample, function=‘multiquadric’)</p>
</section>
<section id="create-a-grid-to-evaluate-the-interpolator" class="level1" data-number="41">
<h1 data-number="41"><span class="header-section-number">41</span> Create a grid to evaluate the interpolator</h1>
<p>x_grid, y_grid = np.meshgrid(np.linspace(0, 5, 100), np.linspace(0, 5, 100)) z_grid = rbf_interpolator(x_grid, y_grid)</p>
</section>
<section id="plot-the-results" class="level1" data-number="42">
<h1 data-number="42"><span class="header-section-number">42</span> Plot the results</h1>
<p>fig = plt.figure(figsize=(12, 6))</p>
</section>
<section id="original-function" class="level1" data-number="43">
<h1 data-number="43"><span class="header-section-number">43</span> Original function</h1>
<p>ax1 = fig.add_subplot(121, projection=‘3d’) ax1.plot_surface(x_grid, y_grid, f(x_grid, y_grid), cmap=‘viridis’) ax1.set_title(‘Original Function’) ax1.set_xlabel(‘X’) ax1.set_ylabel(‘Y’) ax1.set_zlabel(‘f(X, Y)’)</p>
</section>
<section id="rbf-interpolation" class="level1" data-number="44">
<h1 data-number="44"><span class="header-section-number">44</span> RBF Interpolation</h1>
<p>ax2 = fig.add_subplot(122, projection=‘3d’) ax2.plot_surface(x_grid, y_grid, z_grid, cmap=‘viridis’) ax2.scatter(x_sample, y_sample, 0.1+z_sample, color=‘red’) # Scattered data points. Added 0.1 to make the dots visible. ax2.set_title(‘RBF Interpolation’) ax2.set_xlabel(‘X’) ax2.set_ylabel(‘Y’) ax2.set_zlabel(‘f(X, Y)’)</p>
<p>plt.tight_layout() plt.show()</p>
<pre><code>


::: {.content-hidden}
{{&lt; include macros.tex &gt;}}
:::

# Functions &amp; Function Spaces
## Functions
In this book, we will consider functions to be mappings from a set numbers to another set of numbers. In general it is represented as $\vo{f}(\x):\real^n \mapsto \real^m$, which states that $\vo{f}$ maps an object $\x\in\real^n$ to objects in $\real^m$. That is, $$\y := \vo{f}(\x) = \begin{bmatrix} f_1(\x) \\ \vdots \\ f_m(\x) \end{bmatrix},$$ or componentwise $$y_i := f_i(\x),$$ where $f_i(\x):\real^n \mapsto \real$. 

We can define functions between sets of complex numbers as well.

### Derivatives
#### Gradient 
Given scalar function with vector arguments, i.e. $f(\x): \real^n \mapsto \real$, the gradient of $f(\x)$ is denoted by $\nablab f(\x)$ and defined as
$$
\nablab f(\x) := \begin{bmatrix}\frac{\partial f(\x)}{\partial x_1} \\ \vdots \\ \frac{\partial f(\x)}{\partial x_n}\end{bmatrix}, \text{ where  } \x :=\begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}.
$$
Therefore, the gradient of a scalar function with vector inputs, is a *vector*.

#### Jacobian
Given vector function with vector arguments, i.e. $\vo{f}(\x): \real^n \mapsto \real^m$, the jacobian of $\vo{f}(\x)$ is denoted by $\nablab \vo{f}(\x)$ and defined as
$$
\vo{J} = \nablab \vo{f}(\x) := \begin{bmatrix}
\frac{\partial f_1(\x)}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1(\x)}{\partial x_n}\\
\vdots &amp; &amp; \vdots\\
\frac{\partial f_m(\x)}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m(\x)}{\partial x_n}\\
\end{bmatrix}, 
$$
where
$$
\vo{f}(\x) :=\begin{bmatrix} f_1(\x) \\ \vdots \\ f_m(\x)\end{bmatrix} \text{ and } \x :=\begin{bmatrix} x_1 \\ \vdots \\ x_n\end{bmatrix}.
$$
Therefore, the jacobian of a vector function with vector inputs, is a *matrix*.

#### Hessian
Hessian of a scalar function with vector arguments is a *symmetric matrix*. It is (ambiguously) denoted by $\nablab^2 f(\x)$ or the letter $\vo{H}$ and defined by,
$$
\vo{H} = \nablab^2 f(\x) := \nablab(\nablab f(\x)) = \begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} &amp; \frac{\partial^2 f}{\partial x_1\partial x_2} &amp; \cdots &amp;\frac{\partial^2 f}{\partial x_1\partial x_n}\\
\frac{\partial^2 f}{\partial x_2\partial x_1} &amp; \frac{\partial^2 f}{\partial x_2^2} &amp; \cdots &amp;\frac{\partial^2 f}{\partial x_2\partial x_n}\\
\vdots &amp; \vdots &amp; &amp; \vdots\\
\frac{\partial^2 f}{\partial x_n\partial x_1} &amp; \frac{\partial^2 f}{\partial x_n^2} &amp; \cdots &amp;\frac{\partial^2 f}{\partial x_n^2}\\
\end{bmatrix}.
$$

#### Divergence of a Vector Function
The divergence of a vector function with vector arguments  $\vo{f}(\x): \real^n \mapsto \real^n$ is a *scalar*. It is represented as $\nablab \cdot \vo{f}(\x)$ and defined as
$$
\nablab\cdot\vo{f}(\x) := \sum_{i=1}^n \frac{\partial f_i(\x)}{\partial x_i}.
$$

### Automatic Differentiation
Automatic Differentiation (AD), also known as algorithmic differentiation or autodiff, is a powerful computational technique employed in various scientific, engineering, and mathematical fields.  AD is different from symbolic differentiation (e.g., symbolic algebra) and numerical differentiation (e.g., finite differences) in that it provides *exact* gradients efficiently, even for complex and high-dimensional functions. We can efficiently use AD to compute gradients, Jacobians, Hessians, and divergences of various functions, even if these functions are realized as computer code.

At its core, AD is based on the principles of the chain rule from calculus. It takes complex functions and decomposes them into a sequence of elementary operations. These operations are then evaluated not only for their values but also for their derivatives with respect to the input variables. AD computes gradients efficiently by combining these derivatives through the chain rule, making it a valuable tool for numerical optimization, sensitivity analysis, and machine learning.

**Elementary Operations**

In AD, elementary operations such as addition, subtraction, multiplication, division, and trigonometric functions are integral components of the process. Specialized functions are designed to evaluate these operations, and they ensure that both the function's value and its derivative are computed in a single pass.

**Forward Mode vs. Reverse Mode**

AD can be executed in two primary modes: forward mode and reverse mode. Forward mode AD calculates derivatives one input variable at a time while keeping others constant. It is efficient when the number of input variables is limited. In contrast, reverse mode AD computes derivatives for all input variables simultaneously, making it more suitable for functions with numerous input variables.

**Computational Graph**

AD often employs a computational graph to represent the sequence of elementary operations. The graph consists of nodes representing variables and operations and edges denoting dependencies. During graph evaluation, both function values and derivatives are computed, enabling a systematic and precise differentiation process.

**Automatic vs. Manual Differentiation**

While AD can be manually implemented, automatic implementations using specialized software tools and libraries offer greater flexibility and scalability. These automatic implementations can handle complex functions without the need for manual intervention, saving time and effort in computing gradients.

**Advantages of Automatic Differentiation**

The precision and efficiency of AD make it an invaluable tool for gradient-based optimization, sensitivity analysis, and scientific computing. AD provides exact gradients with minimal numerical errors, avoiding the numerical instability issues often associated with finite differences.

To perform automatic differentiation in Python, we can use specialized libraries like NumPy and *autograd*. Here's a Python code example using the autograd library to demonstrate automatic differentiation:

```{python}
import autograd.numpy as np
from autograd import grad

# Define a function for which you want to compute the derivative
def my_function(x):
    return x**3 + 2*x**2 + 1

# Use autograd to compute the derivative of the function
derivative_func = grad(my_function)

# Evaluate the derivative at a specific point
x_value = 2.0
derivative_at_x = derivative_func(x_value)

# Print the result
print(f"The derivative of the function at x = {x_value} is {derivative_at_x}")</code></pre>
<p>Machine learning frameworks such as TensorFlow, JAX, and PyTorch provide very efficient AD libraries that can leverage GPUs and TPUs to perform differentiations for very complex functions.</p>
<p><strong>Dual Numbers and Their Application in Automatic Differentiation</strong> Dual numbers are a mathematical construct that extends real numbers to enable the automatic computation of derivatives. They are a fundamental tool in the field of automatic differentiation (AD), a technique widely used in scientific computing, optimization, and machine learning.</p>
<p>A dual number is represented as <span class="math inline">\(a + b\epsilon\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are real numbers, and <span class="math inline">\(\epsilon\)</span> is a symbol with the property <span class="math inline">\(\epsilon^2 = 0\)</span>. This property is crucial, as it allows us to calculate derivatives efficiently.</p>
<p>Dual numbers follow specific arithmetic rules:</p>
<ul>
<li><strong>Addition:</strong> <span class="math inline">\((a + b\epsilon) + (c + d\epsilon) = (a + c) + (b + d)\epsilon\)</span></li>
<li><strong>Subtraction:</strong> <span class="math inline">\((a + b\epsilon) - (c + d\epsilon) = (a - c) + (b - d)\epsilon\)</span></li>
<li><strong>Multiplication:</strong> <span class="math inline">\((a + b\epsilon) (c + d\epsilon) = (ac) + (ad + bc)\epsilon\)</span></li>
<li><strong>Division:</strong> <span class="math inline">\((a + b\epsilon) / (c + d\epsilon) = (a / c) + ((bc - ad) / c^2)\epsilon\)</span></li>
</ul>
<p>Dual numbers play a pivotal role in AD, allowing for the precise and efficient computation of these derivatives. The fundamental idea is to replace an input variable <span class="math inline">\(x\)</span> in a function <span class="math inline">\(f(x)\)</span> with <span class="math inline">\(x + \epsilon\)</span>. By evaluating <span class="math inline">\(f(x + \epsilon)\)</span>, we obtain a dual number representing both the function’s value and its derivative at the point <span class="math inline">\(x\)</span>. The coefficient of <span class="math inline">\(\epsilon\)</span> in this dual number is the exact derivative of <span class="math inline">\(f(x)\)</span>.</p>
<p>Dual numbers are primarily used in the forward mode of AD. In this mode, derivatives are computed incrementally in a “forward pass” from input variables to the output of a function. This allows for the efficient calculation of gradients, making it well-suited for problems with many input variables.</p>
<p>The following Python code implements a dual number class and uses it to compute derivatives of functions.</p>
<pre class="{python}"><code>class DualNumber:
    def __init__(self, real, dual):
        self.real = real  # Real part
        self.dual = dual  # Dual part

    def __add__(self, other):
        return DualNumber(self.real + other.real, 
            self.dual + other.dual)

    def __mul__(self, other):
        return DualNumber(self.real * other.real, 
             self.real * other.dual + self.dual * other.real)

    def __str__(self):
        return f"{self.real} + {self.dual}"

# Function to differentiate
def f(x):
    # Example function: f(x) = x^2
    return x * x

# Compute derivative at x = 3
x = 3
dx = DualNumber(x, 1)
y = f(dx)

print(f"Value of f(x) at x = {x}: {y.real}")
print(f"Derivative of f(x) at x = {x}: {y.dual}")</code></pre>
<section id="function-spaces" class="level2" data-number="44.1">
<h2 data-number="44.1" class="anchored" data-anchor-id="function-spaces"><span class="header-section-number">44.1</span> Function Spaces</h2>
<p>In machine learning, function spaces are mathematical constructs that describe sets of functions with common properties. These spaces are crucial for understanding the types of functions that can be learned by models and for analyzing the behavior of algorithms.</p>
<section id="vector-spaces" class="level3" data-number="44.1.1">
<h3 data-number="44.1.1" class="anchored" data-anchor-id="vector-spaces"><span class="header-section-number">44.1.1</span> Vector Spaces</h3>
<p>Vector spaces are a fundamental concept in linear algebra and are integral to many aspects of machine learning. Understanding vector spaces provides a foundation for grasping more complex machine learning algorithms and concepts.</p>
<p>A vector space is a collection of objects known as vectors, which can be added together and multiplied by numbers (scalars). Mathematically, a vector space must satisfy a set of axioms related to vector addition and scalar multiplication. In machine learning, vectors often represent data points, and the operations on these vectors follow the rules of vector spaces.</p>
<p>Let <span class="math inline">\(\set{V}\)</span> be a vector spaces (often <span class="math inline">\(\set{V} \subset \real^n\)</span>), the for elements in <span class="math inline">\(\set{V}\)</span>, the following properties hold:</p>
<ul>
<li><strong>Closure</strong>: The sum of any two vectors in the space is also in the space, i.e., <span class="math display">\[\forall \x,\y\in\set{V},\; \x+\y \in\set{V}.\]</span></li>
<li><strong>Associativity of Addition</strong>: The order of addition does not change the result, i.e., <span class="math display">\[\x + \y = \y + \x.\]</span></li>
<li><strong>Existence of Additive Identity</strong>: There is a vector (the zero vector <span class="math inline">\(\vo{0}\in\set{V}\)</span>) that, when added to any vector, leaves the vector unchanged, i.e., <span class="math display">\[\x + \vo{0} = \x.\]</span></li>
<li><strong>Existence of Additive Inverse</strong>: For every vector <span class="math inline">\(\x\in\set{V}\)</span>, there is another vector <span class="math inline">\(-\x\in\set{V}\)</span> that, when added together, results in the zero vector, i.e., <span class="math display">\[ \x + (-\x) = \vo{0}.\]</span></li>
<li><strong>Distributivity and Scalar Multiplication</strong>: Scalars can multiply vectors, and this operation is distributive over vector addition and scalar addition, i.e., for <span class="math inline">\(\alpha, \beta \in\real\)</span> <span class="math display">\[\begin{align*}
&amp; \alpha\x \in \set{V}, \\
&amp; \alpha(\x+\y) = \alpha\x + \alpha\y,\\
&amp; (\alpha+\beta)\x = \x(\alpha + \beta) = \alpha\x + \beta\x.
\end{align*}\]</span></li>
</ul>
<section id="applications-in-machine-learning" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applications-in-machine-learning">Applications in Machine Learning</h4>
<p><strong>Feature Representation</strong>: In machine learning, data points (like images, text, or audio) are often represented as vectors in a high-dimensional space. Each dimension corresponds to a feature of the data point.</p>
<p><strong>Model Parameters</strong>: Many machine learning models, such as linear regression or neural networks, have parameters that are represented as vectors. For example, the weights in a neural network can be thought of as vectors in a vector space.</p>
<p><strong>Operations on Data</strong>: Operations like calculating the distance between data points, dot products for similarity, or vector addition and subtraction are all based on vector space properties.</p>
<p><strong>Dimensionality Reduction</strong>: Techniques like Principal Component Analysis (PCA) transform data into a lower-dimensional vector space while trying to preserve the variance in the data.</p>
<p><strong>Text Analysis (NLP)</strong>: In natural language processing, words or sentences can be represented as vectors in a space where distances between vectors are related to semantic similarity (word embeddings like Word2Vec).</p>
<p><strong>Image Recognition</strong>: In image processing, an image can be represented as a vector where each dimension corresponds to the intensity of a pixel.</p>
<p><strong>Time Series Analysis</strong>: Each point in a time series can be considered a vector in a multi-dimensional space, where each dimension corresponds to a different time point.</p>
<p>Understanding vector spaces allows machine learning practitioners to manipulate and analyze data effectively and provides a foundation for more advanced topics like optimization, kernel methods, and deep learning.</p>
</section>
</section>
<section id="hilbert-spaces" class="level3" data-number="44.1.2">
<h3 data-number="44.1.2" class="anchored" data-anchor-id="hilbert-spaces"><span class="header-section-number">44.1.2</span> Hilbert Spaces</h3>
<p>Hilbert spaces are an essential concept in functional analysis and play a significant role in various machine learning algorithms. They provide a mathematical framework for infinite-dimensional spaces, extending the methods of vector algebra and calculus from the two-dimensional plane and three-dimensional space to spaces with any finite or infinite number of dimensions.</p>
<p>A Hilbert space is a complete vector space equipped with an inner product. It generalizes the notion of Euclidean space to infinite dimensions, allowing for the rigorous handling of limits and convergence.</p>
<p>A Hilbert space <span class="math inline">\(\set{H}\)</span> is defined by two main properties:</p>
<ol type="1">
<li><p><strong>Inner Product</strong>: There is an inner product in <span class="math inline">\(\set{H}\)</span>, denoted as <span class="math inline">\(\langle \x, \y \rangle\)</span>, for vectors <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span>. This inner product satisfies:</p>
<ul>
<li><strong>Conjugate Symmetry</strong>: <span class="math inline">\(\langle \x, \y \rangle = \overline{\langle \y, \x \rangle}\)</span>.</li>
<li><strong>Linearity in the First Argument</strong>: <span class="math inline">\(\langle a\x + b\y, \z \rangle = a\langle \x, \z \rangle + b\langle \y, \z \rangle\)</span>, where <span class="math inline">\(a, b\)</span> are scalars.</li>
<li><strong>Positive Definiteness</strong>: <span class="math inline">\(\langle \x, \x \rangle \geq 0\)</span>, with equality if and only if <span class="math inline">\(\x = \vo{0}\)</span>.</li>
</ul></li>
<li><p><strong>Completeness</strong>: <span class="math inline">\(\set{H}\)</span> is complete, meaning every Cauchy sequence in <span class="math inline">\(\set{H}\)</span> converges to a limit within <span class="math inline">\(\set{H}\)</span>. A sequence <span class="math inline">\(\{\x_n\}\)</span> is Cauchy if for every <span class="math inline">\(\varepsilon &gt; 0\)</span>, there exists an <span class="math inline">\(N\)</span> such that <span class="math inline">\(\|\x_n - \x_m\| &lt; \varepsilon\)</span> for all <span class="math inline">\(m, n &gt; N\)</span>, with the norm defined as <span class="math inline">\(\|\x\| = \sqrt{\langle \x, \x \rangle}\)</span>.</p></li>
</ol>
<p>Few examples of Hilberts spaces include:</p>
<p><strong>Euclidean Space</strong>: The standard Euclidean space <span class="math inline">\(\set{R}^n\)</span> with the usual dot product is a Hilbert space.</p>
<p><strong>Sequence Space (<span class="math inline">\(\set{l}_2\)</span>)</strong>: The space of square-summable <em>sequences</em>, <span class="math inline">\(\set{l}_2\)</span>, is a Hilbert space where a sequence <span class="math inline">\(\{a_n\}\)</span> belongs if <span class="math inline">\(\sum_{n=1}^{\infty} |a_n|^2 &lt; \infty\)</span>. The inner product is <span class="math inline">\(\langle \{a_n\}, \{b_n\} \rangle = \sum_{n=1}^{\infty} a_n \overline{b_n}\)</span>.</p>
<p><strong>Function Space (<span class="math inline">\(\set{L}_2\)</span>)</strong>: Spaces of square-integrable <em>functions</em>, such as <span class="math inline">\(\set{L}_2\)</span> spaces, are Hilbert spaces. A function <span class="math inline">\(f\)</span> is in <span class="math inline">\(\set{L}_2\)</span> if <span class="math inline">\(\int |f(x)|^2 dx &lt; \infty\)</span>. The inner product is <span class="math inline">\(\langle f, g \rangle = \int f(x) \overline{g(x)} dx\)</span>.</p>
<p>Hilbert spaces are fundamental in understanding complex systems and algorithms in machine learning and quantum mechanics, especially due to their properties related to orthogonality, projections, and spectral theory.</p>
<section id="applications-in-machine-learning-1" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applications-in-machine-learning-1">Applications in Machine Learning</h4>
<p>Hilbert spaces play a crucial role in various machine learning algorithms and techniques. They provide a mathematical foundation for handling infinite-dimensional spaces and enable the application of geometric and algebraic concepts to complex datasets. Here are some examples of how Hilbert spaces are used in machine learning:</p>
<p><strong>Kernel Methods in Support Vector Machines (SVMs)</strong>: SVMs are a popular machine learning algorithm used for classification and regression tasks. Kernel methods, a key component of SVMs, allow these algorithms to operate in high-dimensional Hilbert spaces without explicitly computing the coordinates of the data in that space.</p>
<p>The kernel trick maps input data into a higher-dimensional Hilbert space (feature space) where linear separation of the data is easier. For instance, the Radial Basis Function (RBF) kernel implicitly maps data to an infinite-dimensional Hilbert space.</p>
<p><strong>Gaussian Processes</strong>: Gaussian processes are used for probabilistic regression and classification. They are powerful in providing uncertainty estimates along with predictions.</p>
<p>Gaussian processes can be viewed as defining a distribution over functions, where these functions belong to a Hilbert space. This perspective allows for the use of kernel functions to measure similarities in this space, enabling the modeling of complex data relationships.</p>
<p><strong>Quantum Machine Learning</strong>: Quantum machine learning algorithms leverage the principles of quantum mechanics for data processing, often surpassing the capabilities of classical algorithms in certain tasks.</p>
<p>In quantum computing, the state space of quantum systems is a Hilbert space. Understanding the structure of these spaces is crucial for developing and analyzing quantum machine learning algorithms.</p>
<p><strong>Feature Spaces in Neural Networks</strong>: In deep learning, neural networks transform input data through multiple layers, each potentially representing a different feature space.</p>
<p>While not always explicitly stated, the transformations applied by neural networks can be thought of as mapping data into different Hilbert spaces at each layer, particularly in the context of reproducing kernel Hilbert spaces (RKHS).</p>
<p><strong>Principal Component Analysis (PCA)</strong>: PCA is a technique used for dimensionality reduction, feature extraction, and data visualization.</p>
<p>PCA involves projecting data onto the principal components (directions of maximum variance) in a high-dimensional Hilbert space. This projection helps in reducing the dimensionality of the data while preserving as much variance as possible.</p>
<p>In summary, Hilbert spaces provide a theoretical foundation for many machine learning algorithms, particularly those involving kernel methods, probabilistic modeling, and high-dimensional data transformations. Understanding Hilbert spaces enhances the comprehension of the geometric and functional aspects of these algorithms.</p>
</section>
</section>
<section id="banach-spaces" class="level3" data-number="44.1.3">
<h3 data-number="44.1.3" class="anchored" data-anchor-id="banach-spaces"><span class="header-section-number">44.1.3</span> Banach Spaces</h3>
<p>A Banach space is a type of vector space that is both normed and complete. Formally, a Banach space is defined as follows:</p>
<p>A Banach space is a pair <span class="math inline">\((\set{V}, \|\cdot\|)\)</span> consisting of a vector space <span class="math inline">\(\set{V}\)</span> over the field <span class="math inline">\(\set{R}\)</span> or <span class="math inline">\(\set{C}\)</span> and a norm <span class="math inline">\(\|\cdot\|: \set{V} \rightarrow \set{R}\)</span> satisfying the following properties:</p>
<ul>
<li><strong>Positive definiteness</strong>: <span class="math inline">\(\|x\| \geq 0\)</span> for all <span class="math inline">\(x \in \set{V}\)</span> and <span class="math inline">\(\|x\| = 0\)</span> if and only if <span class="math inline">\(x = 0\)</span></li>
<li><strong>Homogeneity</strong>: <span class="math inline">\(\|\lambda x\| = |\lambda|\|x\|\)</span> for all <span class="math inline">\(\lambda\)</span> in the field and <span class="math inline">\(x \in \set{V}\)</span>.</li>
<li><strong>Triangle Inequality</strong>: <span class="math inline">\(\|x + y\| \leq \|x\| + \|y\|\)</span> for all <span class="math inline">\(x, y \in \set{V}\)</span>.</li>
</ul>
<p>Additionally, the space <span class="math inline">\(\set{V}\)</span> is <em>complete</em>, i.e., every Cauchy sequence in <span class="math inline">\(X\)</span> converges to a limit that is also in <span class="math inline">\(\set{V}\)</span>.</p>
<p>In simpler terms, a Banach space is a vector space where the concept of length or size is well-defined (via the norm) and in which every sequence that intuitively should have a limit actually has a limit within the space.</p>
<p>Some examples of Banach space include:</p>
<ul>
<li><p>The set of all continuous real-valued functions on a closed interval <span class="math inline">\([a, b]\)</span>, denoted <span class="math inline">\(C[a, b]\)</span>, with the norm defined as <span class="math inline">\(\|f\| = \sup\{|f(x)| : x \in [a, b]\}\)</span>, is a Banach space.</p></li>
<li><p>The space <span class="math inline">\(\set{L}_p\)</span> for <span class="math inline">\(1 \leq p &lt; \infty\)</span>, which consists of all sequences <span class="math inline">\(\{a_n\}\)</span> for which the series <span class="math inline">\(\sum_{n=1}^{\infty} |a_n|^p\)</span> converges, is a Banach space with the norm <span class="math inline">\(\|a\|_p = \left(\sum_{n=1}^{\infty} |a_n|^p\right)^{1/p}\)</span>.</p></li>
</ul>
<p>Banach spaces are a central object of study in functional analysis and have applications in various areas of mathematics, including analysis, differential equations, and the theory of function spaces.</p>
<section id="comparison-between-hilbert-and-banach-space" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="comparison-between-hilbert-and-banach-space">Comparison between Hilbert and Banach Space</h4>
<p>Hilbert spaces and Banach spaces are both fundamental concepts in functional analysis, but they have distinct characteristics:</p>
<ol type="1">
<li><strong>Inner Product vs.&nbsp;Norm</strong>:
<ul>
<li><strong>Hilbert Space</strong>: A Hilbert space is a vector space equipped with an inner product. This inner product allows for the definition of angles and lengths, similar to Euclidean spaces. The norm in a Hilbert space is derived from the inner product, <span class="math inline">\(\|x\| = \sqrt{\langle x, x \rangle}\)</span>.</li>
<li><strong>Banach Space</strong>: A Banach space is a vector space equipped with a norm, which may or may not be derived from an inner product. The norm in a Banach space measures the size or length of vectors but does not inherently define angles between them.</li>
</ul></li>
<li><strong>Completeness</strong>:
<ul>
<li>Both Hilbert and Banach spaces are complete, meaning that every Cauchy sequence in the space converges to a point within the space. This is a common feature of both spaces.</li>
</ul></li>
<li><strong>Geometric Intuition</strong>:
<ul>
<li><strong>Hilbert Space</strong>: Due to the presence of an inner product, Hilbert spaces have a stronger geometric structure, allowing for concepts like orthogonality and projection, which are critical in many applications.</li>
<li><strong>Banach Space</strong>: Banach spaces, lacking a general inner product, do not inherently have these geometric properties, focusing more on the analysis through the norm.</li>
</ul></li>
<li><strong>Examples and Applications</strong>:
<ul>
<li><strong>Hilbert Space</strong>: Common examples include the space of square-summable sequences (<span class="math inline">\(\set{l}_2\)</span>) and the space of square-integrable functions (<span class="math inline">\(\set{L}_2\)</span>). Applications are found in quantum mechanics, signal processing, and machine learning (e.g., in kernel methods).</li>
<li><strong>Banach Space</strong>: Examples include <span class="math inline">\(\set{L}p\)</span> spaces (for <span class="math inline">\(1 \leq p \leq \infty\)</span>) and the space of continuous functions. Applications span a broad range of mathematical areas, including functional analysis, differential equations, and optimization.</li>
</ul></li>
</ol>
<p>In summary, while both Hilbert and Banach spaces are complete normed vector spaces, the key difference lies in the presence of an inner product in Hilbert spaces, giving them additional geometric properties. Banach spaces are more general and do not necessarily have these geometric features. All Hilbert spaces are Banach spaces, but not all Banach spaces are Hilbert spaces.</p>
</section>
<section id="applications-in-machine-learning-2" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applications-in-machine-learning-2">Applications in Machine Learning</h4>
<p>In machine learning, Banach spaces often appear in the context of optimization and function approximation. While Hilbert spaces are more commonly cited due to their geometric properties, Banach spaces are equally important, particularly in scenarios where specific norms are utilized to measure the error or regularize the models. Here are a few examples where Banach spaces are relevant in machine learning:</p>
<p><strong><span class="math inline">\(\set{L}_p, \set{l}_p\)</span> Spaces in Regularization</strong>: Regularization techniques in machine learning, such as Lasso (1-norm regularization) and Ridge Regression (2-norm regularization), involve minimizing a cost function that includes a term based on the <span class="math inline">\(\|\cdot\|_1\)</span> or <span class="math inline">\(\|\cdot\|_2\)</span> norm of the model parameters.</p>
<p><strong>Function Spaces in Kernel Methods</strong>: While kernel methods such as Support Vector Machines (SVMs) and Gaussian Processes are often associated with Hilbert spaces, some kernel functions, particularly those not associated with an inner product, lead to Banach space formulations.</p>
<p><strong>Spaces of Bounded Functions</strong>: In some machine learning models, particularly in deep learning, the functions approximated by the networks may naturally reside in spaces of bounded, continuous functions, which are examples of Banach spaces.</p>
<p><strong>Optimization Algorithms</strong>: Several optimization algorithms used in training machine learning models can be analyzed within the framework of Banach spaces, especially when dealing with non-differentiable functions or when using specific norms for convergence analysis. Subgradient methods in optimization, commonly used for non-differentiable functions, can be formulated in terms of Banach spaces, particularly when considering convergence properties and robustness.</p>
<p>These examples illustrate how Banach spaces, though less explicitly mentioned than Hilbert spaces, play a significant role in the theoretical foundation and practical implementation of various machine learning algorithms and techniques.</p>
</section>
</section>
<section id="sobolev-spaces" class="level3" data-number="44.1.4">
<h3 data-number="44.1.4" class="anchored" data-anchor-id="sobolev-spaces"><span class="header-section-number">44.1.4</span> Sobolev Spaces</h3>
<p>Sobolev spaces are essential in functional analysis, with significant applications in the study of partial differential equations and approximation theory. They extend the concept of differentiability and integrability to a broader class of functions.</p>
<p>A Sobolev space, denoted as <span class="math inline">\(\set{W}_{k,p}(\Omega)\)</span>, is defined for functions with weak derivatives up to order <span class="math inline">\(k\)</span> that are <span class="math inline">\(\set{L}_p\)</span>-integrable.</p>
<ul>
<li><span class="math inline">\(\Omega\)</span> is an open subset of <span class="math inline">\(\set{R}^n\)</span>.</li>
<li><span class="math inline">\(k\)</span> is a non-negative integer, the order of derivatives.</li>
<li><span class="math inline">\(p\)</span> is a real number, <span class="math inline">\(1 \leq p \leq \infty\)</span>, representing integrability.</li>
</ul>
<p>For <span class="math inline">\(f \in \set{W}_{k,p}(\Omega)\)</span>, the following conditions must be satisfied:</p>
<ul>
<li><p>For all multi-indices <span class="math inline">\(\alpha\)</span> with <span class="math inline">\(|\alpha| \leq k\)</span>, the weak derivatives <span class="math inline">\(D^\alpha f\)</span> exist and <span class="math inline">\(D^\alpha f \in \set{L}_p(\Omega)\)</span>.</p></li>
<li><p>The norm in <span class="math inline">\(\set{W}_{k,p}(\Omega)\)</span> is defined as:</p>
<p><span class="math display">\[ \|f\|_{\set{W}_{k,p}(\Omega)} = \left( \sum_{|\alpha| \leq k} \|D^\alpha f\|_{\set{L}_p(\Omega)}^p \right)^{1/p}. \]</span></p>
<p>For <span class="math inline">\(p = \infty\)</span>, the norm is:</p>
<p><span class="math display">\[ \|f\|_{W_{k,\infty}(\Omega)} = \max_{|\alpha| \leq k} \|D^\alpha f\|_{\set{L}_\infty(\Omega)}. \]</span></p></li>
</ul>
<p>Here are some simple examples to illustrating Sobolev spaces are:</p>
<p><strong>Example 1: First Order Sobolev Space <span class="math inline">\(\set{W}_{1,2}(\Omega)\)</span> on a Real Interval</strong>: Consider the Sobolev space <span class="math inline">\(\set{W}_{1,2}([a, b])\)</span>, where <span class="math inline">\([a, b]\)</span> is a closed interval on the real line. This space consists of all real-valued functions on <span class="math inline">\([a, b]\)</span> that have square-integrable first derivatives.</p>
<p>A function <span class="math inline">\(f\)</span> belongs to <span class="math inline">\(\set{W}_{1,2}([a, b])\)</span> if both <span class="math inline">\(f\)</span> and its weak derivative <span class="math inline">\(f'\)</span> are in <span class="math inline">\(\set{L}_2([a, b])\)</span>. This means <span class="math display">\[\int_a^b |f(x)|^2 dx &lt; \infty \quad \text{and} \quad \int_a^b |f'(x)|^2 dx &lt; \infty. \]</span></p>
<p>An example of a function in this space could be <span class="math inline">\(f(x) = x^3\)</span> on the interval <span class="math inline">\([0, 1]\)</span>. Both the function and its derivative <span class="math inline">\(f'(x) = 3x^2\)</span> are square-integrable over this interval.</p>
<p><strong>Example 2: Sobolev Space <span class="math inline">\(\set{W}_{2,p}(\set{R})\)</span> for <span class="math inline">\(p &gt; 1\)</span></strong>: The Sobolev space <span class="math inline">\(\set{W}_{2,p}(\set{R})\)</span> consists of functions whose first and second weak derivatives are in the Lebesgue space <span class="math inline">\(\set{L}_p(\set{R})\)</span>.</p>
<p>Mathematically a function <span class="math inline">\(g\)</span> is in <span class="math inline">\(W_{2,p}(\set{R})\)</span> if <span class="math inline">\(g, g',\)</span> and <span class="math inline">\(g''\)</span> (first and second derivatives) are all <span class="math inline">\(\set{L}_p\)</span>-integrable. For <span class="math inline">\(p = 2\)</span>, this space is also a Hilbert space.</p>
<p>An example could be <span class="math inline">\(g(x) = e^{-x^2}\)</span>. This function, along with its first and second derivatives, are <span class="math inline">\(p\)</span>-integrable for any <span class="math inline">\(p &gt; 1\)</span>.</p>
<p><strong>Example 3: Zeroth Order Sobolev Space <span class="math inline">\(\set{W}_{0,p}(\Omega)\)</span></strong>: The Sobolev space <span class="math inline">\(\set{W}_{0,p}(\Omega)\)</span> is essentially the Lebesgue space <span class="math inline">\(\set{L}_p(\Omega)\)</span>. It includes functions that are <span class="math inline">\(p\)</span>-integrable over the domain <span class="math inline">\(\Omega\)</span>.</p>
<p>For <span class="math inline">\(\set{W}_{0,p}(\Omega)\)</span>, no derivatives are involved. A function <span class="math inline">\(h\)</span> is in this space if <span class="math display">\[ \int_\Omega |h(x)|^p dx &lt; \infty.\]</span></p>
<p>A simple function like <span class="math inline">\(h(x) = \sin(x)\)</span> on a bounded domain like <span class="math inline">\([0, \pi]\)</span> is in <span class="math inline">\(\set{W}_{0,p}([0, \pi])\)</span> for any <span class="math inline">\(p \geq 1\)</span>.</p>
<p>These examples illustrate the concept of Sobolev spaces with varying orders and integrability conditions, showcasing their flexibility in accommodating different degrees of smoothness and integrability requirements for functions.</p>
<section id="applications-in-machine-learning-3" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applications-in-machine-learning-3">Applications in Machine Learning</h4>
<p>Sobolev spaces are used in machine learning primarily in the context of function approximation, regularization, and understanding the behavior of learning algorithms, especially for problems involving differential equations and smoothness constraints. Here’s a breakdown of their applications:</p>
<p><strong>Deep Learning and Neural Networks</strong>: In deep learning, neural networks can be thought of as function approximators. Sobolev spaces provide a theoretical framework for analyzing the smoothness and differentiability of the functions that these networks can approximate. Specifically, they help in understanding how well neural networks can approximate functions with certain smoothness (derivatives) properties.</p>
<p><strong>Kernel Methods</strong>: In machine learning algorithms like Support Vector Machines (SVMs) and Gaussian Processes, kernel functions implicitly map input data into high-dimensional feature spaces. These feature spaces can often be associated with Sobolev spaces, especially when considering the smoothness and regularity of the functions represented by these kernels.</p>
<p><strong>Sobolev Norms for Regularization</strong>: In machine learning, regularization is used to prevent overfitting by penalizing complex models. Sobolev norms, which measure the smoothness of a function, can be used as a regularization term in the loss function of a learning algorithm. This is particularly relevant in regression problems where the goal includes maintaining a certain degree of smoothness in the learned function.</p>
<p><strong>Learning Solutions to PDEs</strong>: Machine learning, particularly deep learning, is increasingly used to find approximate solutions to complex PDEs. Sobolev spaces are inherently tied to the theory of PDEs, and understanding these spaces is crucial when using machine learning methods to approximate solutions of PDEs, ensuring that the learned solutions possess the desired smoothness and differentiability properties.</p>
<p><strong>Generalization and Complexity Analysis</strong>: In theoretical machine learning, Sobolev spaces can be used to analyze the complexity and generalization capabilities of learning algorithms. They offer a way to quantify the capacity of a model class (e.g., a class of neural networks) to approximate functions with certain smoothness properties.</p>
<p><strong>Image Processing and Computer Vision</strong>: In tasks like image segmentation and edge detection, maintaining the smoothness of the output (e.g., segmented regions) can be crucial. Machine learning models trained with Sobolev space-based regularization can yield smoother and more coherent results.</p>
<p><strong>Scientific Computing</strong>: In scientific fields that use machine learning to model physical phenomena (e.g., climate modeling, fluid dynamics), ensuring that the learned models adhere to physical laws often expressible by PDEs can be aided by the theoretical underpinnings of Sobolev spaces.</p>
<p>In summary, Sobolev spaces in machine learning are mainly behind the scenes, offering a theoretical foundation for understanding the smoothness, differentiability, and general behavior of the functions that machine learning models learn and represent. They are particularly important in areas where the smoothness of the learned function is crucial, both for performance and interpretability.</p>
</section>
</section>
<section id="measure-spaces" class="level3" data-number="44.1.5">
<h3 data-number="44.1.5" class="anchored" data-anchor-id="measure-spaces"><span class="header-section-number">44.1.5</span> Measure Spaces</h3>
<p>A measure space is a fundamental concept in measure theory, a branch of mathematical analysis that deals with generalizations of notions of size and length. It provides a formal framework for defining and working with measures, which can be thought of as a way to assign a size, volume, or probability to subsets of a given set.</p>
<p>A measure space is defined as a triple <span class="math inline">\((\set{X}, \set{M}, \mu)\)</span> where:</p>
<ol type="1">
<li><p><span class="math inline">\(\set{X}\)</span> is a set, often referred to as the ‘universe’ or ‘space’.</p></li>
<li><p><span class="math inline">\(\set{M}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra over <span class="math inline">\(\set{X}\)</span>. A <span class="math inline">\(\sigma\)</span>-algebra is a collection of subsets of <span class="math inline">\(\set{X}\)</span> that includes the empty set, is closed under complementation, and is closed under countable unions. In other words, if <span class="math inline">\(A\)</span> is in <span class="math inline">\(\set{M}\)</span>, then so is its complement <span class="math inline">\(\set{X} \setminus A\)</span>, and if <span class="math inline">\(A_1, A_2, A_3, \ldots\)</span> are in <span class="math inline">\(\set{M}\)</span>, then so is <span class="math inline">\(\bigcup_{i=1}^{\infty} A_i\)</span>.</p></li>
<li><p><span class="math inline">\(\mu\)</span> is a measure on <span class="math inline">\(\set{M}\)</span>. A measure is a function <span class="math inline">\(\mu: \set{M} \rightarrow [0, \infty]\)</span> that assigns a non-negative extended real number to each set in <span class="math inline">\(\set{M}\)</span> in a way that satisfies the following properties:</p></li>
</ol>
<ul>
<li><strong>Non-negativity</strong>: For every <span class="math inline">\(A \in \set{M}\)</span>, <span class="math inline">\(\mu(A) \geq 0\)</span>.</li>
<li><strong>Null empty set</strong>: <span class="math inline">\(\mu(\emptyset) = 0\)</span>.</li>
<li><strong>Countable additivity (or <span class="math inline">\(\sigma\)</span>-additivity)</strong>: For any countable collection <span class="math inline">\(\{A_i\}_{i=1}^{\infty}\)</span> of pairwise disjoint sets in <span class="math inline">\(\set{M}\)</span>, <span class="math inline">\(\mu\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} \mu(A_i)\)</span>.</li>
</ul>
<p>Some examples of measure space include:</p>
<ul>
<li><p><strong>Lebesgue Measure</strong>: On the real line <span class="math inline">\(\set{R}\)</span>, the Lebesgue measure is a classic example. It extends the intuitive concept of length to a wider class of sets. In this case, <span class="math inline">\(\set{X} = \set{R}\)</span>, <span class="math inline">\(\set{M}\)</span> is the <span class="math inline">\(\sigma\)</span>-algebra of Lebesgue measurable sets, and <span class="math inline">\(\mu\)</span> is the Lebesgue measure.</p></li>
<li><p><strong>Probability Space</strong>: In probability theory, a measure space with the total measure of 1 (i.e., <span class="math inline">\(\mu(\set{X}) = 1\)</span>) is called a probability space. Here, the measure of a set represents the probability of that event.</p></li>
</ul>
<p>Measure spaces are essential in various fields of mathematics, including probability theory, statistical theory, and real analysis. They allow for the rigorous treatment of integrals, probabilities, and sizes in more abstract settings than traditional notions of length or volume.</p>
</section>
<section id="lebesgue-spaces" class="level3" data-number="44.1.6">
<h3 data-number="44.1.6" class="anchored" data-anchor-id="lebesgue-spaces"><span class="header-section-number">44.1.6</span> Lebesgue Spaces</h3>
<p>Lebesgue spaces, often denoted as <span class="math inline">\(\set{L}_p\)</span> spaces, are a fundamental concept in functional analysis and measure theory. They generalize the notion of spaces of integrable functions beyond the traditional framework provided by Riemann integration, using the more powerful tool of Lebesgue integration.</p>
<p>For a given measure space <span class="math inline">\((\set{X}, \set{M}, \mu)\)</span> and a real number <span class="math inline">\(p \geq 1\)</span>, the <span class="math inline">\(\set{L}_p\)</span> space, denoted as <span class="math inline">\(\set{L}_p(\set{X}, \set{M}, \mu)\)</span> or simply <span class="math inline">\(\set{L}_p(\set{X})\)</span>, is defined as follows:</p>
<ul>
<li>It consists of all measurable functions <span class="math inline">\(f: \set{X} \rightarrow \set{R}\)</span> (or <span class="math inline">\(\set{C}\)</span>) for which the <span class="math inline">\(p^\text{th}\)</span> power of the absolute value is Lebesgue integrable, i.e.,</li>
</ul>
<p><span class="math display">\[ \int_X |f(x)|^p \, d\mu &lt; \infty \]</span></p>
<p><span class="math inline">\(\set{L}_p\)</span> spaces have the following properties</p>
<ol type="1">
<li><strong>Norm</strong>: Each <span class="math inline">\(\set{L}_p\)</span> space is equipped with a norm, known as the <span class="math inline">\(\set{L}_p\)</span>-norm, defined as:</li>
</ol>
<p><span class="math display">\[ \|f\|_{\set{L}_p} = \left( \int_X |f(x)|^p \, d\mu \right)^{1/p} \]</span></p>
<ol start="2" type="1">
<li><p><strong>Completeness</strong>: Every <span class="math inline">\(\set{L}_p\)</span> space is complete, meaning that every Cauchy sequence in <span class="math inline">\(\set{L}_p\)</span> converges to a limit within <span class="math inline">\(\set{L}_p\)</span>.</p></li>
<li><p><strong>Special Cases</strong>:</p>
<ul>
<li><strong><span class="math inline">\(\set{L}_2\)</span> Space</strong>: This is a Hilbert space with the inner product defined as <span class="math inline">\(\langle f, g \rangle = \int_X f(x) \overline{g(x)} \, d\mu\)</span>. It is widely used in quantum mechanics and signal processing.</li>
<li><strong><span class="math inline">\(\set{L}_1\)</span> Space</strong>: Consists of absolutely integrable functions. It’s important in probability theory and various branches of analysis.</li>
<li><strong><span class="math inline">\(\set{L}_\infty\)</span> Space</strong>: Defined as the space of essentially bounded functions, with the norm given by the essential supremum of the function.</li>
</ul></li>
</ol>
<p>In general, <span class="math inline">\(\set{L}_p\)</span> Lebesgue spaces are defined by integrability conditions. They are always Banach spaces and become Hilbert spaces specifically when <span class="math inline">\(p=2\)</span>.</p>
<section id="applications-in-machine-learning-4" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="applications-in-machine-learning-4">Applications in Machine Learning</h4>
<p>Lebesgue spaces, especially <span class="math inline">\(\set{L}_p\)</span> spaces, find various applications in machine learning, primarily in the formulation of loss functions, regularization techniques, and in the theoretical underpinnings of learning algorithms. Here are some key applications:</p>
<p><strong>Cross-Entropy Loss in Classification</strong>: In classification problems, particularly with neural networks, the cross-entropy loss function is often used. While not directly an <span class="math inline">\(\set{L}_p\)</span> norm, it is related to the concept of integrability and measurement in Lebesgue spaces.</p>
<p><strong>Feature Space Mapping in Kernel Methods</strong>: In Support Vector Machines (SVM) and other kernel-based methods, data is implicitly mapped into a high-dimensional feature space. Understanding these feature spaces often involves concepts from Lebesgue spaces, particularly when analyzing the smoothness and integrability of functions in that space.</p>
<p><strong>Probabilistic Models and Density Estimation</strong>: In probabilistic models and density estimation techniques, the properties of functions in <span class="math inline">\(\set{L}_p\)</span> spaces help in understanding the behavior and properties of probability density functions, especially in terms of integrability and convergence.</p>
<p><strong>Deep Learning Theory</strong>: In deep learning, particularly in the analysis of neural network functions and their approximation capabilities, Lebesgue spaces provide a framework for understanding function spaces that neural networks can represent.</p>
<p><strong>Fourier Analysis and Signal Reconstruction</strong>: In signal processing, many problems involve working with signals in the <span class="math inline">\(\set{L}_2\)</span> space, particularly when using Fourier analysis for signal reconstruction and processing.</p>
<p>In summary, Lebesgue spaces in machine learning are crucial for formulating various models and algorithms, especially in understanding and controlling the behavior of learning algorithms through loss functions and regularization. They provide a mathematical foundation for many of the tools and techniques commonly used in the field.</p>
</section>
</section>
</section>
</section>
<section id="preface" class="level1 unnumbered">
<h1 class="unnumbered">Preface</h1>
<p>This book aims to introduce machine learning to senior-level undergraduates and graduate students in aerospace engineering.</p>
<section id="what-is-machine-learning" class="level3" data-number="44.1.7">
<h3 data-number="44.1.7" class="anchored" data-anchor-id="what-is-machine-learning"><span class="header-section-number">44.1.7</span> What is Machine Learning?</h3>
<p>Machine learning, a transformative branch of artificial intelligence, has revolutionized how we approach problem-solving across various domains. Machine learning involves training computers to learn from data, identify patterns, and make decisions with minimal human intervention. This rapidly evolving field leverages statistical methods to enable machines to improve tasks through feedback and data, offering immense potential for innovation and efficiency.</p>
</section>
<section id="machine-learning-in-aerospace-engineering" class="level3" data-number="44.1.8">
<h3 data-number="44.1.8" class="anchored" data-anchor-id="machine-learning-in-aerospace-engineering"><span class="header-section-number">44.1.8</span> Machine Learning in Aerospace Engineering</h3>
<p>Machine learning (ML) is revolutionizing aerospace engineering with its current applications and promising future potential. There are several emerging areas within aerospace engineering where ML is expected to have significant impact. Some of the emerging trends are summarized below.</p>
<p><strong>Space Exploration</strong> – ML in driving autonomy is particularly critical in space exploration. Space missions, often extending over vast distances and durations, necessitate systems that can operate with minimal human oversight. ML algorithms are instrumental in this context, enabling spacecraft to navigate, manage systems, and make crucial decisions autonomously. This autonomy is vital for deep space missions, where communication delays with Earth make real-time human control impractical. ML allows spacecraft to adapt to unforeseen circumstances, such as changes in trajectory, asteroid avoidance, or equipment malfunctions, ensuring mission success despite space environments’ challenging and unpredictable nature. Additionally, ML-driven autonomous systems are essential for analyzing and processing the vast amounts of scientific data collected during these missions, identifying critical findings that human researchers might miss. As ML technology advances, its role in enhancing the autonomy of space missions becomes increasingly significant, promising groundbreaking discoveries and more ambitious explorations beyond our current capabilities.</p>
<p><strong>Air Traffic Management</strong> – ML is increasingly pivotal in advancing aircraft flight management systems, and it plays a crucial role in the emerging sector of urban air mobility (UAM). In flight management, ML algorithms enhance route optimization, fuel efficiency, and in-flight safety by analyzing vast amounts of flight data and environmental variables in real-time. These systems can adaptively manage complex flight dynamics, navigate busy airspaces, and respond proactively to unforeseen circumstances like adverse weather or mechanical issues. The application of ML is particularly significant in UAM, a field focusing on developing small, automated aircraft for urban transportation. Here, ML is essential for ensuring safe, efficient, and autonomous operation in densely populated urban environments. It enables these aircraft to make intelligent navigation decisions, manage air traffic autonomously, and ensure passenger safety, all while adhering to stringent regulatory standards. As UAM evolves, ML will be at the forefront of this innovation, driving the development of smart, efficient, and safe urban air transport solutions.</p>
<p><strong>Aerospace Digital Twins</strong> – ML is revolutionizing the concept of digital twins in aerospace engineering, offering a sophisticated approach to simulate, monitor, and analyze real-world aircraft and spacecraft systems. Digital twins, essentially virtual replicas of physical systems, utilize ML to process real-time data from various sensors and systems on the actual aircraft or spacecraft. This integration enables predictive analytics and real-time diagnostics, allowing engineers to anticipate potential failures, optimize maintenance schedules, and improve overall operational efficiency. ML algorithms also play a crucial role in simulating complex scenarios, from aerodynamic performance to structural integrity under various conditions, providing invaluable insights for design improvements and innovation. This synergy of ML and digital twin technology in aerospace not only enhances current operational performance but also significantly aids in the development of more advanced, reliable, and efficient future aerospace systems.</p>
<p><strong>Computational Fluid Dynamics</strong> – ML is increasingly being integrated into computational fluid dynamics (CFD) for aerospace applications, offering transformative capabilities in simulating and analyzing aerodynamic behaviors. In CFD, ML algorithms are used to refine and accelerate simulations, making them more efficient and accurate. These algorithms can learn from vast datasets of previous simulations and real-world experiments, enabling them to predict fluid flow patterns, pressure distributions, and thermal characteristics more quickly than traditional methods. This is particularly beneficial in complex scenarios like turbulent flows or supersonic speeds, where traditional CFD can be computationally intensive. ML also assists in optimizing designs for better aerodynamic performance, reducing drag, and enhancing fuel efficiency. By improving the predictive capabilities of CFD models, ML helps aerospace engineers in designing more efficient aircraft, spacecraft, and propulsion systems, ultimately leading to advancements in performance and energy efficiency.</p>
<p><strong>Aircraft Design</strong> – ML is significantly impacting aircraft design, offering new avenues for innovation and efficiency. In aircraft design, ML algorithms analyze vast datasets to identify optimal design parameters that traditional methods might overlook. This includes refining aerodynamic shapes, optimizing weight distribution, and enhancing material selection for better performance and fuel efficiency. ML also plays a pivotal role in structural health monitoring, where it helps predict stress points and potential fatigue in aircraft components, leading to safer and more reliable designs. Furthermore, ML algorithms assist in noise reduction, both internally and externally, by analyzing and predicting acoustic patterns, thereby contributing to more environmentally friendly and passenger-comfort-focused designs. The integration of ML in aircraft design not only accelerates the design process but also enables the development of more advanced, efficient, and sustainable aircraft, pushing the boundaries of what is currently achievable in aerospace engineering.</p>
<p><strong>Aerospace Material Science</strong> – The integration of ML in material science is revolutionizing aerospace engineering, significantly enhancing the development and optimization of aerospace materials. ML algorithms are adept at sifting through and analyzing large datasets, uncovering patterns and relationships in material properties that might be missed by conventional methods. This capability is crucial for discovering new materials with desired properties such as lightweight, high strength, and thermal resistance, essential for aerospace applications. ML also accelerates the process of material testing and validation, predicting how new materials will perform under various stressors and environmental conditions. This predictive power enables more efficient design and testing cycles, reducing the time and cost associated with material development. In essence, ML is not just optimizing existing materials for aerospace use but also paving the way for the discovery and creation of novel materials, potentially leading to lighter, stronger, and more efficient aerospace components and structures.</p>
<p>In conclusion, ML is not just an auxiliary tool but a transformative force in aerospace engineering, enhancing current practices and promising to redefine the future of air and space travel. As ML technology continues to evolve, its role in aerospace engineering is set to become increasingly pivotal, pushing the boundaries of what’s possible in this field.</p>
</section>
<section id="what-machine-learning-is-not" class="level3" data-number="44.1.9">
<h3 data-number="44.1.9" class="anchored" data-anchor-id="what-machine-learning-is-not"><span class="header-section-number">44.1.9</span> What Machine Learning Is Not</h3>
<p>It is essential to clarify a common misconception: machines don’t actually learn in the way we traditionally understand learning. What we call a “learning machine” is a system that discovers a mathematical formula. When applied to a set of inputs known as “training data,” this formula yields the expected outputs. Furthermore, it can produce accurate outputs for various inputs, provided these inputs share a statistical distribution similar to the training data.</p>
<p>However, this process differs significantly from learning in the animal world. For instance, if you learn to play a video game by looking directly at the screen, a slight rotation of the screen won’t drastically affect your ability to play. In contrast, a machine learning algorithm trained under specific conditions may fail when those conditions change slightly, such as in the case of screen rotation, unless it’s specifically trained to recognize such changes.</p>
<p>While working at IBM, the term “machine learning” was coined in 1959 by Arthur Samuel, an American pioneer in computer gaming and artificial intelligence. Much like IBM’s later promotion of “cognitive computing,” this naming was part of a marketing strategy to attract clients and talent. Thus, while “artificial intelligence” doesn’t imply human-like intelligence, “machine learning” does not equate to the learning process seen in animals. Instead, it refers to creating machines that can perform various tasks without being explicitly programmed for each task. The word “learning” here is more analogous to animal learning rather than a literal interpretation.</p>
<p>In contemporary media, machine learning is often shrouded in misconceptions, frequently portrayed as a near-magical or ominously omnipotent technology. Contrary to these dramatizations, machine learning is fundamentally grounded in sophisticated yet comprehensible mathematics. It involves algorithms that analyze and “learn” from data to optimize specific tasks. While it’s highly effective at tasks like pattern recognition and predictive analysis, machine learning isn’t a cure-all. Its “intelligence” is derived from the performance of algorithms tailored and improved over time, informed by data and the laws of physics. It cannot understand moral or ethical issues or grasp abstract concepts beyond its programming. Recognizing these limitations is critical to appreciating the real capabilities and scope of machine learning, which remains a potent yet human-dependent tool.</p>
</section>
<section id="words-of-caution" class="level3" data-number="44.1.10">
<h3 data-number="44.1.10" class="anchored" data-anchor-id="words-of-caution"><span class="header-section-number">44.1.10</span> Words of Caution</h3>
<p>In the realm of engineering, particularly in disciplines as exacting as aerospace engineering, the concept of correctness carries significant weight. Engineering solutions are expected to adhere to stringent standards of accuracy and reliability, a criterion that current machine-learning applications may not always meet. The outputs of machine learning algorithms are often more suited to contexts where accuracy is subjective and less critical. This divergence becomes especially relevant when integrating machine learning into engineering tasks, where the margin for error is minimal. In aerospace engineering, where the stakes are exceptionally high, carefully assessing errors and uncertainties in machine learning outputs becomes imperative. Ensuring the safety, efficiency, and reliability of aerospace systems demands a meticulous evaluation of machine learning applications, underlining the need for precise error analysis and validation against the uncompromising standards of engineering accuracy.</p>
</section>
<section id="an-optimization-mind-set" class="level3" data-number="44.1.11">
<h3 data-number="44.1.11" class="anchored" data-anchor-id="an-optimization-mind-set"><span class="header-section-number">44.1.11</span> An Optimization Mind Set</h3>
<p>This book presents a practical and insightful approach, positioning machine learning as a key optimization tool across various domains, especially critical in aerospace engineering. It emphasizes machine learning’s role in discovering innovative solutions to complex challenges within this field. By leveraging the power of machine learning algorithms to process and analyze intricate, multi-faceted data, we can unearth previously hidden patterns and connections. This strategy isn’t about replacing human creativity; rather, it’s about enhancing it with machine-based computational efficiency. By employing these algorithms, we can more effectively explore vast solution spaces within an optimization framework, thereby accelerating the pace of innovation. This viewpoint underscores the significance of machine learning as an indispensable resource in driving forward advanced designs, elevating safety and operational efficiency, and pioneering new frontiers in aerospace engineering. The book encourages the development of an optimization mindset, crucial for tackling the complex problems of aerospace engineering with machine learning, blending computational power with human expertise to push the boundaries of what’s possible.</p>
</section>
<section id="theory-and-data-driven-aerospace-machine-learning" class="level3" data-number="44.1.12">
<h3 data-number="44.1.12" class="anchored" data-anchor-id="theory-and-data-driven-aerospace-machine-learning"><span class="header-section-number">44.1.12</span> Theory and Data Driven Aerospace Machine Learning</h3>
<p>An ideal strategy for aerospace machine learning should effectively integrate physics principles with machine learning concepts, utilizing mathematical equations to compensate for any data deficiencies. Rather than viewing physical laws as mere restrictions, they should be considered as crucial insights that steer the machine learning process. The efficiency of problem solving can be greatly improved by adeptly combining empirical data with these core physical principles. The result of merging data with physics is the generation of solutions that are as firmly based on empirical evidence as they are on the foundational principles of physics. Adopting such an all-encompassing approach ensures that machine learning algorithms are not only responsive to data but also precisely adjusted in accordance with the fundamental laws that drive aerospace phenomena. This synergy of data and physics carves out a route towards the development of innovative, dependable, and accurately fine-tuned answers to the intricate problems faced in aerospace engineering.</p>
</section>
<section id="scope-of-the-book" class="level3" data-number="44.1.13">
<h3 data-number="44.1.13" class="anchored" data-anchor-id="scope-of-the-book"><span class="header-section-number">44.1.13</span> Scope of the book</h3>
<p>This book aims to complement the newly introduced machine learning course in the Aerospace Engineering Department at Texas A&amp;M University. Recognizing the extensive range of machine learning’s applicability in aerospace, the book offers a detailed examination of select, complex issues within this domain, tailored for machine learning solutions. It is crafted with undergraduate seniors and graduate students in mind, aiming to deepen their comprehension of the interplay and enhancement of aerospace technologies by machine learning. The book is also structured to be approachable for professionals, given they have a foundational understanding of mathematical concepts and computer programming. This book strives to connect academic theory with real-world industry practices by integrating theoretical knowledge with practical case studies. It is envisioned as a critical resource for those keen on exploring the cutting-edge applications of machine learning in aerospace, whether for academic advancement or practical application. As the course evolves, we anticipate expanding the topics covered in this book and hope to achieve these lofty goals.</p>
<p><strong>Raktim Bhattacharya</strong><br> <em>Professor</em><br> Aerospace Engineering, Texas A&amp;M University.<br> College Station, TX, 77843-3141, USA.</p>
</section>
</section>
<section id="introduction-to-machine-learning" class="level1" data-number="45">
<h1 data-number="45"><span class="header-section-number">45</span> Introduction to Machine Learning</h1>
<p>At its essence, machine learning involves the development of algorithms that enable computers to learn from and make decisions based on data. Unlike traditional programming, where tasks are explicitly programmed, ML algorithms adaptively improve their performance as they are exposed to more data over time. This process of learning from data allows machines to uncover hidden insights without being explicitly programmed to find specific answers.</p>
<p>Before the advent and popularization of modern machine learning algorithms, learning from data was primarily conducted through various statistical methods and manual analysis. The key approaches included:</p>
<ol type="1">
<li><p><strong>Classical Statistical Methods</strong>: Traditional statistics was the cornerstone of data analysis, involving techniques like linear regression, logistic regression, ANOVA (Analysis of Variance), and hypothesis testing. These methods were used to infer relationships between variables and to make predictions.</p></li>
<li><p><strong>Rule-Based Systems</strong>: In early forms of artificial intelligence, experts would manually create rules for systems to follow. These systems, known as expert systems, used logic and predefined rules to make decisions or predictions based on input data.</p></li>
<li><p><strong>Signal Processing Techniques</strong>: For analyzing time-series data or data from sensors, signal processing techniques were widely used. These included methods like Fourier transforms and filter theory, which were essential for extracting useful information from raw data.</p></li>
<li><p><strong>Linear Algebra and Optimization</strong>: Techniques from linear algebra and mathematical optimization were used for data analysis and problem-solving, especially in operations research and decision-making scenarios.</p></li>
<li><p><strong>Graphical Models</strong>: Models like Bayesian networks and Markov models, which represent probabilistic relationships among variables, were used for making predictions and understanding data structures.</p></li>
<li><p><strong>Data Mining</strong>: Early data mining techniques involved finding patterns and relationships in large datasets using methods like clustering, decision trees, and association rule learning.</p></li>
<li><p><strong>Exploratory Data Analysis (EDA)</strong>: This approach emphasized analyzing datasets to summarize their main characteristics, often using visual methods. It was more about ‘discovering’ patterns and insights rather than ‘predicting’ or ‘classifying’, which are common goals of modern machine learning.</p></li>
<li><p><strong>Manual Data Inspection</strong>: In many cases, data analysis was done manually, especially in fields like qualitative research, where researchers would manually categorize and interpret data.</p></li>
</ol>
<p>Modern machine learning algorithms represent an evolutionary leap from classical statistical methods, building upon and significantly extending their foundational principles. While classical statistics provided the initial framework for understanding and modeling relationships in data, primarily through hypothesis testing and linear models, contemporary machine learning techniques have expanded this scope to accommodate larger, more complex datasets.</p>
<p>Machine learning algorithms incorporate and refine statistical concepts, allowing for more nuanced and intricate models capable of capturing non-linear relationships and high-dimensional data interactions. Techniques like neural networks, support vector machines, and ensemble methods have transcended traditional limitations by integrating computational advancements and algorithmic innovations. This progression has enabled the handling of unstructured data types, such as text and images, which were previously challenging to analyze using classical methods. Additionally, machine learning’s adaptability and predictive capabilities, particularly in real-time and dynamic environments, mark a significant advancement over more static traditional statistical approaches.</p>
<p>In essence, modern machine learning represents a synergy of statistical fundamentals with cutting-edge computational techniques, leading to more robust, efficient, and accurate models for data analysis and prediction.</p>
<section id="types-of-learning" class="level2" data-number="45.1">
<h2 data-number="45.1" class="anchored" data-anchor-id="types-of-learning"><span class="header-section-number">45.1</span> Types of Learning</h2>
<p>Machine learning can be broadly categorized into several types, each with its own methodology and application areas. The main types of machine learning are:</p>
<ol type="1">
<li><p><strong>Supervised Learning</strong>: This is the most prevalent type of machine learning. In supervised learning, the algorithm is trained on a labeled dataset, meaning that each example in the training dataset is paired with the correct output. The algorithm learns by comparing its actual output with correct outputs to find errors and modify the model accordingly. It is used for applications like regression and classification tasks.</p></li>
<li><p><strong>Unsupervised Learning</strong>: In unsupervised learning, the training data is not labeled, so the algorithm must find patterns and relationships in the data <em>on its own</em>. Common unsupervised learning methods include clustering and dimensionality reduction. These techniques are often used for exploratory data analysis, customer segmentation, and image and pattern recognition.</p></li>
<li><p><strong>Semi-Supervised Learning</strong>: This approach lies between supervised and unsupervised learning. It uses a small amount of labeled data along with a larger amount of unlabeled data. This method can improve learning accuracy while reducing the effort required to label data. It’s useful when labeling data becomes too expensive or time-consuming.</p></li>
<li><p><strong>Reinforcement Learning</strong>: In reinforcement learning, an agent learns to make decisions by performing certain actions and receiving rewards or penalties in return. It’s different from other types of learning in that it focuses on performance in a dynamic environment and is based on feedback rather than data. Reinforcement learning is widely used in areas like robotics, gaming, and navigation.</p></li>
<li><p><strong>Deep Learning</strong>: A subset of machine learning, deep learning uses multi-layered neural networks to analyze various factors in large amounts of data. Deep learning is particularly known for its effectiveness in fields like computer vision, speech recognition, and natural language processing.</p></li>
</ol>
<p>Each type of machine learning has its strengths and is suitable for different kinds of problems and data sets. The choice of which type to use depends on the specific requirements and constraints of the task at hand.</p>
<p>Apart from the primary types of machine learning (supervised, unsupervised, semi-supervised, and reinforcement learning), there are several other techniques and approaches that can be used for learning in different contexts. These include:</p>
<ol type="1">
<li><p><strong>Transfer Learning</strong>: This technique involves taking a pre-trained model (usually trained on a large benchmark dataset) and fine-tuning it for a specific task. Transfer learning is particularly useful when the available data for a task is limited, as it leverages learned features from a related task.</p></li>
<li><p><strong>Ensemble Methods</strong>: Ensemble methods combine multiple machine learning models to improve performance. Techniques like bagging, boosting, and stacking are used to create a stronger model by aggregating the predictions from multiple models. Common examples include Random Forests and Gradient Boosted Machines.</p></li>
<li><p><strong>Active Learning</strong>: In active learning, the algorithm selectively queries the user or a database to label new data points with the greatest potential to improve the model. This approach is useful when labeled data is scarce or expensive to obtain.</p></li>
<li><p><strong>Dimensionality Reduction</strong>: Techniques like Principal Component Analysis (PCA) and t-Distributed Stochastic Neighbor Embedding (t-SNE) reduce the number of variables under consideration, making the data easier to explore and visualize. Dimensionality reduction is often used in preprocessing to improve the efficiency of other learning algorithms.</p></li>
<li><p><strong>Feature Engineering and Selection</strong>: This involves selecting the most relevant features or constructing new features from raw data to improve the performance of machine learning models.</p></li>
<li><p><strong>Evolutionary Algorithms</strong>: These are algorithms that mimic the process of natural selection to solve optimization problems. They are used for tasks where the search space is extremely large and complex.</p></li>
<li><p><strong>Federated Learning</strong>: A technique where machine learning models are trained across multiple decentralized devices or servers holding local data samples, without exchanging them. This approach is beneficial for privacy preservation and efficient use of bandwidth.</p></li>
<li><p><strong>Rule-Based Learning</strong>: This method involves creating a set of rules for decision-making, which can be derived from domain knowledge or learned from data. It includes approaches like decision trees and rule-based classifiers.</p></li>
<li><p><strong>Anomaly Detection</strong>: Used to identify unusual patterns that do not conform to expected behavior. It is commonly used in fraud detection, network security, and fault detection.</p></li>
<li><p><strong>Time Series Analysis</strong>: Specialized techniques for analyzing time-ordered data, often involving unique methods for dealing with trends, seasonality, and autocorrelation in data.</p></li>
</ol>
<p>Each of these techniques addresses specific types of problems and data characteristics, offering a range of tools that can be applied to a wide array of learning tasks in different domains.</p>
</section>
<section id="how-does-a-machine-learn" class="level2" data-number="45.2">
<h2 data-number="45.2" class="anchored" data-anchor-id="how-does-a-machine-learn"><span class="header-section-number">45.2</span> How does a Machine Learn?</h2>
<section id="a-function-approximation-perspective" class="level3" data-number="45.2.1">
<h3 data-number="45.2.1" class="anchored" data-anchor-id="a-function-approximation-perspective"><span class="header-section-number">45.2.1</span> A Function Approximation Perspective</h3>
<p>Machine learning is a process where a computer system is taught to make predictions or decisions based on data, a process deeply intertwined with the concept of function approximation. At its core, machine learning involves a machine ‘learning’ from data—either labeled in supervised learning or unlabeled in unsupervised learning—by processing this data to extract patterns or rules. The crux of this process is to approximate a function that accurately maps inputs to outputs. For instance, in classification tasks, this function categorizes input data, while in regression tasks, it maps inputs to a continuous output. The overarching goal is to <em>discover</em> the function that most accurately represents the relationship between these inputs and outputs.</p>
<p>Different machine learning models, such as decision trees, neural networks, or support vector machines, employ various approaches to function approximation. The choice of model is dictated by the complexity of the function being approximated and the specifics of the problem at hand. <em>In essence, machine learning is a quest to find the best possible mapping from inputs to outputs, based on the available data</em>. The effectiveness of a machine learning model is largely determined by how well it approximates the underlying function and how effectively it generalizes this knowledge to new data.</p>
<p>Various architectures or models used in machine learning, each serving as a different approach to function approximation, include:</p>
<ol type="1">
<li><p><strong>Linear and Logistic Regression</strong>: These are the simplest forms of function approximators used for prediction. Linear regression is used for continuous output prediction, and logistic regression is employed for binary classification tasks. They approximate a linear relationship between input features and the output.</p></li>
<li><p><strong>Decision Trees and Random Forests</strong>: Decision trees split data based on certain criteria and are particularly good for interpretability. Random forests are an ensemble of decision trees and are more robust and less prone to overfitting. They approximate functions by segmenting the input space into simpler, easier to model regions.</p></li>
<li><p><strong>Support Vector Machines (SVMs)</strong>: SVMs are effective in high-dimensional spaces and for classification tasks. They work by finding a hyperplane that best divides a dataset into classes. SVMs are particularly good for approximating complex nonlinear relationships using kernel methods.</p></li>
<li><p><strong>Neural Networks and Deep Learning</strong>: These models consist of layers of interconnected nodes or neurons and are capable of learning complex, nonlinear relationships. Deep learning models, with multiple hidden layers, are particularly potent at approximating functions from large amounts of data, especially in fields like image and speech recognition.</p></li>
<li><p><strong>Convolutional Neural Networks (CNNs)</strong>: Specialized for processing structured array data like images, CNNs are powerful in function approximation for tasks like image classification and object detection. They do this by learning spatial hierarchies of features from the input data.</p></li>
<li><p><strong>Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)</strong>: These are designed for sequential data like time series or language. RNNs and LSTM models can capture temporal dynamics and are useful in approximating functions where the output is dependent on previous elements in the input sequence.</p></li>
<li><p><strong>Autoencoders</strong>: Used for unsupervised learning tasks like dimensionality reduction or feature learning, autoencoders learn efficient data codings in an unsupervised manner, often for the purpose of reconstructing the input data from compressed representations.</p></li>
<li><p><strong>Reinforcement Learning Models</strong>: These models learn optimal actions through trial and error to maximize some notion of cumulative reward. They are typically used in dynamic environments where the function to be approximated is the best action to take in a given state.</p></li>
</ol>
<p>Each of these models has its strengths and is suited for particular types of problems and data characteristics. The choice of model depends on the specific requirements of the task at hand, such as the complexity of the function to be approximated, the nature of the input and output data, and the amount of training data available.</p>
</section>
<section id="learning-as-an-optimization-process" class="level3" data-number="45.2.2">
<h3 data-number="45.2.2" class="anchored" data-anchor-id="learning-as-an-optimization-process"><span class="header-section-number">45.2.2</span> Learning as an Optimization Process</h3>
<p>The learning process is often described as ‘training a model.’ Here, a machine learning algorithm iteratively adjusts its parameters to minimize the difference between its predictions and the actual outcomes in the training data, effectively seeking the function that best fits the data. The machine learning model, in this sense, acts as a function approximator, striving to estimate the true underlying function that describes the input-output relationship. The accuracy of this approximation is influenced by the model’s complexity, the nature of the data, and the algorithm used.</p>
<p>A key part of this process is minimizing an error or loss function, which quantifies the divergence between the model’s predictions and the actual values. Successfully minimizing this error leads to a more accurate approximation of the underlying function. An essential aspect of a machine learning model is its ability to generalize from the training data to unseen data, ensuring that a good function approximation not only fits the training data well but can also predict new, unseen data accurately.</p>
<p>In machine learning, cost functions or loss functions quantify the error between the predicted values by the model and the actual values in the data. Different problems use different loss functions, depending on the nature of the task. Some of the commonly used loss functions are summarized below.</p>
<ol type="1">
<li><strong>Mean Squared Error (MSE)</strong>:
<ul>
<li><em>Used in</em>: Regression Problems.</li>
<li><em>Description</em>: MSE measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. It’s widely used due to its simplicity and the fact that it penalizes larger errors more heavily.</li>
</ul></li>
<li><strong>Mean Absolute Error (MAE)</strong>:
<ul>
<li><em>Used in</em>: Regression Problems.</li>
<li><em>Description</em>: MAE measures the average of the absolute differences between predicted values and actual values. It gives a linear score, which means all individual differences are weighted equally in the average.</li>
</ul></li>
<li><strong>Cross-Entropy Loss or Log Loss</strong>:
<ul>
<li><em>Used in</em>: Classification Problems.</li>
<li><em>Description</em>: Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1. It increases as the predicted probability diverges from the actual label, making it ideal for models where the outputs are probabilities.</li>
</ul></li>
<li><strong>Hinge Loss</strong>:
<ul>
<li><em>Used in</em>: Support Vector Machines for binary classification.</li>
<li><em>Description</em>: Hinge loss is used primarily for “maximum-margin” classification, most notably for support vector machines. It is intended to create a wide margin between data points of different classes.</li>
</ul></li>
<li><strong>Binary Cross-Entropy Loss</strong>:
<ul>
<li><em>Used in</em>: Binary Classification Problems.</li>
<li><em>Description</em>: A special case of cross-entropy loss for binary classification tasks. It calculates the cross-entropy loss between the predicted and actual labels.</li>
</ul></li>
<li><strong>Categorical Cross-Entropy Loss</strong>:
<ul>
<li><em>Used in</em>: Multi-class Classification Problems.</li>
<li><em>Description</em>: It’s used when assigning an observation to one of more than two classes. This loss function is ideal for multi-class classification where each example belongs to a single class.</li>
</ul></li>
<li><strong>Sparse Categorical Cross-Entropy Loss</strong>:
<ul>
<li><em>Used in</em>: Multi-class Classification Problems with many classes.</li>
<li><em>Description</em>: It’s the same as categorical cross-entropy but is used when your classes are mutually exclusive, and the labels are sparse (i.e., each label is a large array of mostly zeros).</li>
</ul></li>
<li><strong>Kullback-Leibler (KL) Divergence</strong>:
<ul>
<li><em>Used in</em>: Model reliability and probability distributions.</li>
<li><em>Description</em>: KL Divergence measures how one probability distribution diverges from a second, expected probability distribution. It’s used in scenarios like variational autoencoders in deep learning.</li>
</ul></li>
<li><strong>Huber Loss</strong>:
<ul>
<li><em>Used in</em>: Robust Regression Problems.</li>
<li><em>Description</em>: Huber loss is less sensitive to outliers in data than the squared error loss. It’s used in robust regression, combining the properties of MSE and MAE.</li>
</ul></li>
<li><strong>Cosine Similarity Loss</strong>:
<ul>
<li><em>Used in</em>: Measuring similarity between two vectors.</li>
<li><em>Description</em>: This loss function measures the cosine of the angle between two vectors and is used in various applications like recommendation systems and text analysis where the magnitude of vectors is not as important as their direction. This is essentially the inner product between two vectors.</li>
</ul></li>
</ol>
<p>Each of these loss functions has a specific scenario or type of problem where they are most effective, and the choice of a loss function can significantly influence the performance of the machine learning model.</p>
</section>
</section>
<section id="summary" class="level2" data-number="45.3">
<h2 data-number="45.3" class="anchored" data-anchor-id="summary"><span class="header-section-number">45.3</span> Summary</h2>
<p>In conclusion, machine learning can fundamentally be understood as a process of <em>function representation</em> and <em>optimization</em>. At its heart, it revolves around the concept of accurately representing complex relationships within data through mathematical functions, and then optimizing these functions to minimize errors in predictions or decisions. Whether it’s through supervised learning that maps input to output, unsupervised learning that uncovers hidden patterns, or reinforcement learning that navigates through a space of actions, each method seeks to <em>approximate an underlying function as closely as possible</em>. The optimization aspect, achieved through various algorithms and loss functions, refines these representations, striving to improve accuracy and efficiency. This dual focus on representation and optimization underscores the essence of machine learning, highlighting its role as a powerful tool in translating vast, often unstructured data into meaningful insights and actions.</p>
</section>
</section>
<section id="linear-algebra" class="level1" data-number="46">
<h1 data-number="46"><span class="header-section-number">46</span> Linear Algebra</h1>
<section id="vectors" class="level2" data-number="46.1">
<h2 data-number="46.1" class="anchored" data-anchor-id="vectors"><span class="header-section-number">46.1</span> Vectors</h2>
<p>A vector of numbers is a one-dimensional arrangement of numbers (integers, real, complex, etc.).</p>
<p>A column-vector is a column arrangement of mathematical objects, e.g., <span class="math display">\[\begin{bmatrix} v_1 \\ v_2 \\v_3\\ \vdots \\ v_n\end{bmatrix}.\]</span></p>
<p>Examples include vector of integers: <span class="math inline">\(\begin{bmatrix} 1 \\ 2 \\3\\ \vdots \\ 20\end{bmatrix}\)</span>, vector of real-numbers: <span class="math inline">\(\begin{bmatrix} 1.234 \\ 2.345 \\3.456\\ \vdots \\ 20.212\end{bmatrix}\)</span>, and vector of complex numbers: <span class="math inline">\(\begin{bmatrix} 1+2j \\ 2+3j \\4+5j\\ \vdots \\ 20+21j\end{bmatrix}\)</span>.</p>
<p>Sometimes we represent the entire vector using a symbol with boldface font, e.g., <span class="math display">\[\x =  \begin{bmatrix} 1 \\ 2 \\3\\ \vdots \\5 \end{bmatrix},\]</span> and refer to the entire vector as <span class="math inline">\(\x\)</span>.</p>
<pre class="{python}"><code>import numpy as np

X = np.array([1, 2, 3, 4, 5])  # This is an array with 5 elements.
print(X)</code></pre>
<section id="dimension" class="level3" data-number="46.1.1">
<h3 data-number="46.1.1" class="anchored" data-anchor-id="dimension"><span class="header-section-number">46.1.1</span> Dimension</h3>
<p>The number of elements in a vector is the dimension of the vector. For example, <span class="math display">\[\x =  \begin{bmatrix} 1 \\ 2 \\3\\ \vdots \\10 \end{bmatrix},\]</span> is ten dimensional. If all the entries are real numbers, then we compactly represent <span class="math inline">\(\x\in\real^{10}\)</span>, where <span class="math inline">\(\real^n\)</span> is the space of n-dimensional vectors with real entries. A vector can have elements that are complex numbers as well. In that case, we will denote <span class="math inline">\(\x\in\complex^{n}\)</span>, where <span class="math inline">\(\complex^n\)</span> is the space of n-dimensional vectors with complex entries.</p>
<p>Here is a Python code computing the dimension of a vector.</p>
<pre class="{python}"><code>import numpy as np

X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])  # This is an array with 10 elements.
print("Dimension of X is: ", X.shape)</code></pre>
</section>
<section id="transpose" class="level3" data-number="46.1.2">
<h3 data-number="46.1.2" class="anchored" data-anchor-id="transpose"><span class="header-section-number">46.1.2</span> Transpose</h3>
<p>If a vector <span class="math inline">\(\x\in\real^n := \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\)</span> then the transpose of <span class="math inline">\(\x\)</span> is denoted by <span class="math inline">\(\x^T\)</span> and defined by <span class="math display">\[
\x^T = \begin{bmatrix} x_1 &amp; \cdots &amp; x_n \end{bmatrix}.\]</span> <em>Note:</em> <span class="math inline">\((\x^T)^T = \x\)</span>, i.e.&nbsp;the transpose of a transposed vector is the same vector.</p>
<p><em>Note:</em> If <span class="math inline">\(\x\)</span> is a column vector, <span class="math inline">\(\x^T\)</span> is a row vector and vice versa.</p>
<p>Here is a Python code computing transpose of a vector.</p>
<pre class="{python}"><code>import numpy as np

# Define a vector as a 1D NumPy array
vector = np.array([1, 2, 3, 4])

# Compute the transpose of the vector (which is still the same vector)
transpose_vector = vector.T

# Print the original vector and its transpose
print("Vector:", vector)

print("Transpose:", transpose_vector)
print(transpose_vector)
</code></pre>
<p>Python doesn’t print the transpose of a column vector as a row vector. If they are treated as matrices then there will be a difference in how they are printed.</p>
</section>
<section id="operations" class="level3" data-number="46.1.3">
<h3 data-number="46.1.3" class="anchored" data-anchor-id="operations"><span class="header-section-number">46.1.3</span> Operations</h3>
<section id="negative-of-vector" class="level4" data-number="46.1.3.1">
<h4 data-number="46.1.3.1" class="anchored" data-anchor-id="negative-of-vector"><span class="header-section-number">46.1.3.1</span> Negative of Vector</h4>
<p>Given a vector <span class="math inline">\(\x = \begin{bmatrix}x_1 \\ \vdots \\ x_n \end{bmatrix}\)</span>, the negative of <span class="math inline">\(\x\)</span> is <span class="math display">\[-\x := \begin{bmatrix}-x_1\\ \vdots \\ -x_n\end{bmatrix}.\]</span></p>
</section>
<section id="multiplication-by-a-scalar" class="level4" data-number="46.1.3.2">
<h4 data-number="46.1.3.2" class="anchored" data-anchor-id="multiplication-by-a-scalar"><span class="header-section-number">46.1.3.2</span> Multiplication by a Scalar</h4>
<p>Multiplication of a scalar with a vector is defined as <span class="math display">\[
\alpha\x =  \begin{bmatrix}\alpha x_1 \\ \alpha x_2 \\ \vdots \end{bmatrix},
\]</span> i.e., the scalar <span class="math inline">\(\alpha\)</span> multiplies all the elements of <span class="math inline">\(\x\)</span>. Also, <span class="math display">\[\alpha\x = \x\alpha.\]</span></p>
</section>
<section id="vector-equality" class="level4" data-number="46.1.3.3">
<h4 data-number="46.1.3.3" class="anchored" data-anchor-id="vector-equality"><span class="header-section-number">46.1.3.3</span> Vector Equality</h4>
<p>Vector equality between two vectors is defined elementwise, i.e., the condition <span class="math inline">\(\x = \y\)</span> is equivalent to <span class="math inline">\(n\)</span> conditions <span class="math inline">\(x_i = y_i\)</span>, for <span class="math inline">\(i=1,\cdots,n\)</span>. Clearly, <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> would have to have the same dimension.</p>
</section>
<section id="vector-addition" class="level4" data-number="46.1.3.4">
<h4 data-number="46.1.3.4" class="anchored" data-anchor-id="vector-addition"><span class="header-section-number">46.1.3.4</span> Vector Addition</h4>
<p>Given two vectors <span class="math inline">\(\x \in \real^n\)</span> and <span class="math inline">\(\y\in\real^n\)</span> with respective components <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>, then <span class="math inline">\(\z := \x +\y\)</span> has components <span class="math inline">\(z_i := x_i + y_i\)</span>. That is <span class="math display">\[\z := \x + \y = \begin{bmatrix}x_1\\ \vdots \\ x_n\end{bmatrix} + \begin{bmatrix}y_1\\ \vdots \\ y_n\end{bmatrix} = \begin{bmatrix}x_1 + y_1\\ \vdots \\ x_n + y_n\end{bmatrix} = \begin{bmatrix}z_1\\ \vdots \\ z_n\end{bmatrix} .\]</span> <em>Note:</em> <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> must of the same dimension.</p>
<p>Vector addition satisfies the following properties: <span class="math display">\[\begin{align*}
\x + (\y + \z) &amp;= (\x + \y) + \z,\\
\alpha(\x + \y)  &amp;= \alpha\x + \alpha\y.
\end{align*}\]</span></p>
</section>
<section id="vector-subtraction" class="level4" data-number="46.1.3.5">
<h4 data-number="46.1.3.5" class="anchored" data-anchor-id="vector-subtraction"><span class="header-section-number">46.1.3.5</span> Vector Subtraction</h4>
<p>Given two vectors <span class="math inline">\(\x \in \real^n\)</span> and <span class="math inline">\(\y\in\real^n\)</span> with respective components <span class="math inline">\(x_i\)</span> and <span class="math inline">\(y_i\)</span>, then <span class="math inline">\(\z := \x -\y = \x + (-\y)\)</span>.<br> <em>Note:</em> <span class="math inline">\(\x\)</span> and <span class="math inline">\(\y\)</span> must of the same dimension.</p>
</section>
<section id="inner-product" class="level4" data-number="46.1.3.6">
<h4 data-number="46.1.3.6" class="anchored" data-anchor-id="inner-product"><span class="header-section-number">46.1.3.6</span> Inner Product</h4>
<p>Inner product between vectors <span class="math inline">\(\x\in\real^n\)</span> and <span class="math inline">\(\y\in\real^n\)</span> is defined as <span class="math display">\[\langle\x,\y\rangle := \x^T\y = \y^T\x = \Sigma_{i=1}^n x_iy_i.\]</span><br> <em>Note:</em> Inner product results in a scalar.</p>
<p>Here is a Python code computing inner product of vectors.</p>
<pre class="{python}"><code>import numpy as np

X = np.array([1, 2, 3, 4, 5])
Y = np.array([6, 7, 8, 9, 10])
print("Inner product between X and Y: ", X.dot(Y))
print("Inner product between Y and X: ", Y.dot(X)) # Should get the same result.</code></pre>
</section>
<section id="outer-product" class="level4" data-number="46.1.3.7">
<h4 data-number="46.1.3.7" class="anchored" data-anchor-id="outer-product"><span class="header-section-number">46.1.3.7</span> Outer Product</h4>
<p>Outer product between two vectors <span class="math inline">\(\x\in\real^n\)</span> and <span class="math inline">\(\y\in\real^m\)</span> is defined as <span class="math display">\[
\x\y^T = \begin{bmatrix}x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} \begin{bmatrix}y_1 &amp; y_2 &amp; \cdots &amp; y_n\end{bmatrix} = \begin{bmatrix} x_1y_1 &amp; x_1y_2 &amp; \cdots &amp; x_1y_m\\
x_2y_1 &amp; x_2y_2 &amp; \cdots &amp; x_2 y_m \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
x_n y_1 &amp; x_n y_2 &amp; \cdots &amp; x_n y_m
\end{bmatrix}
\]</span> <em>Note:</em> Clearly <span class="math inline">\(\x\y^T \neq \y\x^T\)</span>. <br> <em>Note:</em> Outer product result in a matrix.</p>
<p>Here is a Python code computing outer product of vectors.</p>
<pre class="{python}"><code>import numpy as np

X = np.array([1, 2, 3, 4, 5])
Y = np.array([6, 7, 8, 9, 10])
print("Outer product between X and Y: \n", np.multiply.outer(X, Y))
print("\nOuter product between Y and X: \n", np.multiply.outer(Y, X))
</code></pre>
</section>
</section>
<section id="length-of-a-vector" class="level3" data-number="46.1.4">
<h3 data-number="46.1.4" class="anchored" data-anchor-id="length-of-a-vector"><span class="header-section-number">46.1.4</span> Length of a Vector</h3>
<p>The length of a vector <span class="math inline">\(\x\in\real^n\)</span> is denoted by <span class="math display">\[\|\x\|_2 := \sqrt{x_1^2 + \cdots + x_n^2} = \sqrt{\x^T\x}.\]</span> Therefore, the length of a vector is the square-root of the inner product with itself. Often we will drop the subscript <span class="math inline">\(2\)</span> in <span class="math inline">\(\|\x\|_2\)</span>, when it is clear from context.</p>
<p>Here is a Python code computing length of a vector.</p>
<pre class="{python}"><code>import numpy as np

X = np.array([1, 2, 3, 4, 5])
print("Length of vector X is: \n", np.sqrt(X.dot(X)))</code></pre>
</section>
<section id="vector-norms" class="level3" data-number="46.1.5">
<h3 data-number="46.1.5" class="anchored" data-anchor-id="vector-norms"><span class="header-section-number">46.1.5</span> Vector Norms</h3>
<p>A norm is a function <span class="math inline">\(\|\cdot\|: \set{V} \rightarrow \set{R}\)</span> from a vector space <span class="math inline">\(\set{V}\)</span> over a field (typically <span class="math inline">\(\set{R}^n\)</span> or <span class="math inline">\(\set{C}^n\)</span>) to the non-negative real numbers. It satisfies the following properties:</p>
<ol type="1">
<li><p>Non-negativity: <span class="math display">\[\forall \x \in \set{V}, \|\x\| \geq 0 \text{ and } \|\x\| = 0 \iff \x \text{ is the zero vector}.\]</span></p></li>
<li><p>Scalar Multiplication: <span class="math display">\[\forall \alpha \text{ (a scalar) and } \forall \x \in \set{V}, \|\alpha \x\| = |\alpha| \|\x\|.\]</span></p></li>
<li><p>Triangle Inequality: <span class="math display">\[\forall \x, \y \in \set{V}, \|\x + \y\| \leq \|\x\| + \|\y\|.\]</span></p></li>
<li><p>Definiteness: <span class="math display">\[\|\x\| = 0 \iff \x \text{ is the zero vector}.\]</span></p></li>
</ol>
<p>In general, we can define <span class="math display">\[\|\x\|_p := \left(x_1^p + \cdots + x_n^p\right)^{\frac{1}{p}},\]</span> which is the <span class="math inline">\(p^\text{th}\)</span> norm of <span class="math inline">\(\x\)</span>.</p>
<p>We have for <span class="math inline">\(p=1,2,\infty\)</span>:</p>
<ul>
<li><p>Manhattan Norm (<span class="math inline">\(l_1\)</span> Norm): <span class="math display">\[\|\x\|_1 = |x_1| + |x_2| + \cdots + |x_n|.\]</span></p></li>
<li><p>Euclidean Norm (<span class="math inline">\(l_2\)</span> Norm): <span class="math display">\[\|\x\|_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}.\]</span></p></li>
<li><p>Maximum Norm (<span class="math inline">\(l_\infty\)</span> Norm): <span class="math display">\[\|\x\|_{\infty} = \max(|x_1|, |x_2|, \cdots, |x_n|).\]</span></p></li>
</ul>
<p>Here is a Python code computing various vector norms.</p>
<pre class="{python}"><code>import numpy as np

# Define a vector as a NumPy array
vector = np.array([1, 2, 3])

# Compute different vector norms
L1_norm = np.linalg.norm(vector, ord=1)  # L1 norm (Manhattan norm)
L2_norm = np.linalg.norm(vector, ord=2)  # L2 norm (Euclidean norm)
inf_norm = np.linalg.norm(vector, ord=np.inf)  # Infinity norm (Maximum norm)

# Print the original vector and its norms
print("Vector:")
print(vector)

print(f"L1 Norm: {L1_norm}")
print(f"L2 Norm: {L2_norm}")
print(f"Infinity Norm: {inf_norm}")
</code></pre>
</section>
</section>
<section id="matrices" class="level2" data-number="46.2">
<h2 data-number="46.2" class="anchored" data-anchor-id="matrices"><span class="header-section-number">46.2</span> Matrices</h2>
<p>Matrices are two-dimensional arrangement of mathematical objects, such as real-numbers, complex numbers, functions, etc.</p>
<p>An <span class="math inline">\(m\times n\)</span> matrix is defined as <span class="math display">\[
\A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
\vdots &amp; \vdots &amp; &amp; \vdots,\\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix},
\]</span> where <span class="math inline">\(a_{ij}\)</span> is the matrix element in the <span class="math inline">\(i^\text{th}\)</span> row and the <span class="math inline">\(j^\text{th}\)</span> column. The bold-faced symbol <span class="math inline">\(\A\)</span> is used to refer to the entire matrix.</p>
<section id="dimension-1" class="level3" data-number="46.2.1">
<h3 data-number="46.2.1" class="anchored" data-anchor-id="dimension-1"><span class="header-section-number">46.2.1</span> Dimension</h3>
<p>The dimension of a matrix is defined by the number of rows and the number of columns. A matrix with dimension <span class="math inline">\(m\times n\)</span> has <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns. We will use the notation <span class="math inline">\(\A \in \real^{m\times n}\)</span> to represent <span class="math inline">\(m\times n\)</span> vectors of real numbers.</p>
<p>A matrix of dimension <span class="math inline">\(m\times n\)</span> is called <em>square</em> if <span class="math inline">\(m=n\)</span>.</p>
</section>
<section id="transpose-1" class="level3" data-number="46.2.2">
<h3 data-number="46.2.2" class="anchored" data-anchor-id="transpose-1"><span class="header-section-number">46.2.2</span> Transpose</h3>
<p>Matrix transpose is denote by <span class="math inline">\(\A^T\)</span>, which is defined by interchanging the rows and columns. For example,</p>
<p><span class="math display">\[
\A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n}\\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n}\\
\vdots &amp; \vdots &amp; &amp; \vdots,\\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}, \text{ then }
\A^T = \begin{bmatrix}
a_{11} &amp; a_{21} &amp; \cdots &amp; a_{m1}\\
a_{12} &amp; a_{22} &amp; \cdots &amp; a_{m2}\\
\vdots &amp; \vdots &amp; &amp; \vdots,\\
a_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn}
\end{bmatrix}.
\]</span></p>
<p>That is, if <span class="math inline">\(\B=\A^T\)</span>, then <span class="math inline">\(b_{ij} = a_{ji}\)</span>.</p>
<p>Therefore, if <span class="math inline">\(\A\)</span> is an <span class="math inline">\(m\times n\)</span> matrix, then <span class="math inline">\(\A^T\)</span> is an <span class="math inline">\(n\times m\)</span> matrix.</p>
<p>Here is a Python code computing matrix transpose.</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

# Compute the transpose
transpose_matrix = np.transpose(matrix)

# Alternatively, you can use the T attribute to compute the transpose
# transpose_matrix = matrix.T

# Print the original matrix and its transpose
print("Matrix:")
print(matrix)

print("Transpose:")
print(transpose_matrix)
</code></pre>
</section>
<section id="matrix-construction-from-vectors" class="level3" data-number="46.2.3">
<h3 data-number="46.2.3" class="anchored" data-anchor-id="matrix-construction-from-vectors"><span class="header-section-number">46.2.3</span> Matrix Construction from Vectors</h3>
<p>Given <span class="math inline">\(n\)</span> vectors <span class="math inline">\(\x_i\)</span>, for <span class="math inline">\(i=1,\cdots,n\)</span>, each of dimension <span class="math inline">\(m\)</span>; we can construct an <span class="math inline">\(m\times n\)</span> matrix as <span class="math display">\[\A = \begin{bmatrix}\x_1 &amp; \x_2 &amp; \cdots &amp; \x_n\end{bmatrix},\]</span> and <span class="math display">\[\A^T = \begin{bmatrix}\x^T_1 \\ \x^T_2 \\ \vdots \\ \x^T_n\end{bmatrix}.\]</span></p>
<p>Here is a Python code to construct matrices from vectors.</p>
<pre class="{python}"><code>import numpy as np

# Define two vectors as NumPy arrays
vector1 = np.array([1, 2, 3])
vector2 = np.array([4, 5, 6])

# Construct a matrix by stacking vectors horizontally (as rows)
matrix_horizontal = np.hstack((vector1.reshape(-1, 1), vector2.reshape(-1, 1)))

# Construct a matrix by stacking vectors vertically (as columns)
matrix_vertical = np.vstack((vector1, vector2))

# Print the original vectors and the constructed matrices
print("Vector 1:")
print(vector1)

print("Vector 2:")
print(vector2)

print("Matrix Constructed Horizontally (as rows):")
print(matrix_horizontal)

print("Matrix Constructed Vertically (as columns):")
print(matrix_vertical)
</code></pre>
</section>
<section id="rank-of-a-matrix" class="level3" data-number="46.2.4">
<h3 data-number="46.2.4" class="anchored" data-anchor-id="rank-of-a-matrix"><span class="header-section-number">46.2.4</span> Rank of a Matrix</h3>
<p>The rank of a matrix is a fundamental concept in linear algebra and represents the maximum number of linearly independent rows or columns in the matrix. In other words, it quantifies the dimensionality of the vector space spanned by the rows or columns of the matrix. A matrix’s rank can provide important information about its properties and relationships between its rows and columns.</p>
<p>A matrix is said to have <em>full rank</em> if its rank equals the largest possible for a matrix of the same dimensions, which is the lesser of the number of rows and columns. A matrix is said to be <em>rank-deficient</em> if it does not have full rank. The rank deficiency of a matrix is the difference between the lesser of the number of rows and columns, and the rank.</p>
<p>The rank of a matrix can be determined by various methods, including row reduction (Gaussian elimination) and by counting the number of non-zero rows or columns in its reduced row echelon form (RREF).</p>
<p>Here is a Python code computing rank of a matrix</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix 
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

# Compute the rank of the matrix
rank = np.linalg.matrix_rank(matrix)

# Print the matrix and its rank
print("Matrix:")
print(matrix)

print(f"Rank: {rank}")
</code></pre>
</section>
<section id="eigen-values-and-eigen-vectors-of-a-matrix" class="level3" data-number="46.2.5">
<h3 data-number="46.2.5" class="anchored" data-anchor-id="eigen-values-and-eigen-vectors-of-a-matrix"><span class="header-section-number">46.2.5</span> Eigen-Values and Eigen-Vectors of a Matrix</h3>
<p>Eigenvalues, also known as characteristic values or latent roots, are a fundamental concept in linear algebra and matrix theory. They are associated with square matrices and play a crucial role in various applications, including physics, engineering, data analysis, and more.</p>
<p>An eigenvalue of a square matrix <span class="math inline">\(\A\)</span> is a scalar (a single number) <span class="math inline">\(\lambda\)</span> such that when <span class="math inline">\(\A\)</span> is multiplied by a certain nonzero vector <span class="math inline">\(\vo{v}\)</span>, the result is a scaled version of <span class="math inline">\(\vo{v}\)</span>:</p>
<p><span class="math display">\[ \A\vo{v} = \lambda\vo{v}.\]</span></p>
<p>In this equation:</p>
<ul>
<li><span class="math inline">\(\A\)</span> is the square matrix.</li>
<li><span class="math inline">\(\vo{v}\)</span> is a nonzero vector called the eigenvector associated with the eigenvalue <span class="math inline">\(\lambda\)</span>.</li>
<li><span class="math inline">\(\lambda\)</span> is the eigenvalue.</li>
</ul>
<p>In simpler terms, when you multiply a matrix by its eigenvector, the result is a vector that points in the same direction as the original eigenvector, but possibly with a different magnitude (scaled by <span class="math inline">\(\lambda\)</span>).</p>
<p>To find the eigenvalues of a matrix in practice, we typically use numerical methods or specialized libraries like NumPy in Python. The eigenvalues can be computed using functions like <code>numpy.linalg.eigvals</code> in Python, which returns an array of eigenvalues for a given matrix.</p>
<p>Here is a Python code computing the eigenvalues of a matrix.</p>
<pre class="{python}"><code>import numpy as np

# Define a square matrix
matrix = np.array([[1, 2],
                   [3, 4]])

# Compute the eigenvalues
eigenvalues = np.linalg.eigvals(matrix)

# Print the eigenvalues
print("Eigenvalues:")
print(eigenvalues)</code></pre>
</section>
<section id="operations-1" class="level3" data-number="46.2.6">
<h3 data-number="46.2.6" class="anchored" data-anchor-id="operations-1"><span class="header-section-number">46.2.6</span> Operations</h3>
<section id="negative-matrix" class="level4" data-number="46.2.6.1">
<h4 data-number="46.2.6.1" class="anchored" data-anchor-id="negative-matrix"><span class="header-section-number">46.2.6.1</span> Negative Matrix</h4>
<p>If <span class="math inline">\(\A\in\real^{m\times n}\)</span> is a matrix with elements <span class="math inline">\(a_{ij}\)</span>, then <span class="math inline">\(-\A\)</span> is defined by elements <span class="math inline">\(-a_{ij}\)</span>.</p>
</section>
<section id="multiplication-by-a-scalar-1" class="level4" data-number="46.2.6.2">
<h4 data-number="46.2.6.2" class="anchored" data-anchor-id="multiplication-by-a-scalar-1"><span class="header-section-number">46.2.6.2</span> Multiplication by a Scalar</h4>
<p>If <span class="math inline">\(\A\)</span> is a matrix defined by elements <span class="math inline">\(a_{ij}\)</span>, then <span class="math inline">\(\alpha\A\)</span> is defined by the elements <span class="math inline">\(\alpha a_{ij}\)</span>.</p>
</section>
<section id="matrix-equality" class="level4" data-number="46.2.6.3">
<h4 data-number="46.2.6.3" class="anchored" data-anchor-id="matrix-equality"><span class="header-section-number">46.2.6.3</span> Matrix Equality</h4>
<p>Matrix equality is defined only for two matrices with the same dimension, and is defined elementwise. For two matrics <span class="math inline">\(\A\in\real^{m\times n}\)</span> and <span class="math inline">\(\B\in\real^{m\times n}\)</span>, the condition <span class="math inline">\(\A=\B\)</span> is equivalent to <span class="math inline">\(mn\)</span> conditions <span class="math inline">\(a_{ij} = b_{ij}\)</span>, for <span class="math inline">\(i=1,\cdots,m\)</span> and <span class="math inline">\(j=1,\cdots,n\)</span>.</p>
</section>
<section id="matrix-addition" class="level4" data-number="46.2.6.4">
<h4 data-number="46.2.6.4" class="anchored" data-anchor-id="matrix-addition"><span class="header-section-number">46.2.6.4</span> Matrix Addition</h4>
<p>Addition of two matrices <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span> is elementwise, and is only defined if <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span> have the same dimension. If <span class="math inline">\(\C =\A+\B\)</span>, then <span class="math inline">\(c_{ij} = a_{ij} + b_{ij}\)</span>, for <span class="math inline">\(i=1,\cdots,m\)</span> and <span class="math inline">\(j=1,\cdots,n\)</span>.</p>
</section>
<section id="matrix-product" class="level4" data-number="46.2.6.5">
<h4 data-number="46.2.6.5" class="anchored" data-anchor-id="matrix-product"><span class="header-section-number">46.2.6.5</span> Matrix Product</h4>
<p>Product between two matrices <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span> is denoted by <span class="math inline">\(\A\B\)</span>. If <span class="math inline">\(\C = \A\B\)</span>, then the elements of <span class="math inline">\(\C\)</span> are defined by <span class="math display">\[c_{ij} = \sum_{k=1}^p a_{ik}b_{kj},\]</span> where the dimensions <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span> must be compatible. The matrix product <span class="math inline">\(\A\B\)</span> is defined only if the dimension of <span class="math inline">\(\A\)</span> is <span class="math inline">\(m\times p\)</span> and the dimension of <span class="math inline">\(\B\)</span> is <span class="math inline">\(p\times n\)</span>, i.e.&nbsp;<span class="math inline">\(\A\)</span> must have the same number of columns as the number of rows of <span class="math inline">\(\B\)</span>. In that case, the matrix <span class="math inline">\(\C=\A\B\)</span> has dimension <span class="math inline">\(m \times n\)</span>.</p>
<p>Matrix product satisfies the following conditions: <span class="math display">\[\begin{align*}
    &amp;\alpha\A = \A\alpha, \text{ where $\alpha$ is a scalar};\\
    &amp;\A\B \neq \B\A, \\
    &amp;\alpha(\A\B) = (\alpha\A)\B = \A(\alpha\B) = (\A\B)\alpha,\\
    &amp;\A(\B+\C) = \A\B + \A\C,\\
    &amp;(\A\B)^T = \B^T\A^T.
    \end{align*}\]</span></p>
<p>Here is Python code showing matrix multiplication.</p>
<pre class="{python}"><code>import numpy as np

# Define two matrices
matrix_A = np.array([[1, 2],
                     [3, 4]])

matrix_B = np.array([[5, 6],
                     [7, 8]])

# Method 1: Using numpy.dot
result_dot = np.dot(matrix_A, matrix_B)

# Method 2: Using the @ operator
result_operator = matrix_A @ matrix_B

# Print the original matrices and their multiplication results
print("Matrix A:")
print(matrix_A)

print("Matrix B:")
print(matrix_B)

print("Matrix Multiplication using numpy.dot:")
print(result_dot)

print("Matrix Multiplication using @ operator:")
print(result_operator)
</code></pre>
</section>
</section>
<section id="symmetric-matrices" class="level3" data-number="46.2.7">
<h3 data-number="46.2.7" class="anchored" data-anchor-id="symmetric-matrices"><span class="header-section-number">46.2.7</span> Symmetric Matrices</h3>
<p>A symmetric matrix is a <em>square</em> matrix that satisfies the condition <span class="math inline">\(\A = \A^T\)</span>, i.e.&nbsp;<span class="math inline">\(a_{ij} = a_{ji}\)</span>. We often denote the space of symmetric matrices as <span class="math inline">\(\set{S}^n\)</span>, representing <span class="math inline">\(n\times n\)</span> symmetric matrices.</p>
<p><em>Note</em>: Eigen values of a symmetric matrix are all real.</p>
<p>Here is a Python code to generate a symmetric matrix.</p>
<pre class="{python}"><code>import numpy as np

# Specify the size of the symmetric matrix (e.g., 3x3)
matrix_size = 3

# Generate a random square matrix
random_matrix = np.random.rand(matrix_size, matrix_size)

# Make it symmetric by copying the lower triangular part to the upper triangular part
symmetric_matrix = (random_matrix + random_matrix.T) / 2

# Print the symmetric matrix
print("Symmetric Matrix:")
print(symmetric_matrix)

# Compute the eigenvalues
eigenvalues = np.linalg.eigvals(symmetric_matrix)

# All eigen values should be real
print("Eigen values:", eigenvalues)

# Check if all elements are real
are_all_real = np.isreal(eigenvalues).all()

# Assert that all elements are real
assert are_all_real, "All eigen values are not real."

# If the assertion passes, it means all elements are real
print("All eigen values are real.")
</code></pre>
</section>
<section id="skew-symmetric-matrices" class="level3" data-number="46.2.8">
<h3 data-number="46.2.8" class="anchored" data-anchor-id="skew-symmetric-matrices"><span class="header-section-number">46.2.8</span> Skew Symmetric Matrices</h3>
<p>A skew-symmetric matrix is a <em>square</em> matrix and satisfies the condition <span class="math inline">\(\A = -\A^T\)</span>, i.e.&nbsp;<span class="math inline">\(a_{ij} = -a_{ji}\)</span>. The diagonal elements of skew-symmetric matrices are always zero, because <span class="math inline">\(a_{ii} = -a_{ii}\)</span> only when <span class="math inline">\(a_{ii} = 0\)</span>.</p>
<p>Here is a Python code to generate a skew-symmetric matrix</p>
<pre class="{python}"><code>import numpy as np

# Specify the size of the skew-symmetric matrix (e.g., 3x3)
matrix_size = 3

# Generate a random square matrix
random_matrix = np.random.rand(matrix_size, matrix_size)

# Make it skew-symmetric by subtracting its transpose
skew_symmetric_matrix = random_matrix - random_matrix.T

# Print the skew-symmetric matrix
print("Skew-Symmetric Matrix:")
print(skew_symmetric_matrix)
</code></pre>
</section>
<section id="positive-semi-definite-matrices" class="level3" data-number="46.2.9">
<h3 data-number="46.2.9" class="anchored" data-anchor-id="positive-semi-definite-matrices"><span class="header-section-number">46.2.9</span> Positive (Semi) Definite Matrices</h3>
<ul>
<li><p>A symmetric matrix <span class="math inline">\(\A\)</span> is positive semi-definite if the eigen values are <em>greate than or equal to</em> zero, i.e.&nbsp;<span class="math inline">\(\lambda_i(\A) \geq 0\)</span>, where <span class="math inline">\(\lambda_i(\A)\)</span> is the <span class="math inline">\(i^\text{th}\)</span> eigen value of <span class="math inline">\(\A\)</span>. We denote positive semi-definite matrices as <span class="math inline">\(\A \ge 0\)</span>. This doesn’t mean element-wise positiveness of the matrix. The space of <span class="math inline">\(n\times n\)</span> positive semi-definite matrices are denoted by <span class="math inline">\(\set{S}^n_+\)</span>.</p></li>
<li><p>Positive definite matrices are symmetric matrices whose eigen values are <em>strictly greater</em> than zero. i.e.&nbsp;<span class="math inline">\(\lambda_i(\A) &gt; 0\)</span>. We denote positive definite matrices as <span class="math inline">\(\A &gt; 0\)</span>. This doesn’t mean element-wise positiveness of the matrix. The space of <span class="math inline">\(n\times n\)</span> positive definite matrices are denoted by <span class="math inline">\(\set{S}^n_{++}\)</span>.</p></li>
<li><p>For two matrices <span class="math inline">\(\X\in\set{S}^n\)</span> and <span class="math inline">\(\Y\in\set{S}^n\)</span>, the matrix inequality <span class="math inline">\(\X \geq \Y\)</span> means <span class="math inline">\(\Z :=\X-\Y\)</span> is postive semi-definite, i.e.&nbsp;<span class="math inline">\(\Z \geq 0\)</span> or <span class="math inline">\(\lambda_i(\Z) \geq 0\)</span>.</p></li>
</ul>
<p>Here is a Python code generating a positive definite matrix.</p>
<pre class="{python}"><code>import numpy as np

# Specify the size of the positive definite matrix (e.g., 3x3)
matrix_size = 3

# Generate a random symmetric matrix
random_matrix = np.random.rand(matrix_size, matrix_size)
symmetric_matrix = (random_matrix + random_matrix.T) / 2

# Ensure the matrix has positive eigenvalues
eigenvalues = np.linalg.eigvals(symmetric_matrix)
while not all(eigenvalues &gt; 0):
    random_matrix = np.random.rand(matrix_size, matrix_size)
    symmetric_matrix = (random_matrix + random_matrix.T) / 2
    eigenvalues = np.linalg.eigvals(symmetric_matrix)

# Print the positive definite matrix
print("Positive Definite Matrix:")
print(symmetric_matrix)

# Compute the eigenvalues
eigenvalues = np.linalg.eigvals(symmetric_matrix)

# All eigen values should be real and positive
print("Eigen values:", eigenvalues)
</code></pre>
</section>
<section id="identity-matrix" class="level3" data-number="46.2.10">
<h3 data-number="46.2.10" class="anchored" data-anchor-id="identity-matrix"><span class="header-section-number">46.2.10</span> Identity Matrix</h3>
<p>An <span class="math inline">\(n\times n\)</span> identity matrix is denoted by <span class="math inline">\(\vo{I}_n\)</span>, and is a matrix with all off-diagonal elements equal to zero, and all diagonal elements equal to one. For example, a <span class="math inline">\(3\times 3\)</span> identity matrix is defined as <span class="math display">\[
\vo{I}_3 = \begin{bmatrix}1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.
\]</span> For an <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\vo{M}\)</span>, the following are true: <span class="math display">\[
\vo{M}\vo{I}_n = \vo{I}_{m}\vo{M} = \vo{M}.
\]</span></p>
<p>Here is a Python code computing the identity matrix.</p>
<pre class="{python}"><code>import numpy as np

# Specify the size of the identity matrix (e.g., a 3x3 identity matrix)
matrix_size = 3

# Compute the identity matrix of the specified size
identity_matrix = np.eye(matrix_size)

# Print the identity matrix
print("Identity Matrix:")
print(identity_matrix)
</code></pre>
</section>
<section id="determininant-of-a-matrix" class="level3" data-number="46.2.11">
<h3 data-number="46.2.11" class="anchored" data-anchor-id="determininant-of-a-matrix"><span class="header-section-number">46.2.11</span> Determininant of a Matrix</h3>
<p>The determinant of a square matrix <span class="math inline">\(\A\)</span> is denoted by <span class="math inline">\(|\A|\)</span>.</p>
<p>For a <span class="math inline">\(2\times 2\)</span> matrix <span class="math display">\[
\A = \begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix},
\]</span> the determinant is defined by <span class="math display">\[|\A| = ad-bc.\]</span></p>
<p>For a <span class="math inline">\(3\times 3\)</span> matrix, <span class="math display">\[\B = \begin{bmatrix}a &amp; b &amp; c \\ d&amp; e&amp; f\\ g &amp; h &amp; i \end{bmatrix},\]</span> the determinant is defined by <span class="math display">\[
|\B| = a\left|\begin{matrix}e&amp;f\\h&amp;i\end{matrix}\right| - b\left|\begin{matrix}d&amp;f\\g&amp;i\end{matrix}\right| + c\left|\begin{matrix}d&amp;e\\g&amp;h\end{matrix}\right| = a(ei-hf) -b(di-gf) + c(dh-ge).
\]</span></p>
<p>Here is a Python code computing matrix determinant.</p>
<pre class="{python}"><code>import numpy as np

# Define a square matrix
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

# Compute the determinant
determinant = np.linalg.det(matrix)

# Print the original matrix and its determinant
print("Matrix:")
print(matrix)

print(f"Determinant: {determinant}")
</code></pre>
</section>
<section id="matrix-inverse" class="level3" data-number="46.2.12">
<h3 data-number="46.2.12" class="anchored" data-anchor-id="matrix-inverse"><span class="header-section-number">46.2.12</span> Matrix Inverse</h3>
<p>Matrix inverse is defined only for <em>square</em> matrices and is denoted by <span class="math inline">\(\vo{M}^{-1}\)</span>. For an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(\vo{M}\)</span>, <span class="math inline">\(\vo{M}^{-1}\)</span> satisfies the following condition <span class="math display">\[
\vo{M}\vo{M}^{-1} = \vo{M}^{-1}\vo{M} = \vo{I}_n.
\]</span></p>
<p><em>Note:</em> Matrix inverse exists only when the determinant is non zero.</p>
<p>For a <span class="math inline">\(2\times 2\)</span> matrix <span class="math display">\[
\A = \begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix},
\]</span> the inverse is given by <span class="math display">\[\A^{-1} = \frac{1}{|\A|}\begin{bmatrix}a&amp;-c\\-b&amp;d\end{bmatrix},\]</span> assuming <span class="math inline">\(|\A|\neq 0\)</span>.</p>
<p><span class="math display">\[\begin{align*}
&amp; (\A^{-1})^{-1} = \A,\\
&amp;(\alpha\A)^{-1} = \frac{1}{\alpha}\A^{-1}, \text{ for $\alpha \neq 0$},\\
&amp;(\A^T)^{-1} = (\A^{-1})^{T},\\
&amp;(\A\B)^{-1} = \B^{-1}\A^{-1}, \text{ assuming $\A$ and $\B$ are invertible};\\
&amp;|\A^{-1}| = \frac{1}{|\A|}.
\end{align*}\]</span></p>
<p>Here is a Python code computing matrix inverse.</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix
matrix = np.array([[1, 2],
                   [3, 4]])

# Compute the inverse
matrix_inverse = np.linalg.inv(matrix)

# Print the original matrix and its inverse
print("Matrix:")
print(matrix)

print("Inverse:")
print(matrix_inverse)</code></pre>
</section>
<section id="moore-penrose-inverse" class="level3" data-number="46.2.13">
<h3 data-number="46.2.13" class="anchored" data-anchor-id="moore-penrose-inverse"><span class="header-section-number">46.2.13</span> Moore-Penrose Inverse</h3>
<p>For a matrix <span class="math inline">\(\A\in\real^{m\times n}\)</span>, the Moore-Penrose inverse (or pseudo-inverse) is denoted by <span class="math inline">\(\A^+\)</span> and defined as <span class="math display">\[
    \A^+ := (\A^T\A)^{-1}A^T
\]</span></p>
<p>Some properties of <span class="math inline">\(\A^+\)</span> include:</p>
<ol type="1">
<li>For any matrix <span class="math inline">\(\A\)</span> there is one and only one pseudoinverse <span class="math inline">\(\A^+\)</span>.</li>
<li>If <span class="math inline">\(\A\)</span> is invertible, then <span class="math inline">\(\A^{-1} = \A^+\)</span></li>
<li>For <span class="math inline">\(\A\in\real^{m\times n}\)</span>, <em>left inverse</em> is defined as <span class="math inline">\(\A^+ = (\A^T\A)^{-1}\A^T\)</span>, and <span class="math inline">\(\A^+\A= \I_n\)</span>.</li>
<li>For <span class="math inline">\(\A\in\real^{m\times n}\)</span>, <em>right inverse</em> is defined as <span class="math inline">\(\A^+ = \A^T(\A^T\A)^{-1}\)</span>, and <span class="math inline">\(\A\A^+= \I_m\)</span>.</li>
</ol>
<p>Here is a Python code computing Moore-Penrose (or pseudo) inverse</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix
matrix = np.array([[1, 2],
                   [3, 4]])

# Compute the pseudo-inverse
pseudo_inverse = np.linalg.pinv(matrix)

# Print the original matrix and its pseudo-inverse
print("Matrix:")
print(matrix)

print("Pseudo-Inverse:")
print(pseudo_inverse)
</code></pre>
</section>
<section id="orthonormal-matrices" class="level3" data-number="46.2.14">
<h3 data-number="46.2.14" class="anchored" data-anchor-id="orthonormal-matrices"><span class="header-section-number">46.2.14</span> Orthonormal Matrices</h3>
<p>An <span class="math inline">\(n\times n\)</span> square matrix satisfying the condition <span class="math inline">\(\A\A^T = \vo{I}_n\)</span> is <em>orthogonal</em> or <em>orthonormal</em>. Also, if <span class="math inline">\(\A\A^T = \vo{I}_n\)</span>, then <span class="math inline">\(\A^T\A = \vo{I}_n\)</span>.</p>
<p>Here is a Python code checking if a matrix is orthogonal.</p>
<pre class="{python}"><code>import numpy as np

# Define a square matrix 
matrix = np.array([[1, 0],
                   [0, -1]])

# Check if the matrix is square
if matrix.shape[0] == matrix.shape[1]:
    # Compute the product of the matrix and its transpose
    product_matrix_transpose = np.dot(matrix, matrix.T)
    
    # Check if the product is close to the identity matrix within a tolerance
    is_orthonormal = np.allclose(product_matrix_transpose, np.eye(matrix.shape[0]))
    
    if is_orthonormal:
        print("The matrix is orthonormal.")
    else:
        print("The matrix is not orthonormal.")
else:
    print("The matrix is not square. Orthonormality is not defined.")</code></pre>
</section>
<section id="kronecker-product" class="level3" data-number="46.2.15">
<h3 data-number="46.2.15" class="anchored" data-anchor-id="kronecker-product"><span class="header-section-number">46.2.15</span> Kronecker Product</h3>
<p>Kronecker product between two matrices <span class="math inline">\(\A\in\real^{p\times q}\)</span> and <span class="math inline">\(\B\in\real^{m\times n}\)</span> results in a new matrix <span class="math inline">\(\C\in\real^{pm\times qn}\)</span> defined by <span class="math display">\[
\C := \A\otimes\B = \begin{bmatrix}a_{11}\B &amp; a_{12}\B &amp; \cdots &amp; a_{1n}\B\\
a_{21}\B &amp; a_{22}\B &amp; \cdots &amp; a_{2n}\B\\
\vdots   &amp; \vdots&amp; &amp; \vdots \\
a_{m1}\B &amp; a_{m2}\B &amp; \cdots &amp; a_{mn}\B
\end{bmatrix}.
\]</span></p>
<p>Kronecker products have the following properties</p>
<ol type="1">
<li><span class="math inline">\(\A\otimes(\B+\C) = \A\otimes\B + \A\otimes\C\)</span></li>
<li><span class="math inline">\((\B+\C)\otimes\A = \B\otimes\A + \C\otimes\A\)</span></li>
<li><span class="math inline">\((\alpha\A)\otimes\B = \A\otimes(\alpha\B) = \alpha(\A\otimes\B)\)</span></li>
<li><span class="math inline">\((\A\otimes\B)\otimes\C = \A\otimes(\B\otimes\C)\)</span></li>
<li><span class="math inline">\(\A\otimes\vo{0} = \vo{0}\otimes\A = \vo{0}\)</span></li>
<li><span class="math inline">\((\A\otimes\B)(\C\otimes\D) = (\A\C)\otimes(\B\D)\)</span></li>
<li><span class="math inline">\((\A\otimes\B)^{-1} = \A^{-1}\otimes\B^{-1}\)</span></li>
<li><span class="math inline">\((\A\otimes\B)^{+} = \A^{+}\otimes\B^{+}\)</span></li>
<li><span class="math inline">\((\A\otimes\B)^T = \A^T\otimes\B^T\)</span></li>
<li><span class="math inline">\(\det{\A\otimes\B} = \det{\A}^m\det{\B}^n\)</span>, for <span class="math inline">\(\A\in\real^{n\times n}\)</span> and <span class="math inline">\(\B\in\real^{m\times m}\)</span>.</li>
<li>For <span class="math inline">\(\A\X\B = \C\)</span>, <span class="math display">\[\vec{\C} = \vec{\A\X\B} = (\B^T\otimes\A)\vec{\X},\]</span> where <span class="math inline">\(\vec{\cdot}\)</span> is the vectorization operator that vertically stacks the columns of a matrix.</li>
<li><span class="math inline">\(\rank{\A\otimes\B} = \rank{\A}\rank{\B}\)</span></li>
</ol>
<p>Here is a Python code computing Kronecker product.</p>
<pre class="{python}"><code>import numpy as np

# Define two matrices
matrix_A = np.array([[1, 2],
                     [3, 4]])

matrix_B = np.array([[0, 5],
                     [6, 7]])

# Compute the Kronecker product
kron_product = np.kron(matrix_A, matrix_B)

# Print the original matrices and the Kronecker product
print("Matrix A:")
print(matrix_A)

print("Matrix B:")
print(matrix_B)

print("Kronecker Product:")
print(kron_product)
</code></pre>
</section>
<section id="trace-of-a-square-matrix" class="level3" data-number="46.2.16">
<h3 data-number="46.2.16" class="anchored" data-anchor-id="trace-of-a-square-matrix"><span class="header-section-number">46.2.16</span> Trace of a Square Matrix</h3>
<p>For <span class="math inline">\(\A\in\set{R}^{n\times n}\)</span>, the trace of the matrix is denoted by <span class="math inline">\(\tr{\A}\)</span> and is defined as the sum of the diagonal terms, i.e. <span class="math display">\[
\tr{\A} = \sum_i^n a_{ii}.
\]</span></p>
<p>The trace operator has the following properties:</p>
<ol type="1">
<li>It is a linear operator, i.e. <span class="math display">\[\tr{\A+\B} = \tr{\A} + \tr{\B}.\]</span></li>
<li>Trace of a scalar-matrix product <span class="math display">\[\tr{\alpha \A} = \alpha\tr{\A}.\]</span></li>
<li>Trace of a matrix-matrix product between compatible matrices <span class="math display">\[\tr{\A^T\B} = \tr{A\B^T} = \tr{\B^T\A} = \tr{\B\A^T} = \sum_{i=1}^m\sum_{j=1}^n a_{ij}b_{ij}.\]</span></li>
<li>If <span class="math inline">\(\A\)</span> and <span class="math inline">\(\B\)</span> are positive semi-definite matrices of the same size, then the following inequality holds <span class="math display">\[
0 \leq \tr{\A\B}^2 \leq \tr{\A^2}\tr{\B^2}\leq \tr{\A}^2\tr{\B}^2.
\]</span></li>
<li>Trace of Kronecker Product <span class="math display">\[\tr{\A\otimes\B} = \tr{\A}\tr{\B}.\]</span></li>
<li>Trace and Eigen Values <span class="math display">\[\tr{A} = \sum_{i=1}^n\lambda_i(\A).\]</span></li>
</ol>
<p>Here is a Python code computing trace of a matrix.</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix as a 2D NumPy array
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

# Compute the trace of the matrix
trace = np.trace(matrix)

# Print the matrix and its trace
print("Matrix:")
print(matrix)
print(f"Trace: {trace}")</code></pre>
</section>
<section id="matrix-inner-product" class="level3" data-number="46.2.17">
<h3 data-number="46.2.17" class="anchored" data-anchor-id="matrix-inner-product"><span class="header-section-number">46.2.17</span> Matrix Inner Product</h3>
<p>The Frobenius inner product between two matrices is defined as <span class="math display">\[
\langle \A,\B \rangle := \tr{\A^T\B}.
\]</span></p>
<p>Here is a Python code checking if two matrices are orthogonal.</p>
<pre class="{python}"><code>import numpy as np

# Define two square matrices 
matrix_A = np.array([[1, 0],
                     [0, -1]])

matrix_B = np.array([[0, 1],
                     [-1, 0]])

product_AB_transpose = np.dot(matrix_A, matrix_B.T)

# If trace(A'*B) = 0 , A and B are orthogonal. 
print("trace(A'*B) =",np.trace(np.dot(matrix_A.T, matrix_B)))

if np.allclose(np.trace(np.dot(matrix_A.T, matrix_B)),0):
  print("Matrix A and Matrix B are orthogonal")
else:
  print("Matrix A and Matrix B are not orthogonal")   
</code></pre>
</section>
<section id="matrix-norms" class="level3" data-number="46.2.18">
<h3 data-number="46.2.18" class="anchored" data-anchor-id="matrix-norms"><span class="header-section-number">46.2.18</span> Matrix Norms</h3>
<p>For a matrix <span class="math inline">\(\A\in\real^{m\times n}\)</span> defined by elements <span class="math inline">\(a_{ij}\)</span>,</p>
<ul>
<li><span class="math inline">\(\|\A\|_1 := \max_{1\leq j \leq n} \sum_{i=1}^m |a_{ij}|\)</span>, which is the maximum absolution <em>column sum</em> of the matrix.</li>
<li><span class="math inline">\(\|\A\|_2 := \sqrt{\lambda_\text{max}(\A^T\A)} = \sigma_\text{max}(\A)\)</span>, where <span class="math inline">\(\sigma_\text{max}(\A)\)</span> is the largest singular value of <span class="math inline">\(\A\)</span>.</li>
<li><span class="math inline">\(\|\A\|_\infty := \max_{1\leq i \leq m} \sum_{j=1}^n |a_{ij}|\)</span>, which is the maximum absolution <em>row sum</em> of the matrix.</li>
</ul>
<p>Here is a Python code computing matrix norms.</p>
<pre class="{python}"><code>import numpy as np
from numpy.linalg import norm

# Define a matrix
A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# Compute different norms
frobenius_norm = norm(A, 'fro') # Frobenius norm
l1_norm = norm(A, 1)           # L1 norm (maximum column sum)
l2_norm = norm(A, 2)           # L2 norm (largest singular value)
max_norm = norm(A, np.inf)     # Max (or infinity) norm (maximum row sum)

# Display the results
print("Matrix A:\n", A)
print("Frobenius Norm of A:", frobenius_norm)
print("L1 Norm of A:", l1_norm)
print("L2 Norm of A:", l2_norm)
print("Max Norm of A:", max_norm)
</code></pre>
</section>
<section id="singular-values-of-matrix" class="level3" data-number="46.2.19">
<h3 data-number="46.2.19" class="anchored" data-anchor-id="singular-values-of-matrix"><span class="header-section-number">46.2.19</span> Singular Values of Matrix</h3>
<p>Given a matrix <span class="math inline">\(\A\)</span> of size <span class="math inline">\(m \times n\)</span>, its singular values are defined as the square roots of the eigenvalues of the matrix <span class="math inline">\(\A^T\A\)</span> (or <span class="math inline">\(\A\A^T\)</span>), i.e. <span class="math display">\[
\sigma_i(\A) := \sqrt{\lambda_i(\A^T\A)}.
\]</span></p>
<p>Singular values provide insight into the geometric properties of the matrix, such as its rank, range, and null space. They are always non-negative real numbers and are usually presented in descending order.</p>
<p>The singular values tell us about the ‘stretching’ effect of the matrix transformation in various directions. In the context of SVD, a matrix <span class="math inline">\(\A\)</span> can be decomposed as <span class="math inline">\(\A = \vo{U}\vo{\Sigma} \vo{V}^T\)</span>, where <span class="math inline">\(\vo{U}\)</span> and <span class="math inline">\(\vo{V}\)</span> are orthogonal matrices, and <span class="math inline">\(\vo{\Sigma}\)</span> is a diagonal matrix whose diagonal entries are the singular values of <span class="math inline">\(\A\)</span>.</p>
<p>Here’s a Python script using NumPy to compute the singular values of a given matrix:</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix
A = np.array([[1, 2], [3, 4], [5, 6]])

# Compute the singular values
U, singular_values, Vt = np.linalg.svd(A)

# Display the singular values
print("Singular Values of A:\n", singular_values)</code></pre>
<p>Since <span class="math inline">\(\A^T\A\)</span> (or <span class="math inline">\(\A\A^T\)</span>) is positive semi-definite, its eigen values will be real and non zero. The largest and smallest singular values of a matrix hold significant information about the properties and behavior of the matrix in various mathematical and practical contexts. Understanding their importance is crucial in linear algebra, data analysis, and machine learning.</p>
<p>The largest singular value has several important implications, such as:</p>
<ol type="1">
<li><p><strong>Indicates Maximum Stretch</strong>: The largest singular value of a matrix represents the maximum “stretching” factor of the matrix. It gives the largest factor by which the matrix can stretch a vector during the transformation.</p></li>
<li><p><strong>Norm of the Matrix</strong>: The largest singular value is equal to the 2-norm (or spectral norm) of the matrix. This norm represents the maximum length to which the matrix can stretch a unit vector.</p></li>
<li><p><strong>Stability and Conditioning</strong>: In numerical linear algebra, the largest singular value is used to assess the conditioning of the matrix. A very large singular value, especially when compared to the smallest singular value, can indicate that the matrix is ill-conditioned, leading to numerical instability in solutions of linear systems involving the matrix.</p></li>
</ol>
<p>The smallest singular value also has several important implications, such as:</p>
<ol type="1">
<li><p><strong>Indicates Minimum Stretch and Rank</strong>: The smallest singular value provides insight into how much the matrix compresses vectors in the least stretchable direction. If the smallest singular value is zero, it indicates that the matrix is rank-deficient (not full rank), meaning there are linearly dependent columns or rows.</p></li>
<li><p><strong>Measure of Invertibility</strong>: For a square matrix, if the smallest singular value is significantly greater than zero, the matrix is well-conditioned and invertible. A very small (especially zero) singular value in a square matrix implies that the matrix is near singular or singular, and thus not invertible.</p></li>
<li><p><strong>Pseudo-Inverse and Regularization</strong>: In machine learning and statistics, the smallest singular value plays a critical role in regularization and the computation of the Moore-Penrose pseudo-inverse. When dealing with ill-posed problems or in the presence of collinearity, small singular values are regularized to stabilize the solution.</p></li>
<li><p><strong>Data Analysis and Principal Component Analysis (PCA)</strong>: In PCA, small singular values (and their corresponding singular vectors) often correspond to noise or less significant components of the data. Eliminating these components (dimensionality reduction) can help in focusing on the more significant patterns represented by larger singular values.</p></li>
</ol>
<p>In summary, the largest singular value reflects the maximum stretching ability and overall norm of a matrix, while the smallest singular value indicates its invertibility, conditioning, and the presence of redundant or less significant components. In practical applications, these singular values are crucial for understanding the stability, efficiency, and effectiveness of numerical methods and data analysis techniques.</p>
</section>
<section id="matrix-decompositions" class="level3" data-number="46.2.20">
<h3 data-number="46.2.20" class="anchored" data-anchor-id="matrix-decompositions"><span class="header-section-number">46.2.20</span> Matrix Decompositions</h3>
<section id="lu-decomposition" class="level4" data-number="46.2.20.1">
<h4 data-number="46.2.20.1" class="anchored" data-anchor-id="lu-decomposition"><span class="header-section-number">46.2.20.1</span> LU Decomposition</h4>
<p>LU decomposition is a method of decomposing a square matrix <span class="math inline">\(\A\)</span> into the product of a lower triangular matrix <span class="math inline">\(\vo{L}\)</span> and an upper triangular matrix <span class="math inline">\(\vo{U}\)</span>. In mathematical terms, if <span class="math inline">\(\A\)</span> is a square matrix, then the LU decomposition is given by <span class="math inline">\(\A = \vo{LU}\)</span>, where</p>
<ul>
<li><span class="math inline">\(\vo{L}\)</span> is a lower triangular matrix (all entries above the main diagonal are zero),</li>
<li><span class="math inline">\(\vo{U}\)</span> is an upper triangular matrix (all entries below the main diagonal are zero).</li>
</ul>
<p>LU decomposition is used in various applications such as:</p>
<ol type="a">
<li><p><strong>Solving Linear Systems</strong>: It is used to solve systems of linear equations. Once a matrix is decomposed into <span class="math inline">\(\vo{L}\)</span> and <span class="math inline">\(\vo{U}\)</span>, it is easier to solve the equation <span class="math inline">\(\vo{Ax = b}\)</span> by first solving <span class="math inline">\(\vo{Ly = b}\)</span> for <span class="math inline">\(\vo{y}\)</span> and then solving <span class="math inline">\(\vo{Ux = y}\)</span> for <span class="math inline">\(\vo{x}\)</span>.</p></li>
<li><p><strong>Matrix Inversion</strong>: LU decomposition can be used to find the inverse of a matrix, which is significantly faster than direct computation, especially for large matrices.</p></li>
<li><p><strong>Determinant Calculation</strong>: The determinant of <span class="math inline">\(\A\)</span> can be easily calculated as the product of the diagonals of <span class="math inline">\(\vo{L}\)</span> and <span class="math inline">\(\vo{U}\)</span>, as the determinant of a triangular matrix is the product of its diagonal entries.</p></li>
</ol>
<p>Here is a Python code computing LU Decomposition of a matrix.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import lu

# Define a square matrix
A = np.array([[4, 3], [6, 3]])

# Perform LU Decomposition
P, L, U = lu(A)

# Display the results
print("Original Matrix:\n", A)
print("Permutation Matrix (P):\n", P)
print("Lower Triangular Matrix (L):\n", L)
print("Upper Triangular Matrix (U):\n", U)

# Verify the decomposition
print("Verification (P * L * U):\n", np.dot(P, np.dot(L, U)))</code></pre>
</section>
<section id="qr-factorization-of-a-matrix" class="level4" data-number="46.2.20.2">
<h4 data-number="46.2.20.2" class="anchored" data-anchor-id="qr-factorization-of-a-matrix"><span class="header-section-number">46.2.20.2</span> QR Factorization of a Matrix</h4>
<p>QR factorization is a method for decomposing a matrix into two components: an orthogonal matrix <span class="math inline">\(\vo{Q}\)</span> and an upper triangular matrix <span class="math inline">\(\vo{R}\)</span>. For a given matrix <span class="math inline">\(\A\)</span>, the QR factorization is expressed as <span class="math inline">\(\A = \vo{Q}\vo{R}\)</span>, where:</p>
<ul>
<li><span class="math inline">\(\vo{Q}\)</span> is an orthogonal matrix, i.e., <span class="math inline">\(\vo{Q}^T \vo{Q} = \vo{Q} \vo{Q}^T = \vo{I}\)</span>, where <span class="math inline">\(\vo{I}\)</span> is the identity matrix</li>
<li><span class="math inline">\(\vo{R}\)</span> is an upper triangular matrix.</li>
</ul>
<p>QR factorization is utilized in various mathematical and computational applications, such as:</p>
<ol type="1">
<li><strong>Solving Linear Systems</strong>: Similar to LU decomposition, QR factorization can be used to solve linear systems <span class="math inline">\(\vo{A}\vo{x} = \vo{b}\)</span>. The system is solved by first computing <span class="math inline">\(\vo{Q}^T \vo{b}\)</span> and then solving the upper triangular system <span class="math inline">\(\vo{R}\vo{x} = \vo{Q}^T \vo{b}\)</span>.</li>
<li><strong>Eigenvalue Computation</strong>: QR factorization is a key component in algorithms for computing eigenvalues of a matrix, particularly in the QR algorithm for eigenvalue computation.</li>
<li><strong>Least Squares Fitting</strong>: In statistical analysis and data fitting, QR factorization is often used to solve least squares problems efficiently, especially when <span class="math inline">\(\A\)</span> is not square or is ill-conditioned.</li>
</ol>
<p>Here is a Python code computing QR Decomposition of a matrix.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import qr

# Define a matrix
A = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]])

# Perform QR Decomposition
Q, R = qr(A)

# Display the results
print("Original Matrix:\n", A)
print("Orthogonal Matrix (Q):\n", Q)
print("Upper Triangular Matrix (R):\n", R)

# Verify the decomposition
print("Verification (Q * R):\n", np.dot(Q, R))</code></pre>
</section>
<section id="cholesky-factorization-of-a-matrix" class="level4" data-number="46.2.20.3">
<h4 data-number="46.2.20.3" class="anchored" data-anchor-id="cholesky-factorization-of-a-matrix"><span class="header-section-number">46.2.20.3</span> Cholesky Factorization of a Matrix</h4>
<p>Cholesky factorization is a method for decomposing a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose. For a given matrix <span class="math inline">\(\A\)</span>, the Cholesky factorization is expressed as: <span class="math inline">\(\A = \vo{L}\vo{L}^T\)</span>, where <span class="math inline">\(\vo{L}\)</span> is a lower triangular matrix.</p>
<p>Cholesky factorization is used in various applications, including:</p>
<ol type="1">
<li><p><strong>Solving Linear Systems</strong>: Cholesky factorization is particularly efficient for solving linear systems of the form <span class="math inline">\(\vo{A}\vo{x} = \vo{b}\)</span> when <span class="math inline">\(\vo{A}\)</span> is Hermitian and positive-definite. The system can be solved by first solving <span class="math inline">\(\vo{L}\vo{y} = \vo{b}\)</span> for <span class="math inline">\(\vo{y}\)</span>, and then solving <span class="math inline">\(\vo{L}^T \vo{x} = \vo{y}\)</span>.</p></li>
<li><p><strong>Computing Matrix Inverses</strong>: For Hermitian, positive-definite matrices, Cholesky factorization can be used to compute the inverse of <span class="math inline">\(\vo{A}\)</span> efficiently.</p></li>
<li><p><strong>Monte Carlo Simulations</strong>: In financial and engineering simulations, Cholesky factorization is used to generate random samples from multivariate normal distributions.</p></li>
<li><p><strong>Optimization Problems</strong>: In optimization, especially quadratic programming, Cholesky factorization is used to solve linear equations and invert matrices efficiently, which is often a computational bottleneck.</p></li>
</ol>
<p>Here is a Python code computing Cholesky Decomposition of a matrix.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import cholesky

# Define a positive definite matrix
A = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])

# Perform Cholesky Decomposition
L = cholesky(A, lower=True)

# Display the results
print("Original Matrix:\n", A)
print("Lower Triangular Matrix (L):\n", L)

# Verify the decomposition
print("Verification (L * \set{L}_T):\n", np.dot(L, L.T))</code></pre>
</section>
<section id="matrix-square-root" class="level4" data-number="46.2.20.4">
<h4 data-number="46.2.20.4" class="anchored" data-anchor-id="matrix-square-root"><span class="header-section-number">46.2.20.4</span> Matrix Square Root</h4>
<p>The matrix square root of a matrix <span class="math inline">\(\vo{A}\)</span> is a matrix <span class="math inline">\(\vo{B}\)</span> such that <span class="math inline">\(\vo{B}\vo{B} = \vo{A}\)</span>. This means that when <span class="math inline">\(\vo{B}\)</span> is multiplied by itself, the result is the original matrix <span class="math inline">\(\vo{A}\)</span>. The matrix square root is particularly relevant for positive <em>definite</em> matrices.</p>
<p>Matrix square roots are used in various applications, including:</p>
<ol type="1">
<li><p><strong>Control Theory</strong>: In control theory, the matrix square root is used in the analysis and design of control systems, particularly in the context of state transition matrices and system stability.</p></li>
<li><p><strong>Quantum Mechanics</strong>: In quantum mechanics, the square root of a density matrix (a matrix representing a quantum state) is often calculated as part of quantum state tomography and other analyses.</p></li>
<li><p><strong>Computer Graphics</strong>: In computer graphics, matrix square roots are used in algorithms for transformations and in the manipulation of geometric shapes.</p></li>
<li><p><strong>Statistics and Data Analysis</strong>: In statistics, the matrix square root (particularly the square root of a covariance matrix) is used in multivariate analysis, including principal component analysis (PCA) and other forms of data reduction.</p></li>
</ol>
<p>These applications leverage the matrix square root for its ability to simplify and solve complex mathematical problems, particularly in systems analysis and probabilistic modeling.</p>
<p>To compute the square root of a matrix in Python, you can use the <code>scipy.linalg</code> library, which provides a function for this purpose. Below is a Python code using SciPy to compute the square root of a given matrix.</p>
<p>Here is a Python code computng matrix square root.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import sqrtm

# Define your matrix A (it should be a positive definite matrix)
A = np.array([[4, 2], [2, 3]])

# Compute the square root of the matrix
sqrt_A = sqrtm(A)

# Verify the result
print("Square Root of A:\n", sqrt_A)
print("Verification (Square Root of A multiplied by itself):\n", np.dot(sqrt_A, sqrt_A))</code></pre>
</section>
<section id="singular-value-decomposition" class="level4" data-number="46.2.20.5">
<h4 data-number="46.2.20.5" class="anchored" data-anchor-id="singular-value-decomposition"><span class="header-section-number">46.2.20.5</span> Singular Value Decomposition</h4>
<p>Singular Value Decomposition (SVD) is a key matrix decomposition technique in linear algebra, used in many scientific and engineering fields. It decomposes any <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\A\)</span> into three matrices <span class="math inline">\(\vo{A} = \vo{U} \vo{\Sigma} \vo{V}^T\)</span>, where</p>
<ol type="1">
<li><p><span class="math inline">\(\vo{U}\)</span>: An <span class="math inline">\(m\times m\)</span> unitary matrix. The columns of <span class="math inline">\(\vo{U}\)</span>, known as the left singular vectors, form an orthonormal basis for the range of <span class="math inline">\(\A\)</span>.</p></li>
<li><p><span class="math inline">\(\vo{\Sigma}\)</span>: An <span class="math inline">\(m \times n\)</span> rectangular diagonal matrix with non-negative real numbers on the diagonal. The diagonal entries, the singular values of <span class="math inline">\(\A\)</span>, are typically arranged in descending order. They represent the lengths of the semi-axes of the ellipsoid described by <span class="math inline">\(\A\)</span>. The number of non-zero singular values equals the rank of <span class="math inline">\(\A\)</span>.</p></li>
<li><p><span class="math inline">\(\vo{V}^T\)</span>: An <span class="math inline">\(n\times n\)</span> unitary matrix. The columns of <span class="math inline">\(\vo{V}\)</span>, the right singular vectors, form an orthonormal basis for the domain of <span class="math inline">\(\A\)</span>.</p></li>
</ol>
<p>Key Properties and Uses of SVD are the following:</p>
<ul>
<li><p>SVD simplifies the analysis of a matrix by breaking it down into simpler constituent parts.</p></li>
<li><p>It is instrumental in solving overdetermined or underdetermined linear systems, via computing the pseudo-inverse of a matrix.</p></li>
<li><p>In data science and machine learning, SVD is crucial for algorithms like Principal Component Analysis (PCA), used in dimensionality reduction, data compression, and noise reduction.</p></li>
<li><p>It plays a significant role in signal processing and statistics for pattern identification and compact representations.</p></li>
<li><p>SVD is also used in solving linear least squares problems, common in mathematical modeling and scientific computing.</p></li>
</ul>
<p>SVD’s versatility and robustness make it an essential tool in computational and applied mathematics.</p>
<p>Here is a Python code computing SVD of a matrix.</p>
<pre class="{python}"><code>import numpy as np

# Define a matrix
A = np.array([[1, 2], [3, 4], [5, 6]])

# Perform Singular Value Decomposition
U, S, Vt = np.linalg.svd(A)

# Display the results
print("Original Matrix:\n", A)
print("Left Singular Vectors (U):\n", U)
print("Singular Values (S):\n", S)
print("Right Singular Vectors Transposed (Vt):\n", Vt)

# Reconstruct the original matrix
Sigma = np.zeros((A.shape[0], A.shape[1]))
Sigma[:A.shape[1], :A.shape[1]] = np.diag(S)
A_reconstructed = U @ Sigma @ Vt

print("Reconstructed Matrix (U * Sigma * Vt):\n", A_reconstructed)</code></pre>
</section>
<section id="schur-complement" class="level4" data-number="46.2.20.6">
<h4 data-number="46.2.20.6" class="anchored" data-anchor-id="schur-complement"><span class="header-section-number">46.2.20.6</span> Schur Complement</h4>
<p>The Schur complement is a useful concept in linear algebra for analyzing and simplifying block matrices. Given a block matrix: <span class="math display">\[ \vo{M} = \begin{bmatrix} \vo{A} &amp; \vo{B} \\ \vo{C} &amp; \vo{D} \end{bmatrix}, \]</span></p>
<p>where <span class="math inline">\(\A\)</span>, <span class="math inline">\(\B\)</span>, <span class="math inline">\(\C\)</span>, and <span class="math inline">\(\D\)</span> are submatrices with <span class="math inline">\(\A\)</span> and <span class="math inline">\(\D\)</span> being square, and <span class="math inline">\(\A\)</span> is invertible, the Schur complement of <span class="math inline">\(\A\)</span> in <span class="math inline">\(\vo{M}\)</span> is defined as: <span class="math display">\[ \vo{S} = \vo{D} - \vo{C} \vo{A}^{-1} \vo{B}.\]</span></p>
<p>Applications of the Schur Complement include</p>
<ul>
<li><strong>Matrix Inversion</strong>: Simplifies the inversion of block matrices.</li>
<li><strong>Determinant Calculation</strong>: Allows expressing the determinant of <span class="math inline">\(\vo{M}\)</span> in terms of the determinant of <span class="math inline">\(\A\)</span> and its Schur complement.</li>
<li><strong>Solving Linear Systems</strong>: Used in solving partitioned systems of linear equations.</li>
<li><strong>Control Theory</strong>: Applied in stability analysis and controller design.</li>
</ul>
<p>Here is a Python code computing Schur Complement.</p>
<pre class="{python}"><code>import numpy as np

# Define the block matrix components
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = np.array([[9, 10], [11, 12]])
D = np.array([[13, 14], [15, 16]])

# Ensure A is invertible
if np.linalg.det(A) == 0:
    raise ValueError("Matrix A is not invertible.")

# Calculate the Schur complement of A in M
# Schur complement S = D - C * A^-1 * B
A_inv = np.linalg.inv(A)
S = D - np.dot(C, np.dot(A_inv, B))

print("Schur Complement of A in M:\n", S)</code></pre>
</section>
<section id="schur-decomposition" class="level4" data-number="46.2.20.7">
<h4 data-number="46.2.20.7" class="anchored" data-anchor-id="schur-decomposition"><span class="header-section-number">46.2.20.7</span> Schur Decomposition</h4>
<p>Schur Decomposition is a key technique in linear algebra for decomposing square matrices. It represents any square matrix <span class="math inline">\(\A\)</span> as a product of three matrices <span class="math display">\[ \vo{A} = \vo{Q}\vo{U}\vo{Q}^T\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\vo{Q}\)</span> is a unitary matrix, satisfying <span class="math inline">\(\vo{Q}^T\vo{Q} = \vo{I}\)</span>.</li>
<li><span class="math inline">\(\vo{U}\)</span> is an upper triangular matrix.</li>
<li><span class="math inline">\(\vo{Q}^T\)</span> is the transpose of <span class="math inline">\(\vo{Q}\)</span>.</li>
</ul>
<p>Schur Decomposition is used in various applications in numerical linear algebra, including eigenvalue computations and matrix exponentials.</p>
<p>Here is a Python code computing Schur Decomposition of a matrix.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import schur

# Define a square matrix A
A = np.array([[4, -2, 1],
              [7,  0, 5],
              [6,  3, 2]])

# Perform Schur decomposition
T, U = schur(A)

# Print the original matrix and the Schur decomposition matrices
print("Matrix A:")
print(A)

print("Schur Decomposition - T:")
print(T)

print("Schur Decomposition - U:")
print(U)
</code></pre>
</section>
<section id="matrix-exponential" class="level4" data-number="46.2.20.8">
<h4 data-number="46.2.20.8" class="anchored" data-anchor-id="matrix-exponential"><span class="header-section-number">46.2.20.8</span> Matrix Exponential</h4>
<p>The matrix exponential is a significant concept in linear algebra, extending the exponential function to square matrices. For any square matrix <span class="math inline">\(\A\)</span>, the matrix exponential is defined by the series: <span class="math display">\[ e^{\mathbf{A}} = \sum_{k=0}^{\infty} \frac{1}{k!} \mathbf{A}^k.\]</span></p>
<p><em>Note</em>: This is <strong>not</strong> element-wise exponential.</p>
<p>Matrix exponentials are crucial in solving linear differential equations, control theory, and quantum mechanics, among other applications.</p>
<p>Here is a Python code showing the computation of a matrix exponential.</p>
<pre class="{python}"><code>import numpy as np
from scipy.linalg import expm

# Define a square matrix
A = np.array([[0, -1], [1, 0]])

# Compute the matrix exponential of A
exp_A = expm(A)

# Display the result
print("Matrix A:\n", A)
print("Matrix Exponential of A (exp(A)):\n", exp_A)</code></pre>
</section>
</section>
</section>
</section>
<section id="foundational-mathematics" class="level1" data-number="47">
<h1 data-number="47"><span class="header-section-number">47</span> Foundational Mathematics</h1>
<section id="monte-carlo-methods" class="level3" data-number="47.0.1">
<h3 data-number="47.0.1" class="anchored" data-anchor-id="monte-carlo-methods"><span class="header-section-number">47.0.1</span> Monte Carlo Methods*</h3>
<ul>
<li><strong>Monte Carlo Simulation</strong>: Estimating integrals and expectations, important in approximation algorithms.</li>
<li><strong>Markov Chain Monte Carlo (MCMC)</strong>: Algorithms for sampling from complex distributions. Propagation of distributions through functions.</li>
<li><strong>Python Code</strong>: Simple Monte Carlo integration and examples of MCMC.</li>
</ul>
</section>
</section>
<section id="optimization" class="level1" data-number="48">
<h1 data-number="48"><span class="header-section-number">48</span> Optimization</h1>
<section id="convex-sets" class="level2" data-number="48.1">
<h2 data-number="48.1" class="anchored" data-anchor-id="convex-sets"><span class="header-section-number">48.1</span> Convex Sets</h2>
</section>
<section id="convex-functions" class="level2" data-number="48.2">
<h2 data-number="48.2" class="anchored" data-anchor-id="convex-functions"><span class="header-section-number">48.2</span> Convex Functions</h2>
</section>
<section id="some-useful-inequalities" class="level2" data-number="48.3">
<h2 data-number="48.3" class="anchored" data-anchor-id="some-useful-inequalities"><span class="header-section-number">48.3</span> Some Useful Inequalities</h2>
</section>
</section>
<section id="random-processes-and-sequences" class="level1" data-number="49">
<h1 data-number="49"><span class="header-section-number">49</span> Random Processes and Sequences</h1>
<p>A random process, also known as a stochastic process, is a mathematical concept that describes a collection of random variables ordered in time or space. It is used to model systems that evolve randomly over time or space.</p>
<p>Mathematicaly, a random process is a family of random variables, <span class="math inline">\(\{X(t) : t \in T\}\)</span>, where <span class="math inline">\(t\)</span> represents a parameter often interpreted as time (but can also represent space or other dimensions), and <span class="math inline">\(T\)</span> is the index set (usually a subset of real numbers). Each random variable <span class="math inline">\(X(t)\)</span> in the family has its own probability distribution, and the joint distribution of any finite number of these random variables is specified.</p>
<p>The following are important characteristics of random processes:</p>
<ul>
<li><strong>Time Dependence</strong>: The evolution of the process over time can be deterministic or stochastic.</li>
<li><strong>State Space</strong>: The set of possible values (or states) the random variables can take. It can be discrete (like a set of integers) or continuous (like real numbers).</li>
</ul>
<p>There are several kinds of random processes. Some of the commonly used are summarized next.</p>
<section id="discrete-time-processes" class="level2" data-number="49.1">
<h2 data-number="49.1" class="anchored" data-anchor-id="discrete-time-processes"><span class="header-section-number">49.1</span> Discrete-Time Processes</h2>
<p>Discrete-time processes are a type of stochastic or random process where the set of indices (usually representing time) is discrete. This means that the process is observed or defined only at specific, separated points in time.</p>
<p>Key characteristics of discrete-time processes are</p>
<ul>
<li><strong>Time Index</strong>: In discrete-time processes, the time index, often denoted as <span class="math inline">\(t\)</span>, takes values from a discrete set, like integers. It’s as if the process is observed at distinct time steps (e.g., daily stock prices).</li>
<li><strong>Random Variables</strong>: At each time index, the process is described by a random variable. These variables can be independent or have some form of dependency.</li>
<li><strong>Examples</strong>: Common examples of discrete-time processes include time series in economics, daily weather records, and signal processing data.</li>
</ul>
<p>The following are three important types of discete-time processes:</p>
<section id="bernoulli-process" class="level3" data-number="49.1.1">
<h3 data-number="49.1.1" class="anchored" data-anchor-id="bernoulli-process"><span class="header-section-number">49.1.1</span> Bernoulli Process</h3>
<p>A Bernoulli Process is a fundamental type of discrete-time stochastic process. It is a sequence of independent and identically distributed (i.i.d.) random variables that take values from a binary set, typically {0, 1}, representing two outcomes, commonly termed “success” and “failure.”</p>
<p>Let’s denote the random variables in the sequence as <span class="math inline">\(X_1, X_2, ..., X_n\)</span>. Each <span class="math inline">\(X_i\)</span> is a Bernoulli random variable, which means:</p>
<ul>
<li><span class="math inline">\(X_i\)</span> takes the value 1 with probability <span class="math inline">\(p\)</span> (success).</li>
<li><span class="math inline">\(X_i\)</span> takes the value 0 with probability <span class="math inline">\(1 - p\)</span> (failure).</li>
</ul>
<p>So, mathematically, the probability mass function (PMF) of each <span class="math inline">\(X_i\)</span> is given by:</p>
<p><span class="math display">\[ P(X_i = x) = p^x (1 - p)^{1 - x},\]</span></p>
<p>for <span class="math inline">\(x \in \{0, 1\}\)</span>, where <span class="math inline">\(p\)</span> is the probability of success.</p>
<p>Notably:</p>
<ol type="1">
<li>Each trial (or each random variable in the sequence) is independent of the others.</li>
<li>Each random variable follows the same Bernoulli distribution with the same probability of success <span class="math inline">\(p\)</span>.</li>
<li>The process is defined at discrete time intervals (e.g., coin flips in a series of experiments).</li>
</ol>
<p>Some examples where Bernoulli process is used to model sequences are:</p>
<ul>
<li><strong>Coin Flipping</strong>: A sequence of coin flips where each flip is independent, and the probability of getting heads (success) is <span class="math inline">\(p\)</span>.</li>
<li><strong>Quality Control</strong>: In manufacturing, where each item produced either passes (success) or fails (failure) quality control with some probability.</li>
<li><strong>Binary Data Modeling</strong>: In scenarios where data can be modeled as a sequence of binary outcomes, like click/no-click, buy/not buy in user behavior analysis.</li>
</ul>
<p>The number of successes in a sequence of Bernoulli trials can be modeled by a Binomial distribution. If you conduct <span class="math inline">\(n\)</span> Bernoulli trials (each with success probability <span class="math inline">\(p\)</span>), the total number of successes follows a Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.</p>
<p>A Bernoulli Process is a simple yet powerful model for representing scenarios where events or outcomes are binary and independent, a common situation in many practical applications in statistics and machine learning.</p>
<p>Simulating a Bernoulli process in Python is straightforward and can be done using the numpy library, which offers efficient array operations and random number generation capabilities. Here’s an example of how to simulate a Bernoulli process:</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Sequence of events following a Bernoulli distribution. This models occurence of head or tail from flipping a fair coin."
import numpy as np
import matplotlib.pyplot as plt

def bernoulli_process(p, n):
    """
    Simulate a Bernoulli process.
    
    Parameters:
        p (float): Probability of success for each trial.
        n (int): Number of trials.

    Returns:
        numpy.ndarray: An array representing the Bernoulli process (successes and failures).
    """
    # Each trial results in 1 (success) with probability p and 0 (failure) with probability 1-p
    return np.random.binomial(1, p, n)

# Parameters for the Bernoulli process
probability_of_success = 0.5  # Example probability (like flipping a fair coin)
number_of_trials = 50        # Total number of trials

# Simulate the Bernoulli process
process = bernoulli_process(probability_of_success, number_of_trials)

# Plotting the results
plt.figure(figsize=(10, 4))
plt.stem(process)
plt.title('Bernoulli Process Simulation')
plt.xlabel('Trial')
plt.ylabel('Outcome (0=Failure, 1=Success)')
plt.yticks([0, 1]);
plt.show()</code></pre>
<p>In this script, the <code>bernoulli_process</code> function simulates the Bernoulli process. It uses <code>numpy.random.binomial</code> with 1 trial for each step, which effectively makes it a Bernoulli trial. We define the probability of success (e.g., 0.5 for a fair coin toss) and the number of trials. The function returns an array of 0s and 1s, representing failures and successes, respectively. The resulting Bernoulli process is visualized using a stem plot, where each stem represents the outcome of a single trial. This code demonstrates a basic simulation of a Bernoulli process and is useful for understanding the behavior of binary outcomes over a series of independent trials.</p>
</section>
<section id="random-walk" class="level3" data-number="49.1.2">
<h3 data-number="49.1.2" class="anchored" data-anchor-id="random-walk"><span class="header-section-number">49.1.2</span> Random Walk</h3>
<p>A random walk is a mathematical model that describes a path consisting of a succession of random steps. This model is widely used in various fields such as physics, finance, biology, and computer science.</p>
<p>In its simplest form, a one-dimensional random walk can be defined as follows:</p>
<ol type="1">
<li><p><strong>Initial Position</strong>: Start at a fixed point, usually <span class="math inline">\(x = 0\)</span>.</p></li>
<li><p><strong>Random Steps</strong>: At each time step <span class="math inline">\(t\)</span>, take a step either to the left or to the right. The direction of each step is determined randomly.</p></li>
<li><p><strong>Discrete Steps</strong>: If the step is to the right, the position increases by 1 (i.e., <span class="math inline">\(x_{t+1} = x_t + 1\)</span>). If the step is to the left, the position decreases by 1 (i.e., <span class="math inline">\(x_{t+1} = x_t - 1\)</span>).</p></li>
</ol>
<p>Mathematically, if <span class="math inline">\(S_n\)</span> denotes the position after <span class="math inline">\(n\)</span> steps, and each step <span class="math inline">\(X_i\)</span> is a random variable that takes the value +1 or -1 with equal probability, then:</p>
<p><span class="math display">\[S_n = \sum_{i=1}^n X_i.\]</span></p>
<p>Notably:</p>
<ul>
<li>The steps <span class="math inline">\(X_i\)</span> are independent of each other.</li>
<li>Each step has an equal probability of being +1 or -1.</li>
<li>The process takes place in discrete time steps and discrete spatial steps.</li>
<li>The expected position after <span class="math inline">\(n\)</span> steps is 0, as the walk is symmetric.</li>
<li>The variance after <span class="math inline">\(n\)</span> steps is <span class="math inline">\(n\)</span>, reflecting the increasing uncertainty about the position over time.</li>
<li>The concept extends to higher dimensions, where at each step, the move is made in one of the possible directions in the plane or space.</li>
</ul>
<p>Random walks are used to model many phenomenon. For example:</p>
<ul>
<li><strong>Physics</strong>: Modeling diffusion processes, such as the movement of molecules.</li>
<li><strong>Finance</strong>: Stock price movements are often modeled as random walks.</li>
<li><strong>Biology</strong>: Pathways of motile organisms or molecules within cells.</li>
<li><strong>Computer Science</strong>: Algorithms for searching or optimization.</li>
</ul>
<p>A random walk is a fundamental stochastic process and provides a simple yet powerful model for various phenomena. It is a cornerstone model in the study of stochastic processes and has a profound impact on our understanding of random behavior in natural and artificial systems.</p>
<p>Here’s a Python example that simulates a one-dimensional random walk:</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Simulation of a random walk as random -1s and 1s, representing moves in opposite directions. The position at each step is the cumulative sum of the steps taken up to that point."
import numpy as np
import matplotlib.pyplot as plt

def random_walk(steps):
    # Steps can be -1 or 1
    walk_steps = np.random.choice([-1, 1], size=steps)
    # Cumulative sum to simulate the walk
    walk = np.cumsum(walk_steps)
    return walk

# Number of steps
n_steps = 100

# Simulate a random walk
walk = random_walk(n_steps)

# Plot the random walk
plt.figure(figsize=(10, 6))
plt.plot(walk)
plt.title('1D Random Walk')
plt.xlabel('Steps')
plt.ylabel('Position')
plt.grid(True)
plt.show()</code></pre>
</section>
<section id="markov-chain" class="level3" data-number="49.1.3">
<h3 data-number="49.1.3" class="anchored" data-anchor-id="markov-chain"><span class="header-section-number">49.1.3</span> Markov Chain</h3>
<p>A Markov Chain is a stochastic process that undergoes transitions from one state to another within a finite or countably infinite number of possible states. It is characterized by the property that the future state depends only on the current state, not on the sequence of events that preceded it. This property is known as the Markov property or memorylessness.</p>
<p>Mathematically, a Markov Chain is defined by:</p>
<ol type="1">
<li><strong>States</strong>: A set of states <span class="math inline">\(S = \{s_1, s_2, ..., s_n\}\)</span>.</li>
<li><strong>Transition Probability</strong>: The probability of moving from one state to another. For a state <span class="math inline">\(s_i\)</span> and <span class="math inline">\(s_j\)</span>, the transition probability is denoted as <span class="math inline">\(P_{ij}\)</span>, which is the probability of transitioning from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>.</li>
<li><strong>Transition Matrix</strong>: A matrix <span class="math inline">\(P\)</span> where each element <span class="math inline">\(P_{ij}\)</span> represents the transition probability from state <span class="math inline">\(s_i\)</span> to state <span class="math inline">\(s_j\)</span>. For a Markov chain, the sum of the probabilities in each row of the matrix is 1, i.e., <span class="math inline">\(\sum_{j} P_{ij} = 1\)</span> for all <span class="math inline">\(i\)</span>.</li>
</ol>
<p>They have the following properties:</p>
<ul>
<li><strong>Discrete Time Steps</strong>: The process moves from one state to another at discrete time steps.</li>
<li><strong>Memorylessness</strong>: The next state depends only on the current state, not on the past states.</li>
</ul>
<p>There are three kinds of Markov Chains:</p>
<ul>
<li><strong>Irreducible</strong>: A Markov chain is irreducible if it is possible to get to any state from any state.</li>
<li><strong>Periodic/Aperiodic</strong>: A state has a period if the chain can return to the state only at multiples of some integer. A Markov chain is aperiodic if it has no periodic states.</li>
<li><strong>Transient and Recurrent States</strong>: A state is transient if the chain can leave it and never return; otherwise, it is recurrent.</li>
</ul>
<p>Markov Chains are used in various fields, including economics, genetics, game theory, and computer science.</p>
<p>As an example, consider the following the Markov chain.</p>
<!-- ![A three state Markov chain.](./tikz_figs/markov_chain.png){width=75%} -->
<p>```{r, engine = ‘tikz’} #| fig-cap: “A three state Markov chain.” #| echo: false</p>
<pre><code>

The transition matrix is given by
$$
\vo{M} = \begin{bmatrix}
0.1 &amp; 0.6&amp; 0.3\\
0.4 &amp; 0.2&amp; 0.4\\
0.3&amp; 0.3&amp; 0.4
\end{bmatrix}.
$$
If $\vo{p}(k) = \begin{bmatrix} p_0(k) &amp; p_1(k) &amp; p_2(k) \end{bmatrix}$ is the probability (row) vector at time $k$, defined by the propabilities $p_0(k), p_1(k),$ and $p_2(k)$ that the system is in state $S_0$, $S_1$, and $S_2$ respectively, the probabilities in the next time step is given by
$$
\vo{p}(k+1) = \vo{p}(k)\vo{M}.
$$
We start with $\vo{p}(0)$ and iterate $\vo{p}(k)$ with the above update equation.

Here's a Python script to simulate the Markov Chain shown above:

```{python, results='show'}
import numpy as np
M = np.array([[0.1, 0.6, 0.3],
              [0.4, 0.2, 0.4],
              [0.3, 0.3, 0.4]])

p = np.array([1.,0.,0.])
np.set_printoptions(formatter={'float': '{: 0.3f}'.format})

for iter in range(11):
  print(f"k={iter}, p={p}")
  p = p@M</code></pre>
<p>In the code we started from the state <span class="math inline">\(S_0\)</span>, therefore <span class="math inline">\(\vo{p}(0) = \begin{bmatrix} 1 &amp; 0 &amp; 0 \end{bmatrix}\)</span>. The probabilities associated with each state in the first 10 steps are shown.</p>
<p>Thus in Markov Chain simulation, we propagate the probabilities associated with states since the system can be in any state with the given probability. In this case, we see that the system reaches a steady-state probability of <span class="math inline">\(\vo{p}(\infty)=\begin{bmatrix} 0.279 &amp; 0.349 &amp; 0.372 \end{bmatrix}\)</span>.</p>
<p>The steady-state probabilities can be computed from the eigen-values of the transition matrix. This is shown in the next Python code. ```{python, results=‘show’} #| jupyter: python3 import numpy as np</p>
</section>
</section>
</section>
<section id="define-the-transition-matrix" class="level1" data-number="50">
<h1 data-number="50"><span class="header-section-number">50</span> Define the transition matrix</h1>
<p>transition_matrix = np.array([[0.1, 0.6, 0.3], [0.4, 0.2, 0.4], [0.3, 0.3, 0.4]])</p>
</section>
<section id="compute-left-eigenvectors-and-eigenvalues" class="level1" data-number="51">
<h1 data-number="51"><span class="header-section-number">51</span> Compute left eigenvectors and eigenvalues</h1>
<p>eigenvalues, left_eigenvectors = np.linalg.eig(transition_matrix.T)</p>
</section>
<section id="find-the-left-eigenvector-corresponding-to-the-eigenvalue-1-steady-state" class="level1" data-number="52">
<h1 data-number="52"><span class="header-section-number">52</span> Find the left eigenvector corresponding to the eigenvalue 1 (steady state)</h1>
<p>steady_state_index = np.argmin(np.abs(eigenvalues - 1)) steady_state_vector = left_eigenvectors[:, steady_state_index].real</p>
</section>
<section id="normalize-the-steady-state-vector" class="level1" data-number="53">
<h1 data-number="53"><span class="header-section-number">53</span> Normalize the steady state vector</h1>
<p>steady_state_vector /= steady_state_vector.sum()</p>
<p>print(f”The steady-state probabilities are: {steady_state_vector}“)</p>
<pre><code>
The steady state vector for the given Markov Chain, calculated using the left eigenvectors and eigenvalues of the transition matrix, is approximately: $\begin{bmatrix} 0.279 &amp; 0.349&amp; 0.372\end{bmatrix}$, which matches the simulated output. Therefore, we don't need to simulate a Markov chain to determine the steady-state probabilities.

The steady-state probability vector indicates that, in the long run, the system will spend approximately 27.9% of the time in state $S_0$, 34.9% in state $S_1$, and 37.2% in state $S_2$. This steady state is reached regardless of the initial state of the system, given the chain is *ergodic* (see below for a discussion on ergodic processes).

## Continuous-Time Processes
A continuous time random process is a collection of random variables ordered in time, where time is considered as a continuous variable. This type of process is used to model systems or phenomena that evolve or change state in a continuous manner over time.

Key characteristics of a continuous time random process are:

1. The process is defined for every instant in a continuous time interval. For instance, $X(t)$ for $t$ in the interval $[0, \infty)$ or $[a, b]$.

2. Each $X(t)$ is a random variable representing the state of the process at time $t$.

3. The state space, which is the set of possible values of $X(t)$, can be discrete or continuous.

4. For each $t$, $X(t)$ has a probability distribution, and the joint distribution of $X(t_1), ..., X(t_n)$ for any $t_1, ..., t_n$ is defined.

5. The mean function $m(t) = \Exp{X(t)}$ and covariance function $$R(s, t) = \Exp{(X(s) - m(s))(X(t) - m(t))},$$ describe the first and second moments of the process.

Many continuous time processes are analyzed and solved using differential equations. 

Continuous time random processes are essential in modeling and analyzing systems that exhibit random behavior in a continuous temporal framework. They provide a foundation for understanding and predicting the behavior of a wide range of real-world phenomena. Some examples include:

- **Time Series Analysis**: Modeling and predicting stock prices, weather patterns, and other phenomena that change continuously over time.
- **Signal Processing**: Analyzing and filtering continuous signals in communications and audio processing.
- **Regression Models**: Gaussian processes are used for non-parametric regression, providing uncertainty measurements along with predictions.


We next describe some important types of continuous time random processes.

### Brownian Motion (Wiener Process)
Brownian Motion, also known as the Wiener Process, is a fundamental continuous-time stochastic process in mathematics, physics, and finance. It models random motion, often used to represent the unpredictable movement of particles in fluid or the erratic fluctuations in financial markets.

Key characteristics are:

1. **Continuous Path**: Unlike discrete processes, Brownian motion has a continuous path with continuous time parameter.
2. **Normal Increments**: The increments of the process are normally distributed. For any two times $t$ and $s$, the increment $W(t) - W(s)$ is normally distributed with mean 0 and variance $|t-s|$.
3. **Independent Increments**: Increments over non-overlapping intervals are independent.
4. **Starts at Zero**: The process typically starts at 0, i.e., $W(0) = 0$.

Some applications include:

- **Physics**: Modeling the random motion of particles suspended in a fluid (a phenomenon observed by botanist Robert Brown).
- **Finance**: Used in the Black-Scholes model for option pricing and to model stock prices in the Efficient Market Hypothesis.
- **Mathematics**: Fundamental in the study of stochastic processes and calculus.

Here's a simple Python script to simulate Brownian Motion using normal increments:

```{python}
#| fig-cap: "Simulation of a Brownian motion."
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt

def simulate_brownian_motion(steps, delta_t):
    """
    Simulate Brownian Motion (Wiener Process).

    Parameters:
        steps (int): Number of steps in the simulation.
        delta_t (float): Time interval between steps.

    Returns:
        numpy.ndarray: Simulated path of Brownian Motion.
    """
    # Normal increments with mean 0 and variance delta_t
    increments = np.random.normal(0, np.sqrt(delta_t), steps)
    # Cumulative sum to simulate the path
    return np.cumsum(increments)

# Parameters for the simulation
number_of_steps = 1000
time_interval = 0.01

# Simulate Brownian Motion
path = simulate_brownian_motion(number_of_steps, time_interval)

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(path)
plt.title('Brownian Motion Simulation')
plt.xlabel('Time Steps')
plt.ylabel('Position')
plt.grid(True)
plt.show()</code></pre>
<p>In this script, Brownian motion is simulated as a cumulative sum of normal increments. Each increment is normally distributed with mean 0 and variance proportional to the time interval <code>delta_t</code>. The <code>simulate_brownian_motion</code> function generates the path of Brownian motion over a specified number of steps and time interval. The resulting path is plotted to visualize the typical “random walk” pattern of Brownian motion.</p>
<section id="poisson-process" class="level3" data-number="53.0.1">
<h3 data-number="53.0.1" class="anchored" data-anchor-id="poisson-process"><span class="header-section-number">53.0.1</span> Poisson Process</h3>
<p>A Poisson Process is a fundamental stochastic process used extensively in various fields, including mathematics, physics, finance, and queueing theory. It models the occurrence of random events over time and is particularly useful for situations where events happen independently of each other.</p>
<p>Key characteristics are:</p>
<ol type="1">
<li>The process models random events (like phone calls to a call center, decay of radioactive particles, or arrival of customers at a store) happening over time.</li>
<li>Events occur independently of each other. The occurrence of one event does not affect the probability of another event occurring.</li>
<li>The probability of an event occurring in a fixed interval of time is the same for all corresponding intervals of the same length.</li>
<li>If the rate (average number of events per time unit) is constant the the Poisson process is ordinary or homogeneous. Else, the process is non-homogeneous.</li>
<li>In a time interval of length <span class="math inline">\(t\)</span>, the number of events <span class="math inline">\(N(t)\)</span> follows a Poisson distribution with parameter <span class="math inline">\(\lambda t\)</span>, where <span class="math inline">\(\lambda\)</span> is the rate of the process.</li>
<li>The probability of observing <span class="math inline">\(k\)</span> events in time <span class="math inline">\(t\)</span> is given by: <span class="math display">\[ P(N(t) = k) = \frac{e^{-\lambda t} (\lambda t)^k}{k!}.\]</span></li>
<li>The time between consecutive events follows an exponential distribution with rate <span class="math inline">\(\lambda\)</span>.</li>
<li>Poisson processes have the memoryless property, meaning that the probability of an event occurring in the future is independent of how much time has already elapsed.</li>
<li>It is often used to model rare events, where the actual number of occurrences in a fixed time interval is low relative to the potential number of occurrences.</li>
</ol>
<p>Some common applications of Poisson process include:</p>
<ul>
<li><strong>Queueing Systems</strong>: Modeling arrival of customers, calls, or requests in a system.</li>
<li><strong>Telecommunications</strong>: Describing the arrival of packets or messages in a network.</li>
<li><strong>Finance</strong>: Modeling the occurrence of certain types of financial transactions or market events.</li>
<li><strong>Biology and Medicine</strong>: Modeling random events like mutation occurrences or spread of diseases.</li>
<li><strong>Physics</strong>: Used in the study of decay processes of unstable particles.</li>
</ul>
<p>In summary, the Poisson Process is a powerful tool for modeling and understanding phenomena where events occur randomly and independently over time. It serves as a foundation for more complex stochastic models and is integral to the field of stochastic processes.</p>
<p>Here is a Python code to simulate a Poisson process.</p>
<p>```{python,results=‘hide’} #| fig-cap: “The plot shows a Poisson process. The y-axis represents the cumulative number of events that have occurred. Each step in the plot (marked with an ‘o’) indicates the occurrence of an event.”</p>
<p>import numpy as np import matplotlib.pyplot as plt</p>
<p>def simulate_poisson_process(rate, duration): ““” Simulate a Poisson process.</p>
<pre><code>Parameters:
    rate (float): The average rate (lambda) of events per unit time.
    duration (float): The total time duration for the simulation.

Returns:
    list: Times at which events occur.
"""
# The time between events follows an exponential distribution
inter_event_times = np.random.exponential(1/rate, int(rate * duration))
# The actual event times are the cumulative sum of the inter-event times
event_times = np.cumsum(inter_event_times)

# Filter out times beyond the duration
event_times = event_times[event_times &lt;= duration]

return event_times</code></pre>
</section>
</section>
<section id="parameters" class="level1" data-number="54">
<h1 data-number="54"><span class="header-section-number">54</span> Parameters</h1>
<p>rate = 2 # Average rate of 2 events per unit time duration = 10 # Total duration of 10 time units</p>
</section>
<section id="simulate-the-poisson-process" class="level1" data-number="55">
<h1 data-number="55"><span class="header-section-number">55</span> Simulate the Poisson process</h1>
<p>event_times = simulate_poisson_process(rate, duration)</p>
</section>
<section id="plotting" class="level1" data-number="56">
<h1 data-number="56"><span class="header-section-number">56</span> Plotting</h1>
<p>plt.figure(figsize=(10, 6)) plt.plot(event_times, np.arange(len(event_times)), drawstyle=‘steps-post’, marker=‘o’) plt.title(‘Simulation of a Poisson Process’) plt.xlabel(‘Time’) plt.ylabel(‘Number of Events’) plt.grid(True) plt.show()</p>
<pre><code>
The Python script simulates a Poisson process. In this simulation:

- The Poisson process is characterized by an average rate (lambda) of 2 events per unit time.
- The process is simulated over a duration of 10 time units.
- The times at which events occur are generated based on the exponential distribution of inter-event times, which is a characteristic feature of the Poisson process.

### Gaussian Process 
A Gaussian Process (GP) is a powerful, flexible probabilistic model used in machine learning, particularly for tasks like regression, classification, and optimization. It is essentially a collection of random variables, any finite number of which have a joint Gaussian distribution.

Key concepts involved are:

1. Unlike a Gaussian distribution defined over a discrete set of points, a GP is defined over a continuous domain, like time or space.
2. A GP can be thought of as a distribution over functions. It provides a way to specify prior beliefs about the function being modeled (e.g., smoothness, periodicity).
3. A GP is fully specified by its mean function $m(x)$ and covariance function $k(x, x')$, also known as the kernel. The mean function represents the average value of the function at point $x$, and the covariance function defines the similarity between the function values at different points $x$ and $x'$.

Mathematically, a GP is defined as:

$$ f(x) \sim \mathcal{GP}(m(x), k(x, x')),$$

where:

- $f(x)$ is a random function.
- $m(x)$ is the mean function, often assumed to be zero (or another constant) for simplicity.
- $k(x, x')$ is the covariance function or kernel, which encapsulates our assumptions about the function (like smoothness, periodicity, etc.).

GP is widely used in machine learning, such as

1. **Regression (Gaussian Process Regression)**: Used for non-linear regression tasks. GPs are particularly useful because they provide not only predictions but also a measure of uncertainty in these predictions.

2. **Classification**: GPs can be extended to classification tasks.

3. **Hyperparameter Tuning and Optimization**: In Bayesian optimization, GPs are used to model the function to be optimized, particularly useful for optimizing expensive-to-evaluate functions.

4. **Spatial Data Modeling**: Useful in geostatistics and environmental modeling for interpolating and predicting spatial data.

5. **Time Series Analysis**: Applied in modeling and forecasting in time series data.

Some of the key advantages of using GP in machine learning include:

- **Flexibility**: The choice of kernel allows for a wide range of behaviors.
- **Uncertainty Quantification**: Provides a probabilistic measure of uncertainty in predictions.
- **Non-Parametric**: GPs are non-parametric, meaning they can model complex datasets without having to assume a specific functional form.

However, there are some shortcomings, such as:

- **Computational Complexity**: Involves operations on covariance matrices which can be computationally expensive, particularly for large datasets.
- **Choice of Kernel**: Selecting and tuning the right kernel is crucial and can be non-trivial.

In general, Gaussian Processes offer a principled approach to learning in uncertain environments, providing both predictions and assessments of uncertainty. They are a cornerstone in probabilistic modeling and Bayesian approaches in machine learning.

Here is a Python code that generates several sample paths from a given specification of a Gaussian process:

```{python}
#| fig-cap: "In the plot each line represents a sample path from the Gaussian Process. The variability in the paths illustrates the range of functions consistent with the Gaussian Process prior and the chosen RBF kernel."
#| layout-ncol: 1

import numpy as np
import matplotlib.pyplot as plt
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

def plot_gaussian_process_samples(kernel, n_samples=5, n_points=100):
    """
    Plot samples from a Gaussian Process with a specified kernel.

    Parameters:
        kernel: Kernel function for the Gaussian Process.
        n_samples (int): Number of sample paths to generate.
        n_points (int): Number of points in each sample path.
    """
    # Create a Gaussian Process model with the specified kernel
    gp = GaussianProcessRegressor(kernel=kernel)

    # Generate points at which to sample
    x = np.linspace(0, 10, n_points).reshape(-1, 1)

    # Generate sample paths
    y_samples = gp.sample_y(x, n_samples)

    # Plotting
    plt.figure(figsize=(10, 6))
    for i in range(n_samples):
        plt.plot(x, y_samples[:, i], lw=2, label=f'Sample {i+1}')
    plt.title(f'{n_samples} Sample functions from a Gaussian Process')
    plt.xlabel('x')
    plt.ylabel('y')
    plt.legend()
    plt.show()

# Define the kernel (RBF in this case)
kernel = RBF(length_scale=1.0)

# Plot sample paths from the Gaussian Process
plot_gaussian_process_samples(kernel, n_samples=5)</code></pre>
<p>The Python script uses the <code>sklearn.gaussian_process</code> module to simulate and plot sample paths from a Gaussian Process with a Radial Basis Function (RBF) kernel.</p>
<section id="continuous-time-random-walks" class="level3" data-number="56.0.1">
<h3 data-number="56.0.1" class="anchored" data-anchor-id="continuous-time-random-walks"><span class="header-section-number">56.0.1</span> Continuous Time Random Walks</h3>
<p>A Continuous Time Random Walk (CTRW) is a stochastic model that extends the concept of a traditional random walk to a continuous time setting. Unlike discrete random walks where steps occur at regular intervals, CTRW allows for irregular timing of steps, making it a versatile tool for modeling a wide range of real-world phenomena. In a CTRW, the steps themselves and the waiting times between these steps are treated as random variables. The step sizes can follow any chosen distribution, allowing the model to adapt to the specific characteristics of the system being studied, such as the erratic movement of stock prices or the random motion of particles in fluid dynamics.</p>
<p>The waiting time distribution between steps, often modeled by exponential or heavy-tailed distributions, introduces a degree of randomness in the timing of events, which is crucial for systems where events occur sporadically. This randomness in both step sizes and timing distinguishes CTRW from models like Brownian motion, which assumes infinitesimally small, Gaussian-distributed steps at continuous intervals.</p>
<p>CTRWs are widely used in various fields including physics, for modeling diffusion in complex media; finance, for stock price movement; ecology, for animal movement tracking; and network theory, for traffic and communication patterns. Their flexibility and adaptability in modeling both the magnitude and timing of changes make them particularly valuable in studying complex systems where traditional models may fall short. The analysis of CTRWs often involves Monte Carlo simulations and statistical methods to understand and predict the behavior of the underlying process.</p>
</section>
<section id="some-important-definitions" class="level2" data-number="56.1">
<h2 data-number="56.1" class="anchored" data-anchor-id="some-important-definitions"><span class="header-section-number">56.1</span> Some Important Definitions</h2>
<section id="ergodic-process" class="level3" data-number="56.1.1">
<h3 data-number="56.1.1" class="anchored" data-anchor-id="ergodic-process"><span class="header-section-number">56.1.1</span> Ergodic Process</h3>
<p>An ergodic process is a type of stochastic process that, over a long period, exhibits the same behavior averaged over time as it does averaged over the space of all its possible states. In other words, time averages and ensemble averages are equivalent for ergodic processes. This concept is vital in statistics, thermodynamics, and information theory, as it ensures that long-term observations are representative of the whole process.</p>
<p><strong>Ergodic Process in Discrete Time</strong> In discrete-time stochastic processes (like Markov chains), an ergodic process must satisfy two conditions:</p>
<ol type="1">
<li><p><strong>Irreducibility</strong>: From any state, there is a non-zero probability of reaching any other state. This ensures that the process doesn’t get stuck in a subset of states but can explore the entire state space over time.</p></li>
<li><p><strong>Aperiodicity</strong>: The process should not be locked into a cyclic pattern. In other words, the system should not return to the same state only at multiples of some fixed number of steps.</p></li>
</ol>
<p>When a discrete-time stochastic process, such as a Markov chain, is ergodic, it means that it will eventually reach a steady state distribution that does not depend on the initial state. This steady state distribution is used for ensemble averages, and over a long time, the time averages of the process will converge to the same values.</p>
<p><strong>Ergodic Process in Continuous Time</strong></p>
<p>In continuous-time stochastic processes (like Brownian motion or certain differential equations), ergodicity implies that the statistical properties (like mean and variance) can be deduced from a single, sufficiently long, random sample path of the process.</p>
<p>In continuous time, an ergodic process must satisfy two conditions:</p>
<ul>
<li>The process must be stationary: its statistical properties should not change over time.</li>
<li>Similar to the discrete case, the process should not be restricted to a subset of its state space; it should be able to explore its entire state space given enough time.</li>
</ul>
<p>Ergodicity plays a significant role in machine learning, particularly in ensuring the reliability and robustness of various algorithms and models. Some examples are:</p>
<ol type="1">
<li><p><strong>Markov Chain Monte Carlo (MCMC) Methods</strong>: In MCMC, ergodicity is crucial to ensure that the Markov chain explores the entire state space adequately, thereby guaranteeing that samples generated from the chain are representative of the target distribution. This is essential in Bayesian inference, where MCMC methods are used to approximate posterior distributions when analytical solutions are infeasible.</p></li>
<li><p><strong>Time Series Analysis</strong>: In time series analysis, the assumption of ergodicity allows for the use of time averages as substitutes for ensemble averages. This is particularly important when dealing with real-world data where obtaining multiple sample paths is impractical. Ergodic time series models ensure that inferences drawn from a single observed sequence over time are representative of the process’s overall behavior.</p></li>
<li><p><strong>Reinforcement Learning (RL)</strong>: In RL, many algorithms rely on the ergodicity of Markov Decision Processes (MDPs) to ensure that the state-action space is sufficiently explored. This exploration guarantees that the learned policy will perform well across the entire state space, not just in frequently visited regions.</p></li>
<li><p><strong>Stationary Processes in Machine Learning Models</strong>: Ergodicity is often assumed in stationary processes, where statistical properties like mean and variance are constant over time. This assumption simplifies the modeling and prediction tasks in various machine learning applications, from natural language processing to financial modeling.</p></li>
<li><p><strong>Robustness of Algorithms</strong>: Ergodicity can contribute to the robustness of machine learning algorithms by ensuring that the conclusions drawn from the training process (such as parameter estimates or feature importance) are representative of the entire data distribution and not biased by specific characteristics of the training set.</p></li>
<li><p><strong>Statistical Learning Theory</strong>: In the broader context of statistical learning, ergodicity helps in understanding and proving convergence properties of learning algorithms, ensuring that they perform well not just on the training data but also on unseen data.</p></li>
</ol>
<p>In summary, ergodicity is leveraged in machine learning to ensure that models and algorithms are representative, robust, and reliable, particularly in scenarios involving stochastic processes, Bayesian inference, time series analysis, and reinforcement learning. It forms a foundational aspect of ensuring that long-term behavior or averaged behavior of models aligns with the true underlying patterns of the data.</p>
</section>
<section id="markov-processes" class="level3" data-number="56.1.2">
<h3 data-number="56.1.2" class="anchored" data-anchor-id="markov-processes"><span class="header-section-number">56.1.2</span> Markov Processes</h3>
<p>A Markov Process, fundamentally characterized by the Markov property, asserts that the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. In discrete time, this process is exemplified by a Markov Chain, where transitions between a finite or countably infinite set of states are governed by a transition matrix <span class="math inline">\(P\)</span>. Each element <span class="math inline">\(P_{ij}\)</span> in this matrix represents the probability of moving from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span> in one time step, thus encapsulating the process’s dynamics. The transition matrix is a (right) stochastic matrix, i.e., the rows sums are equal to one. The Markov Chain’s analysis often involves studying its stationary distribution, if it exists, where the state probabilities stabilize over time.</p>
<p>In contrast, continuous-time Markov Processes, which are more complex, often manifest as Markovian jump processes or are defined by Stochastic Differential Equations (SDEs). The state transitions occur at random intervals, dictated by exponential distributions, and are characterized by a rate matrix or generator matrix, <span class="math inline">\(Q\)</span>, where each element <span class="math inline">\(Q_{ij}\)</span> (for <span class="math inline">\(i \neq j\)</span>) indicates the rate of transitioning from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>. The diagonal elements <span class="math inline">\(Q_{ii}\)</span> are defined such that each row sums to zero, ensuring the conservation of probability. These processes are key in modeling systems where changes are continuous and event timings are stochastic, such as in financial modeling for interest rates or particle dynamics in physics.</p>
<p>Both discrete and continuous-time Markov Processes share the principle of memorylessness — the future evolution is independent of the past, given the present. This property simplifies the analysis and modeling of complex stochastic systems, making Markov Processes a fundamental tool in fields ranging from statistical mechanics to quantitative finance. The study of these processes involves a range of mathematical techniques, from linear algebra and probability theory in discrete time to differential equations and stochastic calculus in continuous time.</p>
<p>In machine learning, Markov Processes, encompassing both discrete-time Markov chains and continuous-time processes, are instrumental in various algorithms and models, particularly those involving sequential data or decision-making under uncertainty. Some examples are:</p>
<ol type="1">
<li><p><strong>Hidden Markov Models (HMMs)</strong>: These are widely used in sequence modeling tasks such as speech recognition, natural language processing, and bioinformatics. In HMMs, the observed data are considered to be a probabilistic function of underlying hidden states that follow a Markov process. The model aims to learn the hidden state sequence that most likely explains the observed data.</p></li>
<li><p><strong>Reinforcement Learning (RL)</strong>: Markov Decision Processes (MDPs), an extension of Markov processes, are foundational in RL. They provide a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. RL algorithms learn optimal policies to maximize cumulative rewards in MDPs, applicable in robotics, game playing, and autonomous systems.</p></li>
<li><p><strong>Time Series Analysis</strong>: Markov models are used for forecasting in financial modeling, weather prediction, and other areas where future states of a series depend on current states. They provide a way to model temporal dynamics and make predictions.</p></li>
<li><p><strong>Graphical Models</strong>: In probabilistic graphical models like Bayesian Networks, the Markov property is used to simplify the joint probability distribution over a set of random variables. This approach is useful in various tasks like causal inference, diagnosis, and prediction in complex systems.</p></li>
<li><p><strong>Natural Language Processing (NLP)</strong>: Beyond HMMs, other Markov process-based models are used for tasks like text generation, where the next word in a sequence depends on the previous ones.</p></li>
<li><p><strong>Clustering and Segmentation</strong>: Markov models can be used to identify clusters or segments in sequential data, such as segmenting customer transaction sequences into meaningful phases.</p></li>
<li><p><strong>Queueing Theory in Systems Optimization</strong>: In areas like network traffic management or supply chain optimization, Markov processes model the behavior of queues, aiding in the design of more efficient systems.</p></li>
</ol>
<p>In machine learning, the application of Markov Processes is rooted in their ability to model stochastic behavior in sequential data and decision-making environments. Their versatility makes them a key component in the toolkit for addressing a wide range of problems where understanding and predicting temporal dynamics are crucial.</p>
</section>
<section id="stationary-non-stationary-processes" class="level3" data-number="56.1.3">
<h3 data-number="56.1.3" class="anchored" data-anchor-id="stationary-non-stationary-processes"><span class="header-section-number">56.1.3</span> Stationary &amp; Non Stationary Processes</h3>
<p>A stationary process is a type of stochastic process whose statistical properties, such as mean, variance, and autocorrelation, do not change over time. This concept is crucial in time series analysis and signal processing, as it implies that the process behaves consistently over time, making it easier to analyze and predict.</p>
<p><strong>Types of Stationarity</strong> 1. <strong>Strict (Strong) Stationarity</strong>: A process is strictly stationary if the joint probability distribution of any set of variables <span class="math inline">\(X_{t_1}, X_{t_2}, ..., X_{t_n}\)</span> is the same as the distribution of $X_{t_1+h}, X_{t_2+h}, …, X_{t_n+h} $ for all <span class="math inline">\(t_1, t_2, ..., t_n\)</span> and for any time shift <span class="math inline">\(h\)</span>. This means that not just the mean and variance, but all moments and joint distributions, are invariant over time.</p>
<ol start="2" type="1">
<li><strong>Weak (Covariance) Stationarity</strong>: A process is weakly stationary if the mean is constant over time, the variance is finite and time-invariant, and the covariance between two time points depends only on the time lag between them and not on the actual time points themselves.</li>
</ol>
<p>Examples of stationary processes are:</p>
<ol type="1">
<li><p><strong>White Noise</strong>: A classic example of a stationary process is white noise. In white noise, each random variable has the same distribution, usually a normal distribution with constant mean and variance, and each variable is independent of the others. This process is often used as a baseline or simple error model in time series analysis.</p></li>
<li><p><strong>Rolling Dice</strong>: The process of rolling a fair dice repeatedly over time can be considered a stationary process. The probability distribution (each face having a 1/6 chance of appearing) does not change over time, satisfying the criteria for stationarity.</p></li>
<li><p><strong>Daily Temperature Variations</strong>: Assuming a stable climate, the daily temperature variations in a specific location could be modeled as a stationary process if the average temperature and variation remain consistent year after year.</p></li>
</ol>
<p>In contrast, a non-stationary process is one whose statistical properties change over time. Such processes are more complex to analyze and model because their behavior varies, and the techniques used for stationary processes are often not applicable.</p>
<p>Some characteristics of non-stationary process are:</p>
<ol type="1">
<li><strong>Changing Mean or Variance</strong>: The mean or variance (or both) of a non-stationary process may change over time, which can be due to trends, cyclic patterns, or other structural changes in the data.</li>
<li><strong>Time-Dependent Covariance</strong>: The covariance between observations may depend on the actual time at which the observations are made, not just the lag between them.</li>
</ol>
<p>Examples of non-stationary processes are:</p>
<ol type="1">
<li><p><strong>Economic Growth</strong>: The Gross Domestic Product (GDP) of a country is a typical example of a non-stationary process. It often shows a trend over time, with the average GDP and its variance changing, reflecting economic growth or recession phases.</p></li>
<li><p><strong>Stock Prices</strong>: The prices of stocks in financial markets are non-stationary. They often exhibit trends, sudden jumps, and changes in volatility (variance), making them challenging to predict over long periods.</p></li>
<li><p><strong>Seasonal Sales Data</strong>: Retail sales data often show non-stationarity due to seasonal effects. For example, sales might increase during the holiday season each year, showing a recurring but time-dependent pattern.</p></li>
<li><p><strong>Climate Change Data</strong>: Long-term climate data, such as global temperatures, can be non-stationary, especially in the context of global warming, where there is a clear upward trend in average temperatures over the years.</p></li>
</ol>
<p>In the aforementioned examples, the distinguishing factor is the evolution of the process’s statistical characteristics over time. Stationary processes maintain consistent statistical properties, simplifying their analysis and modeling. Conversely, non-stationary processes exhibit changing statistical features, such as mean and variance, requiring more intricate models to accommodate these changes. Understanding the process’s nature is essential for selecting suitable statistical or machine learning methods for analysis and prediction.</p>
<p>Analytical approaches often presuppose stationarity, particularly weak stationarity. For data that is non-stationary, techniques like differencing, detrending, or seasonal adjustments are typically employed to render the data stationary prior to analysis. The selection of predictive or analytical models is greatly influenced by the process’s stationarity. For example, ARIMA models are frequently used for stationary data, whereas variations of ARIMA that include differencing or trend components are applied to non-stationary data. Therefore, discerning whether a process is stationary or non-stationary is fundamental in choosing the appropriate analytical tools and methods for a range of fields such as economics, finance, meteorology, and signal processing.</p>
</section>
</section>
</section>
<section id="random-variables" class="level1" data-number="57">
<h1 data-number="57"><span class="header-section-number">57</span> Random Variables</h1>
<p>In this chapter, we delve into the fundamental concepts of random variables and stochastic processes, essential components of probability theory and statistical analysis in the realm of machine learning. This section aims to provide an introduction to the principles governing probability, characteristics of random variables, and the nature of stochastic processes. Central to understanding data’s inherent randomness and uncertainty, these topics are indispensable for developing a solid theoretical foundation in machine learning algorithms. This overview encompasses a spectrum of topics, from elementary probability distributions to sophisticated techniques such as Bayesian inference and Monte Carlo methods. Emphasizing both theoretical acumen and practical application, the course integrates Python-based examples to demonstrate the real-world applicability of these concepts. This overview is designed to equip students with the essential knowledge and skills to become experts in machine learning.</p>
<section id="definitions-and-probability-axioms" class="level2" data-number="57.1">
<h2 data-number="57.1" class="anchored" data-anchor-id="definitions-and-probability-axioms"><span class="header-section-number">57.1</span> Definitions and Probability Axioms</h2>
<p>Probability theory begins with the fundamental notion of a <strong>probability space</strong>, which consists of three elements: a sample space <span class="math inline">\(\Omega\)</span>, a set of events <span class="math inline">\(F\)</span>, and a probability measure <span class="math inline">\(P\)</span>.</p>
<ul>
<li><p><strong>Sample Space (<span class="math inline">\(\Omega\)</span>)</strong>: The set of all possible outcomes of a random experiment. For example, in a coin toss, <span class="math inline">\(\Omega = \{\text{heads}, \text{tails}\}\)</span>.</p></li>
<li><p><strong>Events (<span class="math inline">\(F\)</span>)</strong>: A collection of outcomes (a subset of <span class="math inline">\(\Omega\)</span>). An event is said to occur if the outcome of the experiment is in this set.</p></li>
<li><p><strong>Probability Measure (<span class="math inline">\(P\)</span>)</strong>: A function that assigns a probability to each event in <span class="math inline">\(F\)</span>, satisfying the following axioms:</p>
<ol type="1">
<li><strong>Non-negativity</strong>: For every event <span class="math inline">\(A \in F\)</span>, <span class="math inline">\(P(A) \geq 0\)</span>.</li>
<li><strong>Normalization</strong>: <span class="math inline">\(P(\Omega) = 1\)</span>.</li>
<li><strong>Additivity</strong>: For any sequence of mutually exclusive events <span class="math inline">\(A_1, A_2, \ldots\)</span>, <span class="math inline">\(P(\bigcup_{i} A_i) = \sum_{i} P(A_i)\)</span>.</li>
</ol></li>
</ul>
</section>
<section id="conditional-probability" class="level2" data-number="57.2">
<h2 data-number="57.2" class="anchored" data-anchor-id="conditional-probability"><span class="header-section-number">57.2</span> Conditional Probability</h2>
<p>Conditional probability refers to the probability of an event given that another event has occurred, denoted as <span class="math inline">\(P(A|B)\)</span>for events <span class="math inline">\(A\)</span>and <span class="math inline">\(B\)</span>. It is defined as: <span class="math display">\[ P(A|B) = \frac{P(A \cap B)}{P(B)} \]</span> provided that <span class="math inline">\(P(B) &gt; 0\)</span>.</p>
</section>
<section id="bayes-theorem" class="level2" data-number="57.3">
<h2 data-number="57.3" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">57.3</span> Bayes’ Theorem</h2>
<p>Bayes’ theorem provides a way to update our probability estimates based on new information. It is expressed as: <span class="math display">\[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]</span> where:</p>
<ul>
<li><span class="math inline">\(P(A|B)\)</span> is the posterior probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>.</li>
<li><span class="math inline">\(P(B|A)\)</span> is the likelihood of <span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(A)\)</span> is the prior probability of <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(P(B)\)</span> is the marginal probability of <span class="math inline">\(B\)</span>.</li>
</ul>
<p>Bayes’ theorem is foundational in various machine learning algorithms, especially in Bayesian inference, where it is used to update the probability estimate for a hypothesis as more evidence or information becomes available.</p>
</section>
<section id="discrete-random-variables" class="level2" data-number="57.4">
<h2 data-number="57.4" class="anchored" data-anchor-id="discrete-random-variables"><span class="header-section-number">57.4</span> Discrete Random Variables</h2>
<ul>
<li><strong>Definition</strong>: A discrete random variable is a type of random variable that can take on a countable number of distinct outcomes. Common examples include the number of heads in a coin toss or the number of successes in a series of Bernoulli trials.</li>
<li><strong>Probability Mass Function (PMF)</strong>: The PMF of a discrete random variable provides the probabilities of all possible outcomes. For a discrete random variable <span class="math inline">\(X\)</span>, the PMF <span class="math inline">\(P(X=x)\)</span> gives the probability that <span class="math inline">\(X\)</span> equals a particular value <span class="math inline">\(x\)</span>.</li>
</ul>
<section id="continuous-random-variables" class="level3" data-number="57.4.1">
<h3 data-number="57.4.1" class="anchored" data-anchor-id="continuous-random-variables"><span class="header-section-number">57.4.1</span> Continuous Random Variables</h3>
<ul>
<li><strong>Definition</strong>: A continuous random variable is a random variable that can take on an uncountably infinite number of possible values. For example, the exact time it takes for a chemical reaction to occur or the precise temperature at a given location.</li>
<li><strong>Probability Density Function (PDF)</strong>: For continuous random variables, the PDF is used to specify the probability of the random variable falling within a particular range of values. The probability of the variable falling within an interval is given by the integral of the PDF over that interval.</li>
</ul>
</section>
</section>
<section id="cumulative-distribution-functions-cdfs" class="level2" data-number="57.5">
<h2 data-number="57.5" class="anchored" data-anchor-id="cumulative-distribution-functions-cdfs"><span class="header-section-number">57.5</span> Cumulative Distribution Functions (CDFs)</h2>
<ul>
<li><strong>Definition</strong>: The CDF is a function that gives the probability that a random variable is less than or equal to a certain value. It applies to both discrete and continuous random variables.</li>
<li><strong>Expression</strong>: For a random variable <span class="math inline">\(X\)</span>, the CDF <span class="math inline">\(F(x)\)</span> is defined as <span class="math inline">\(F(x) = P(X \leq x)\)</span>. In the case of a discrete random variable, it is the sum of the probabilities up to <span class="math inline">\(x\)</span>; for a continuous variable, it is the integral of the PDF up to <span class="math inline">\(x\)</span>.</li>
</ul>
<p>We’ll use Python to generate illustrations for PMFs, PDFs, and CDFs. For PMFs, we’ll consider a simple binomial distribution, and for PDFs and CDFs, we’ll use a normal distribution.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "The Python code creates a figure with three subplots: the first showing the PMF of a binomial distribution, the second showing the PDF of a normal distribution, and the third displaying the CDF of the same normal distribution."

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, norm

# PMF of a Binomial Distribution
n, p = 10, 0.5  # Parameters for the binomial distribution
x_binom = np.arange(0, n+1)
pmf_binom = binom.pmf(x_binom, n, p)

plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
plt.bar(x_binom, pmf_binom)
plt.title('PMF of a Binomial Distribution')
plt.xlabel('Number of Successes')
plt.ylabel('Probability')

# PDF and CDF of a Normal Distribution
mu, sigma = 0, 1  # Mean and standard deviation
x_norm = np.linspace(-4, 4, 1000)
pdf_norm = norm.pdf(x_norm, mu, sigma)
cdf_norm = norm.cdf(x_norm, mu, sigma)

plt.subplot(1, 3, 2)
plt.plot(x_norm, pdf_norm)
plt.title('PDF of a Normal Distribution')
plt.xlabel('Value')
plt.ylabel('Density')

plt.subplot(1, 3, 3)
plt.plot(x_norm, cdf_norm)
plt.title('CDF of a Normal Distribution')
plt.xlabel('Value')
plt.ylabel('Cumulative Probability')

plt.tight_layout()
plt.show()</code></pre>
</section>
<section id="expected-value" class="level2" data-number="57.6">
<h2 data-number="57.6" class="anchored" data-anchor-id="expected-value"><span class="header-section-number">57.6</span> Expected Value</h2>
<ul>
<li><p><strong>Definition</strong>: The expected value (or mean) of a random variable is a measure of the central tendency of the variable’s probability distribution. Mathematically, for a discrete random variable <span class="math inline">\(X\)</span> with possible values <span class="math inline">\(x_1, x_2, \ldots\)</span> and a probability mass function <span class="math inline">\(P(X)\)</span>, the expected value <span class="math inline">\(\Exp{X}\)</span> is defined as: <span class="math display">\[ \Exp{X} = \sum_{i} x_i P(X = x_i).\]</span> For a continuous random variable with a probability density function <span class="math inline">\(f(x)\)</span>, it is defined as: <span class="math display">\[ \Exp{X} = \int_{-\infty}^{\infty} x f(x) dx \]</span></p></li>
<li><p><strong>Properties</strong>: The expected value is linear, meaning for any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, we have <span class="math inline">\(\Exp{aX + bY} = a\Exp{X} + b\Exp{Y}\)</span>.</p></li>
<li><p><strong>Significance in Machine Learning</strong>: The expected value is used in various machine learning contexts, such as defining the loss functions (e.g., mean squared error) and as a measure of central tendency in data analysis.</p></li>
</ul>
</section>
<section id="variance" class="level2" data-number="57.7">
<h2 data-number="57.7" class="anchored" data-anchor-id="variance"><span class="header-section-number">57.7</span> Variance</h2>
<ul>
<li><p><strong>Definition</strong>: The variance of a random variable measures the spread of its values. It is defined as the expected value of the squared deviation from the mean. For a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu = \Exp{X}\)</span>, the variance <span class="math inline">\(\text{Var}(X)\)</span> is given by: <span class="math display">\[ \text{Var}(X) = \Exp{(X - \mu}^2] \]</span> which can also be written as: <span class="math display">\[\text{Var}(X) = \Exp{X^2} - (\Exp{X})^2\]</span></p></li>
<li><p><strong>Properties</strong>: Variance measures the dispersion of the data. A high variance indicates that the data points are spread out from the mean, while a low variance indicates that they are clustered closely around the mean.</p></li>
<li><p><strong>Significance in Machine Learning</strong>: In machine learning, variance is a key concept in understanding the model’s behavior, particularly in the bias-variance tradeoff, which is crucial for understanding model performance and overfitting.</p></li>
</ul>
<p>We’ll use Python to illustrate the computation of expected value and variance for a simple discrete distribution (e.g., rolling a fair six-sided die) and a continuous distribution (e.g., normal distribution).</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "This code calculates and prints the expected value and variance for both a fair die roll (discrete) and a normal distribution (continuous). It also provides visualizations of their probability distributions. These examples demonstrate how expected value and variance are computed and their importance in describing the characteristics of a distribution."

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import randint, norm

# Expected Value and Variance of a Discrete Random Variable (Die Roll)
values = np.arange(1, 7)
probabilities = np.full(6, 1/6)  # Fair die
expected_value_die = np.sum(values * probabilities)
variance_die = np.sum((values - expected_value_die)**2 * probabilities)

print("Expected Value (Die Roll):", expected_value_die)
print("Variance (Die Roll):", variance_die)

# Expected Value and Variance of a Continuous Random Variable (Normal Distribution)
mu, sigma = 0, 1  # Mean and standard deviation
x = np.linspace(-5, 5, 1000)
pdf = norm.pdf(x, mu, sigma)
expected_value_normal = np.sum(x * pdf) * (x[1] - x[0])  # Approximate integral
variance_normal = np.sum(((x - mu)**2) * pdf) * (x[1] - x[0])

print("Expected Value (Normal Distribution):", expected_value_normal)
print("Variance (Normal Distribution):", variance_normal)

# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.bar(values, probabilities)
plt.title('PMF of a Fair Die')
plt.xlabel('Die Value')
plt.ylabel('Probability')

plt.subplot(1, 2, 2)
plt.plot(x, pdf)
plt.title('PDF of a Normal Distribution')
plt.xlabel('Value')
plt.ylabel('Density')

plt.show()</code></pre>
</section>
<section id="law-of-large-numbers" class="level2" data-number="57.8">
<h2 data-number="57.8" class="anchored" data-anchor-id="law-of-large-numbers"><span class="header-section-number">57.8</span> Law of Large Numbers</h2>
<p>The Law of Large Numbers (LLN) is a fundamental theorem in probability theory that describes the result of performing the same experiment a large number of times. It states that as the number of trials or observations increases, the average of the results obtained from these trials converges to the expected value. This theorem provides a solid foundation for the concept of statistical stability and the predictability of outcomes in probability and statistics.</p>
<p>There are two types of Law of Large Numbers:</p>
<ol type="1">
<li><p><strong>Weak Law of Large Numbers (WLLN)</strong>: This version, also known as Khinchin’s Law, asserts that the sample mean converges in probability towards the expected value as the sample size increases. Mathematically, for a sequence of independent and identically distributed (i.i.d.) random variables <span class="math inline">\(X_1, X_2, ..., X_n\)</span> with expected value <span class="math inline">\(\Exp{X} = \mu\)</span>, the WLLN states that for any positive number <span class="math inline">\(\epsilon\)</span>, <span class="math display">\[ P\left( \left| \frac{1}{n}\sum_{i=1}^n X_i - \mu \right| &lt; \epsilon \right) \to 1 \text{ as } n \to \infty. \]</span> This means the probability that the sample mean differs from the true mean by more than <span class="math inline">\(\epsilon\)</span> tends to zero as <span class="math inline">\(n\)</span> becomes large.</p></li>
<li><p><strong>Strong Law of Large Numbers (SLLN)</strong>: The SLLN states that the sample mean almost surely converges to the expected value as the sample size goes to infinity. In other words, the sample mean and the expected value will be equal with probability 1 in the limit of an infinite number of trials. This is a stronger statement than the WLLN and requires slightly stronger conditions.</p></li>
</ol>
<p>Some application of LLNs include:</p>
<ul>
<li><strong>Empirical Predictability</strong>: LLN explains why averages of larger samples are more stable and reliable than those of smaller samples, a principle that underpins much of empirical science.</li>
<li><strong>Statistics and Data Analysis</strong>: In statistics, LLN is crucial for the justification of using sample means as estimates for population means.</li>
<li><strong>Financial Modeling</strong>: In finance, LLN helps in predicting long-term investment outcomes based on historical averages.</li>
<li><strong>Quality Control</strong>: In industrial processes, LLN is used to understand that averaging the results of a process over a long period will give a good estimation of the overall process performance.</li>
</ul>
<p>The Law of Large Numbers is a cornerstone of probability theory and statistics, providing a rationale for the apparent regularity of large systems and the basis for making inferences about population parameters based on sample statistics. It essentially underlines the reliability of averages in large datasets, a key concept in many practical applications across various disciplines.</p>
<p>A simple demonstration of LLN is achieved by the following Python code.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "In the plot, the x-axis represents the number of trials (on a logarithmic scale for clarity), ranging from small to large sample sizes. The y-axis shows the sample mean for each set of trials. The red dashed line indicates the true mean of the distribution (0 in this case). We observe that as the number of samples increases, the emprical mean converges to the true mean."
import numpy as np
import matplotlib.pyplot as plt

# Define the number of trials
trials = [10, 50, 100, 500, 1000, 5000, 10000]

# Define a random variable (e.g., a normal distribution)
mean = 0
std_dev = 1
np.random.seed(0)  # for reproducibility

# Store the average outcomes
averages = []

# Simulate the trials
for n in trials:
    samples = np.random.normal(mean, std_dev, n)
    average = np.mean(samples)
    averages.append(average)

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot(trials, averages, marker='o', linestyle='-', color='b')
plt.axhline(y=mean, color='r', linestyle='--')
plt.title('Demonstration of the Law of Large Numbers')
plt.xscale('log')  # Log scale for better visualization
plt.xlabel('Number of Trials (log scale)')
plt.ylabel('Sample Mean')
plt.grid(True)
plt.show()
</code></pre>
<p>The Python script demonstrates the Law of Large Numbers (LLN) by simulating random samples from a normal distribution with a mean of 0 and a standard deviation of 1. As the number of trials increases, the script calculates the sample mean for each set of trials and plots these means. As illustrated in the plot, as the number of trials increases, the sample mean converges towards the true mean of the distribution, consistent with the Law of Large Numbers. This demonstrates that with a larger number of observations, the average of the outcomes tends to get closer to the expected value, showcasing the LLN’s fundamental principle in probability and statistics.</p>
</section>
<section id="central-limit-theorem" class="level2" data-number="57.9">
<h2 data-number="57.9" class="anchored" data-anchor-id="central-limit-theorem"><span class="header-section-number">57.9</span> Central Limit Theorem</h2>
<p>The Central Limit Theorem (CLT) is a fundamental principle in probability theory that states that the distribution of the sum (or average) of a large number of independent, identically distributed (i.i.d.) random variables will be approximately normally distributed, regardless of the underlying distribution of the individual variables. This theorem is pivotal in statistics because it justifies the use of the normal distribution in many real-world situations.</p>
<p>Key Points of the CLT:</p>
<ol type="1">
<li><strong>Normal Approximation</strong>: As the sample size grows, the distribution of the sample mean will approach a normal distribution, even if the original variables are not normally distributed.</li>
<li><strong>Conditions</strong>: The CLT holds under certain conditions, primarily that the variables are independent and identically distributed with a finite mean and variance.</li>
<li><strong>Sample Mean and Variance</strong>: The mean of the sample means will be equal to the mean of the original distribution, and the variance of the sample means will be equal to the variance of the original distribution divided by the sample size.</li>
</ol>
<p>The Central Limit Theorem and the Law of Large Numbers (LLN) are related but address different aspects of convergence:</p>
<ul>
<li><p><strong>Law of Large Numbers</strong>: The LLN focuses on the convergence of the sample mean to the expected value as the sample size increases. It asserts that the average of a large number of i.i.d. random variables will be close to the expected value, providing a strong foundation for statistical estimation.</p></li>
<li><p><strong>Central Limit Theorem</strong>: The CLT takes this a step further by describing the shape of the distribution of the sample mean. It not only asserts that the sample mean will converge to the expected value but also that the way in which it converges will follow a normal distribution if the sample size is large enough. This is crucial for hypothesis testing and confidence interval estimation.</p></li>
</ul>
<p>In summary, while the LLN tells us that the sample mean will be a good estimate of the population mean for large sample sizes, the CLT tells us about the distribution of this estimate, enabling the use of normal distribution-based inference methods even when the underlying data does not follow a normal distribution.</p>
<p>The following Python script demonstrates the Central Limit Theorem (CLT) by generating distributions of sample means from an original normal distribution (with a mean of 5 and standard deviation of 2) for different sample sizes. The script creates multiple samples for each specified sample size and computes the mean of each sample.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "In the figure, each histogram represents the distribution of sample means for a particular sample size. As the sample size increases, the distribution of the sample means becomes more tightly clustered around the true mean of the original distribution (5 in this case). The shape of these distributions of sample means increasingly resembles a normal distribution, especially as the sample size gets larger."
import numpy as np
import matplotlib.pyplot as plt

# Settings for the demonstration
sample_sizes = [10, 30, 50, 100, 500]
number_of_samples = 1000
original_distribution_mean = 5
original_distribution_std = 2

# Function to generate sample means
def generate_sample_means(sample_size, number_of_samples, mean, std):
    return [np.mean(np.random.normal(mean, std, sample_size)) for _ in range(number_of_samples)]

# Plot the distributions of sample means
plt.figure(figsize=(12, 8))
for sample_size in sample_sizes:
    sample_means = generate_sample_means(sample_size, number_of_samples, original_distribution_mean, original_distribution_std)
    plt.hist(sample_means, bins=30, alpha=0.5, label=f'Sample Size = {sample_size}')

plt.title('Central Limit Theorem Demonstration')
plt.xlabel('Sample Mean')
plt.ylabel('Frequency')
plt.legend()
plt.show()</code></pre>
<p>The plot effectively illustrates the essence of the CLT: regardless of the original distribution (which is normal in this case but could be any distribution with a defined mean and variance), the distribution of the sample means approaches a normal distribution as the sample size increases. This phenomenon underpins many statistical methods, especially those involving hypothesis testing and confidence interval estimation.</p>
</section>
<section id="common-distributions" class="level2" data-number="57.10">
<h2 data-number="57.10" class="anchored" data-anchor-id="common-distributions"><span class="header-section-number">57.10</span> Common Distributions</h2>
<section id="binomial-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h5>
<ul>
<li><strong>Description</strong>: The binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.</li>
<li><strong>Parameters</strong>: Number of trials <span class="math inline">\(n\)</span> and probability of success <span class="math inline">\(p\)</span> in each trial.</li>
<li><strong>PMF</strong>: <span class="math inline">\(P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}\)</span> for <span class="math inline">\(k = 0, 1, 2, \ldots, n\)</span>.</li>
</ul>
</section>
<section id="poisson-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h5>
<ul>
<li><strong>Description</strong>: The Poisson distribution models the number of times an event occurs in a fixed interval of time or space.</li>
<li><strong>Parameter</strong>: Average number of occurrences <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>PMF</strong>: <span class="math inline">\(P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}\)</span> for <span class="math inline">\(k = 0, 1, 2, \ldots\)</span>.</li>
</ul>
</section>
<section id="gaussian-normal-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="gaussian-normal-distribution">Gaussian (Normal) Distribution</h5>
<ul>
<li><strong>Description</strong>: The Gaussian distribution is a continuous distribution that is symmetric around its mean, showing the familiar bell-shaped curve.</li>
<li><strong>Parameters</strong>: Mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}\)</span>.</li>
</ul>
</section>
<section id="uniform-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="uniform-distribution">Uniform Distribution</h5>
<ul>
<li><strong>Description</strong>: The uniform distribution describes an experiment where each outcome is equally likely.</li>
<li><strong>Parameters</strong>: Lower bound <span class="math inline">\(a\)</span> and upper bound <span class="math inline">\(b\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \frac{1}{b - a}\)</span> for <span class="math inline">\(x\)</span> between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</li>
</ul>
</section>
<section id="exponential-distribution" class="level5 unnumbered">
<h5 class="unnumbered anchored" data-anchor-id="exponential-distribution">Exponential Distribution</h5>
<ul>
<li><strong>Description</strong>: The exponential distribution models the time between events in a Poisson point process.</li>
<li><strong>Parameter</strong>: Rate <span class="math inline">\(\lambda\)</span>.</li>
<li><strong>PDF</strong>: <span class="math inline">\(f(x) = \lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x \geq 0\)</span>.</li>
</ul>
<p>Let’s illustrate these distributions using Python:</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "This code will generate a series of plots illustrating the probability mass or density functions for each of the five distributions. These visualizations help in understanding the characteristics and differences of these common distributions, which are widely used in various machine learning and statistical modeling scenarios."

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import binom, poisson, norm, uniform, expon

# Parameters
n, p = 10, 0.5  # Binomial parameters
lambda_ = 3     # Poisson parameter
mu, sigma = 0, 1  # Gaussian parameters
a, b = 0, 10     # Uniform parameters
lambda_exp = 1   # Exponential parameter

# Data for plotting
x_binom = np.arange(0, n+1)
x_poisson = np.arange(0, 20)
x_gaussian = np.linspace(-4, 4, 1000)
x_uniform = np.linspace(a, b, 1000)
x_exponential = np.linspace(0, 10, 1000)

# Plotting
plt.figure(figsize=(15, 3))

plt.subplot(1, 5, 1)
plt.bar(x_binom, binom.pmf(x_binom, n, p))
plt.title('Binomial Distribution')

plt.subplot(1, 5, 2)
plt.bar(x_poisson, poisson.pmf(x_poisson, lambda_))
plt.title('Poisson Distribution')

plt.subplot(1, 5, 3)
plt.plot(x_gaussian, norm.pdf(x_gaussian, mu, sigma))
plt.title('Gaussian Distribution')

plt.subplot(1, 5, 4)
plt.plot(x_uniform, uniform.pdf(x_uniform, a, b-a))
plt.title('Uniform Distribution')

plt.subplot(1, 5, 5)
plt.plot(x_exponential, expon.pdf(x_exponential, scale=1/lambda_exp))
plt.title('Exponential Distribution')

plt.tight_layout()
plt.show()</code></pre>
</section>
</section>
<section id="joint-marginal-and-conditional-distributions" class="level2" data-number="57.11">
<h2 data-number="57.11" class="anchored" data-anchor-id="joint-marginal-and-conditional-distributions"><span class="header-section-number">57.11</span> Joint, Marginal, and Conditional Distributions</h2>
<p>Understanding the relationships between multiple random variables is crucial in statistics and machine learning. Here we’ll discuss joint, marginal, and conditional distributions.</p>
<section id="joint-distributions" class="level3" data-number="57.11.1">
<h3 data-number="57.11.1" class="anchored" data-anchor-id="joint-distributions"><span class="header-section-number">57.11.1</span> Joint Distributions</h3>
<p>Joint distributions are fundamental in understanding the relationship between two or more random variables. They can be categorized based on whether the variables involved are discrete, continuous, or a combination of both.</p>
<p>For discrete random variables, the joint distribution is defined by a joint probability mass function (PMF). This function gives the probability that each of the random variables falls within a specific range of values. For discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the joint PMF <span class="math inline">\(P(X = x, Y = y)\)</span> provides the probability that <span class="math inline">\(X = x\)</span> and <span class="math inline">\(Y = y\)</span> simultaneously. Probabilities are calculated by summing the joint PMF over the desired range of values. They have the following properties:</p>
<ul>
<li>The sum of the joint PMF over all possible values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is 1.</li>
<li>The joint PMF is always non-negative: <span class="math inline">\(P(X = x, Y = y) \geq 0\)</span>.</li>
</ul>
<p>In the case of continuous random variables, the joint distribution is described by a joint probability density function (PDF). The joint PDF <span class="math inline">\(f_{X,Y}(x, y)\)</span> represents the density of probabilities at any point in the range of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Probabilities are calculated by integrating the joint PDF over the region of interest. They have the following properties:</p>
<ul>
<li>The integral of the joint PDF over the entire space is 1.</li>
<li>The joint PDF is non-negative: <span class="math inline">\(f_{X,Y}(x, y) \geq 0\)</span>.</li>
</ul>
<p>Joint distributions are used in machine learning for modeling the relationships between features, in probabilistic classifiers, and in Bayesian inference, where understanding the dependence between variables is crucial. Joint distributions also form a core concept in statistics and machine learning, as they provide a comprehensive picture of how multiple random variables interact with each other.</p>
<section id="homogenous-joint-distributions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="homogenous-joint-distributions">Homogenous Joint Distributions</h4>
<p>Homogeneous joint distributions refer to joint probability distributions where the involve random variables that follow identical types of distributions. They also refer to probability distributions where the statistical properties and relationships between two or more random variables remain consistent across the entire range of their values. The following are some examples of homegenous joint distributions:</p>
<p><strong>Joint Probability Mass Function (Discrete Random Variables)</strong><br> Consider the outcomes from tossing two dice. Let <span class="math inline">\(X\)</span> be the outcome of the first die, and <span class="math inline">\(Y\)</span> be the outcome of the second die. Both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables taking values from 1 to 6. The joint PMF <span class="math inline">\(P(X = x, Y = y)\)</span> is <span class="math inline">\(\frac{1}{36}\)</span> for <span class="math inline">\(x, y \in \{1, 2, 3, 4, 5, 6\}\)</span>, assuming fair dice.</p>
<p><strong>Joint Probability Density Function (Continuous Random Variables)</strong><br> Consider a height and weight distribution. Let <span class="math inline">\(X\)</span> be a person’s height (in cm) and <span class="math inline">\(Y\)</span> their weight (in kg). Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a joint PDF given by <span class="math inline">\(f(x, y) = k \cdot e^{-2x - 3y}\)</span> for <span class="math inline">\(x, y &gt; 0\)</span> (a hypothetical example). Here, <span class="math inline">\(k\)</span> is a normalizing constant to ensure that the total probability integrates to 1.</p>
<p><strong>Bivariate Normal Distribution</strong><br> A common example in statistics is the bivariate normal distribution. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly normally distributed with means <span class="math inline">\(\mu_X, \mu_Y\)</span>, standard deviations <span class="math inline">\(\sigma_X, \sigma_Y\)</span>, and correlation coefficient <span class="math inline">\(\rho\)</span>. The joint PDF is: <span class="math display">\[ f(x, y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left(-\frac{1}{2(1-\rho^2)}\left[\frac{(x-\mu_X)^2}{\sigma_X^2} + \frac{(y-\mu_Y)^2}{\sigma_Y^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y}\right]\right).\]</span></p>
<p><strong>Multivariate Gaussian Distribution</strong><br> The Multivariate Gaussian (or Normal) Distribution is a generalization of the one-dimensional Gaussian distribution to multiple dimensions. It describes a random vector in <span class="math inline">\(n\)</span>-dimensional space with a specific correlation structure between its elements.</p>
<p>A random vector <span class="math inline">\(\x = [x_1, x_2, ..., x_n]^T\)</span> is said to follow a multivariate Gaussian distribution if its probability density function (PDF) is given by: <span class="math display">\[ f(\mathbf{x}; \mub, \Sigmab) = \frac{1}{\sqrt{(2\pi)^n |\Sigmab|}} \exp\left(-\frac{1}{2}(\mathbf{x} - \mub)^T \Sigmab^{-1} (\mathbf{x} - \mub)\right),\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\x\)</span> is a realization of the random vector <span class="math inline">\(\x\)</span>.</li>
<li><span class="math inline">\(\mub\)</span> is the mean vector, and $$is the covariance matrix.</li>
<li><span class="math inline">\(|\Sigmab|\)</span> is the determinant of the covariance matrix.</li>
<li><span class="math inline">\(\Sigmab^{-1}\)</span> is the inverse of the covariance matrix.</li>
</ul>
<p>It has the following properties:</p>
<ul>
<li><strong>Mean Vector</strong>: <span class="math inline">\(\mub\)</span> specifies the mean of each component of the vector <span class="math inline">\(\x\)</span>.</li>
<li><strong>Covariance Matrix</strong>: <span class="math inline">\(\Sigmab\)</span> describes the variance of each component and the covariances between them. The diagonal elements of <span class="math inline">\(\Sigmab\)</span> are the variances of the individual components, and the off-diagonal elements are the covariances.</li>
<li><strong>Correlation Structure</strong>: The covariance matrix determines how the components of <span class="math inline">\(\x\)</span> are linearly related to each other.</li>
<li><strong>Shape and Geometry</strong>: The shape of the distribution in <span class="math inline">\(n\)</span>-dimensional space can range from spherical (when <span class="math inline">\(\Sigmab\)</span> is proportional to the identity matrix) to elliptical (when <span class="math inline">\(\Sigmab\)</span> has distinct eigenvalues).</li>
</ul>
<p><strong>Uniform Distribution Over a Region</strong><br> Consider distribution of random points in a rectangle. Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> represent the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> coordinates of a point uniformly distributed over a rectangle <span class="math inline">\([a, b] \times [c, d]\)</span>. The joint PDF is <span class="math inline">\(f(x, y) = \frac{1}{(b-a)(d-c)}\)</span> for <span class="math inline">\(a \leq x \leq b\)</span> and <span class="math inline">\(c \leq y \leq d\)</span>, and 0 otherwise.</p>
</section>
<section id="heterogenous-joint-distributions" class="level4 unnumbered">
<h4 class="unnumbered anchored" data-anchor-id="heterogenous-joint-distributions">Heterogenous Joint Distributions</h4>
<p>Joint distibutions need not be homogenous. They can be heterogenous as well. Heterogeneous joint distributions involve random variables that may follow different types of distributions, and they can capture complex relationships in probabilistic models. Here are some mathematical examples:</p>
<p><strong>Joint Distribution of a Discrete and a Continuous Variable</strong><br> Suppose <span class="math inline">\(X\)</span> is a discrete random variable representing the number of defects in a product (following a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>), and <span class="math inline">\(Y\)</span> is a continuous random variable representing the time (in hours) until the first defect is detected (following an Exponential distribution with rate <span class="math inline">\(\lambda\)</span>). The joint probability function is not a standard PMF or PDF but a mixed distribution that combines the characteristics of both. The joint distribution might be defined as <span class="math inline">\(f(x, y) = P(X = x) \cdot f_Y(y|x)\)</span>, where <span class="math inline">\(f_Y(y|x)\)</span> is the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>.</p>
<p><strong>Joint Distribution with Conditional Dependence</strong><br> Let <span class="math inline">\(X\)</span> be a Bernoulli random variable indicating whether an event occurs (1) or not (0), and <span class="math inline">\(Y\)</span> be a normally distributed variable representing the outcome measurement, whose mean depends on <span class="math inline">\(X\)</span>. For instance, if <span class="math inline">\(X = 1\)</span>, <span class="math inline">\(Y\)</span> follows <span class="math inline">\(N(\mu_1, \sigma^2)\)</span>, and if <span class="math inline">\(X = 0\)</span>, <span class="math inline">\(Y\)</span> follows <span class="math inline">\(N(\mu_0, \sigma^2)\)</span>. The joint distribution is a combination of a Bernoulli and a conditional normal distribution.</p>
<p><strong>Bivariate Distribution with Different Marginals</strong><br> Consider a scenario with two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, where <span class="math inline">\(X\)</span> follows a Uniform distribution <span class="math inline">\(U(a, b)\)</span> and <span class="math inline">\(Y\)</span>, conditionally on <span class="math inline">\(X\)</span>, follows a Normal distribution with mean <span class="math inline">\(X\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. The joint distribution in this case would be <span class="math inline">\(f(x, y) = \frac{1}{b-a} \cdot \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y-x)^2}{2\sigma^2}\right)\)</span> for <span class="math inline">\(a \leq x \leq b\)</span>.</p>
<p><strong>Piecewise Joint Distribution</strong><br> Imagine a joint distribution where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> follow different distributions in different regions of their support. For example, for <span class="math inline">\(x &lt; 0\)</span>, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> might jointly follow one distribution (e.g., bivariate normal), and for <span class="math inline">\(x \geq 0\)</span>, they follow a different distribution (e.g., bivariate exponential).</p>
<p>These heterogeneous joint distributions are particularly useful in modeling complex real-world phenomena where different variables exhibit different types of probabilistic behavior or where their behavior changes under different conditions. Such models are widely used in areas like econometrics, biostatistics, and environmental science.</p>
<p>Some practical examples illustrating such heterogeneous joint distributions are:</p>
<ol type="1">
<li><p>Consider a situation where <span class="math inline">\(X\)</span> is a discrete random variable representing the number of customers arriving at a store (which can be modeled by a Poisson distribution), and <span class="math inline">\(Y\)</span> is a continuous random variable representing the total amount spent by these customers (which could be modeled by a normal distribution). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be a mix of a discrete and a continuous distribution.</p></li>
<li><p>Suppose <span class="math inline">\(X\)</span> is a binary variable indicating whether a machine is in operation (1) or not (0), and <span class="math inline">\(Y\)</span> is a continuous variable representing the temperature of the machine when it is operational. The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> combines a Bernoulli distribution (for <span class="math inline">\(X\)</span>) and a conditional continuous distribution (for <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = 1\)</span>). This is an example of a joint distribution with binary and continuous variables.</p></li>
<li><p>Imagine a survey where <span class="math inline">\(X\)</span> is a nominal variable representing a respondent’s preferred type of music (e.g., rock, jazz, classical), and <span class="math inline">\(Y\)</span> is an ordinal variable indicating satisfaction level (e.g., unsatisfied, neutral, satisfied). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would be a cross-tabulation of these categories, showing the frequencies or probabilities of each combination. This is an example of a joint distribution of nominal and ordinal variables.</p></li>
<li><p>Consider a scenario where <span class="math inline">\(X\)</span> is the number of products sold (following a Poisson distribution), and <span class="math inline">\(Y\)</span> is the number of customer complaints (which could follow a different discrete distribution, such as a binomial distribution). The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> would describe the relationship between sales and complaints, but each variable follows its own distinct distribution. This is an example of a joint distribution with multiple discrete variables with different distributions.</p></li>
</ol>
<p>These examples of heterogeneous joint distributions highlight the complexity and diversity of real-world data, where variables can be of different types and follow different distributions, yet their interdependence can be crucial for analysis and decision-making.</p>
</section>
</section>
<section id="marginal-distribution" class="level3" data-number="57.11.2">
<h3 data-number="57.11.2" class="anchored" data-anchor-id="marginal-distribution"><span class="header-section-number">57.11.2</span> Marginal Distribution</h3>
<p>Marginal distributions are used to describe the probability distribution of a subset of a collection of variables, irrespective of the values of the other variables. They are particularly important in multivariate analysis, where we have multiple interrelated random variables. The marginal distribution of a variable is the probability distribution of that variable alone, obtained by summing (in the discrete case) or integrating (in the continuous case) the joint distribution over the other variable.</p>
<p>For discrete variables, <span class="math display">\[ P(X = x) = \sum_{y} P(X = x, Y = y). \]</span></p>
<p>For continuous random variables, the marginal distribution of a variable is obtained by integrating the joint probability density function (PDF) over the range of the other variables. Suppose <span class="math inline">\(f(x, y)\)</span> is the joint PDF of two continuous random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The marginal PDFs of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are given by:</p>
<ul>
<li><p><strong>Marginal PDF of <span class="math inline">\(X\)</span></strong>: <span class="math display">\[ f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy.\]</span></p></li>
<li><p><strong>Marginal PDF of <span class="math inline">\(Y\)</span></strong>: <span class="math display">\[ f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx.\]</span></p></li>
</ul>
<p>These integrals sum up the joint probability over all possible values of the other variable, effectively ‘collapsing’ the joint distribution into a single-variable distribution.</p>
<p>Marginal distributions are significant in machine learning in the following way:</p>
<ol type="1">
<li><p><strong>Feature Analysis</strong>: Understanding the marginal distribution of each feature can provide insights into the data’s structure and inform preprocessing and feature engineering steps.</p></li>
<li><p><strong>Model Assumptions</strong>: Certain models assume independence between features, which can be examined by comparing joint and marginal distributions.</p></li>
<li><p><strong>Probabilistic Models</strong>: In probabilistic modeling, marginal distributions are crucial for making predictions and understanding the behavior of a system when only partial information is available.</p></li>
</ol>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "The plot shows a joint distribution of two continuous random variables $X$ and $Y$ as a contour plot in the center and the marginal distributions along the X and Y axes. The use of additional axes for the marginal distributions alongside the contour plot offers a comprehensive view of how the marginals relate to the joint distribution."

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal
from matplotlib.ticker import NullFormatter

# Parameters for the bivariate Gaussian
mean = [0, 0]
cov = [[1, 0.5], [0.5, 1]]  # Covariance matrix

# Create a grid of (x,y) values
x = np.linspace(-3, 3, 100)
y = np.linspace(-3, 3, 100)
X, Y = np.meshgrid(x, y)

# Bivariate Gaussian distribution
rv = multivariate_normal(mean, cov)
Z = rv.pdf(np.dstack((X, Y)))

# Marginal distributions
marginal_x = multivariate_normal(mean[0], cov[0][0])
marginal_y = multivariate_normal(mean[1], cov[1][1])

# Create figure
fig = plt.figure(figsize=(8, 8))

# Define axes
left, width = 0.1, 0.65
bottom, height = 0.1, 0.65
spacing = 0.005

rect_scatter = [left, bottom, width, height]
rect_histx = [left, bottom + height + spacing, width, 0.2]
rect_histy = [left + width + spacing, bottom, 0.2, height]

# Add axes to the figure
ax_scatter = plt.axes(rect_scatter)
ax_histx = plt.axes(rect_histx)
ax_histy = plt.axes(rect_histy)

# No labels for the additional axes
ax_histx.xaxis.set_major_formatter(NullFormatter())
ax_histy.yaxis.set_major_formatter(NullFormatter())

# Contour plot
ax_scatter.contour(X, Y, Z)

# Plot marginal distributions
ax_histx.plot(x, marginal_x.pdf(x))
ax_histy.plot(marginal_y.pdf(y), y)

# Set limits and labels
ax_scatter.set_xlim(-3, 3)
ax_scatter.set_ylim(-3, 3)
ax_histx.set_xlim(ax_scatter.get_xlim())
ax_histy.set_ylim(ax_scatter.get_ylim())
ax_scatter.set_xlabel("X")
ax_scatter.set_ylabel("Y")
ax_histx.set_ylabel('Density')
ax_histy.set_xlabel('Density')

plt.show()
</code></pre>
</section>
<section id="conditional-distribution" class="level3" data-number="57.11.3">
<h3 data-number="57.11.3" class="anchored" data-anchor-id="conditional-distribution"><span class="header-section-number">57.11.3</span> Conditional Distribution</h3>
<p>The conditional distribution describes the probability of one random variable given the occurrence of another random variable. It essentially gives the distribution of a variable contingent on the value of another.</p>
<p>For discrete random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional probability mass function (PMF) of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is defined as: <span class="math display">\[ P(X = x | Y = y) = \frac{P(X = x, Y = y)}{P(Y = y)}.\]</span> provided <span class="math inline">\(P(Y = y) &gt; 0\)</span>.</p>
<p>For continuous random variables, the conditional probability density function (PDF) is defined similarly. If <span class="math inline">\(f_{X,Y}(x, y)\)</span> is the joint PDF of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and <span class="math inline">\(f_Y(y)\)</span> is the marginal PDF of <span class="math inline">\(Y\)</span>, then the conditional PDF of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span> is: <span class="math display">\[ f_{X|Y}(x | y) = \frac{f_{X,Y}(x, y)}{f_Y(y)},\]</span> again, provided <span class="math inline">\(f_Y(y) &gt; 0\)</span>.</p>
<p>We will create a bivariate distribution using sampling from a Beta distribution and a Gaussian distribution, and then compute the conditional distribution.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Figure shows a joint Beta-Gaussian for (x,y). It also shows the conditional distribution of y for x = 0.5. This conditional distribution is visualized in the form of a histogram. The scatter plot of the bivariate distribution helps visualize the dependency between x and y."
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import beta, norm

# Generating a bivariate distribution: Beta and Gaussian
np.random.seed(0)
x = beta.rvs(2, 5, size=10000)  # Beta distribution
y = x + norm.rvs(scale=0.1, size=10000)  # Gaussian distribution, dependent on x

# Compute conditional distribution: P(Y|X=x0)
x0 = 0.5  # Condition on this value of X
indices = (x &gt; x0 - 0.05) &amp; (x &lt; x0 + 0.05)
conditional_samples = y[indices]

# Plotting
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.scatter(x, y, 1, alpha=0.5)
plt.axvline(x=x0, color='red', linestyle='--')
plt.xlabel('X (Beta distributed)')
plt.ylabel('Y (Gaussian distributed)')
plt.title('Bivariate Distribution (Beta and Gaussian)')

plt.subplot(1, 2, 2)
plt.hist(conditional_samples, bins=20, density=True, alpha=0.7, color='g')
plt.xlabel('Y values')
plt.ylabel('Density')
plt.title(f'Conditional Distribution P(Y|X={x0})')

plt.tight_layout()
plt.show()</code></pre>
</section>
<section id="covariance" class="level3" data-number="57.11.4">
<h3 data-number="57.11.4" class="anchored" data-anchor-id="covariance"><span class="header-section-number">57.11.4</span> Covariance</h3>
<p>Covariance measures the joint variability of two random variables. It indicates the direction of the linear relationship between variables.</p>
<p>For discrete variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with mean values <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span>, the covariance is: <span class="math display">\[\text{Cov}(X, Y) = \sum (x_i - \mu_X)(y_i - \mu_Y)P(x_i, y_i).\]</span></p>
<p>For continuous variables, it is: <span class="math display">\[\text{Cov}(X, Y) = \int \int (x - \mu_X)(y - \mu_Y)f_{X,Y}(x, y) dx dy.\]</span></p>
<p>Positive covariance indicates that higher values of one variable are associated with higher values of the other, and vice versa. Negative covariance indicates the opposite.</p>
</section>
<section id="correlation" class="level3" data-number="57.11.5">
<h3 data-number="57.11.5" class="anchored" data-anchor-id="correlation"><span class="header-section-number">57.11.5</span> Correlation</h3>
<p>Correlation, specifically the Pearson correlation coefficient, measures the strength and direction of a linear relationship between two variables. Unlike covariance, it is dimensionless and normalized. It is defined as <span class="math display">\[\rho_{X,Y} = \frac{\text{Cov}(X, Y)}{\sigma_X \sigma_Y},\]</span> where <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.The correlation coefficient ranges from -1 to 1. A value of 1 implies a perfect positive linear relationship, -1 implies a perfect negative linear relationship, and 0 implies no linear relationship.</p>
<p>We’ll use a simple dataset to compute and visualize the covariance and correlation between two variables:</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "The script generates two variables, X and Y, where Y is linearly dependent on X with some added noise. It calculates the covariance and correlation between these variables and then plots them to show their relationship visually. The scatter plot provides a clear picture of how Y changes with X, and the covariance and correlation values quantify this relationship."
import numpy as np
import matplotlib.pyplot as plt

# Generate a simple dataset
np.random.seed(0)
X = np.random.rand(1000)
Y = 2 * X + np.random.normal(0, 0.1, 1000)  # Y is linearly related to X

# Compute Covariance
covariance = np.cov(X, Y)[0, 1]

# Compute Correlation
correlation = np.corrcoef(X, Y)[0, 1]

# Print the computed values
print("Covariance between X and Y:", covariance)
print("Correlation between X and Y:", correlation)

# Plotting the variables
plt.scatter(X, Y, 1, alpha=0.7)
plt.title("Scatter Plot of X vs Y")
plt.xlabel("X")
plt.ylabel("Y")
plt.grid(True)
plt.show()</code></pre>
</section>
</section>
<section id="various-sampling-techniques-in-statistics-and-machine-learning" class="level2" data-number="57.12">
<h2 data-number="57.12" class="anchored" data-anchor-id="various-sampling-techniques-in-statistics-and-machine-learning"><span class="header-section-number">57.12</span> Various Sampling Techniques in Statistics and Machine Learning</h2>
<p>Sampling techniques are methods used to select a subset of data from a larger dataset. In statistics and machine learning, different sampling methods are employed based on the nature of the data and the specific goals of the analysis. Here are some common sampling techniques:</p>
<section id="simple-random-sampling" class="level3" data-number="57.12.1">
<h3 data-number="57.12.1" class="anchored" data-anchor-id="simple-random-sampling"><span class="header-section-number">57.12.1</span> Simple Random Sampling</h3>
<p>Simple Random Sampling (SRS) is a fundamental statistical method where each member of a population has an equal chance of being selected for a sample. This method is highly valued for its ability to provide unbiased representations of a larger group. In SRS, the selection of one individual is completely independent of the selection of any other, ensuring that every subset of the population has an equal probability of being chosen. This randomness can be achieved through various means, such as using lottery methods or random number generators.</p>
<p>One of the primary advantages of SRS is its simplicity and the reduction of selection bias, making it an effective tool for obtaining representative samples. However, its practicality can be challenged in large populations due to logistical constraints, and its reliance on randomness does not always guarantee a perfectly representative sample, especially with smaller sample sizes. In the realm of machine learning and data analysis, SRS is often employed for tasks like creating balanced training and test datasets or for statistical estimations of population parameters. For example, in Python, the <code>numpy.random.choice</code> function can be utilized to perform SRS, allowing for the efficient selection of a random subset from a larger dataset.</p>
</section>
<section id="stratified-sampling" class="level3" data-number="57.12.2">
<h3 data-number="57.12.2" class="anchored" data-anchor-id="stratified-sampling"><span class="header-section-number">57.12.2</span> Stratified Sampling</h3>
<p>Stratified Sampling is a statistical technique designed to improve the representativeness and efficiency of a sample by dividing the population into distinct subgroups, or strata, based on shared characteristics before sampling. This method ensures that each subgroup is adequately represented in the final sample, addressing the potential shortcomings of simple random sampling, especially in diverse populations. The process involves first identifying relevant strata within the population – these could be based on factors like age, income, education level, or any other relevant criteria. Once the strata are established, samples are drawn independently from each stratum, typically through simple random sampling or systematic sampling methods. The size of the sample from each stratum can be proportional to the stratum’s size in the population or can be equal-sized to give equal representation to each stratum regardless of its size in the population.</p>
<p>Stratified sampling is particularly useful in surveys and research studies where certain subgroups within a population may be underrepresented or have significant variability. By ensuring that these subgroups are adequately represented, stratified sampling enhances the accuracy and reliability of results, making it a powerful tool for obtaining a comprehensive understanding of the entire population.</p>
</section>
<section id="cluster-sampling" class="level3" data-number="57.12.3">
<h3 data-number="57.12.3" class="anchored" data-anchor-id="cluster-sampling"><span class="header-section-number">57.12.3</span> Cluster Sampling</h3>
<p>Cluster sampling is a practical and efficient sampling method, especially useful in situations where the population is large and geographically dispersed. This technique involves dividing the entire population into groups or clusters, often based on geographical regions or other natural groupings. These clusters should ideally represent small-scale versions of the population. In cluster sampling, instead of selecting individual members from the entire population, a random sample of these clusters is chosen for the study, and all individuals within these selected clusters are included in the sample.</p>
<p>This approach can be particularly advantageous in large-scale surveys or field studies where reaching every individual is logistically challenging and cost-prohibitive. In Python, cluster sampling can be simulated by dividing a population dataset into clusters and then randomly selecting a subset of these clusters. The data from the chosen clusters are then combined to form the sample. This method reduces the cost and time of data collection significantly, making it a preferred choice for many large-scale research projects and surveys.</p>
</section>
<section id="systematic-sampling" class="level3" data-number="57.12.4">
<h3 data-number="57.12.4" class="anchored" data-anchor-id="systematic-sampling"><span class="header-section-number">57.12.4</span> Systematic Sampling</h3>
<p>Systematic sampling is a streamlined and efficient approach to sampling, particularly effective when dealing with large, ordered populations. This method starts by arranging the population in a sequence, after which a fixed interval or step size, known as the sampling interval, is determined. This interval is typically calculated by dividing the total population size by the desired sample size. The key to systematic sampling is the selection of a random starting point within the first interval, which ensures an element of randomness in the process. From this point, every <span class="math inline">\(k^\text{th}\)</span> member of the population is selected to be part of the sample, where <span class="math inline">\(k\)</span> is the sampling interval.</p>
<p>One of the significant advantages of systematic sampling is its simplicity and the uniform coverage it provides across the population. It’s particularly useful when a comprehensive list of the population is available, and the sampling process needs to be quick and straightforward. However, it’s important to be aware of the potential for bias, especially if the population’s ordering has a hidden periodicity that aligns with the sampling interval. Despite this, systematic sampling is a popular choice in various fields due to its ease of implementation, which can be efficiently executed in Python and other programming environments. This sampling method strikes a balance between the randomness of simple random sampling and the convenience of having a structured approach to selecting a sample.</p>
<p>Here’s a Python script illustrating these sampling techniques:</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Histograms visualizing various distributions of the samples obtained through each technique described above."
import numpy as np
import matplotlib.pyplot as plt

# Population Data
population = np.random.randn(1000)  # Normally distributed data

# 1. Simple Random Sampling
simple_random_sample = np.random.choice(population, 100)

# 2. Stratified Sampling
# Assuming a binary characteristic for simplicity
strata_1 = population[population &lt; 0]  # Negative values
strata_2 = population[population &gt;= 0] # Non-negative values
stratified_sample = np.concatenate([np.random.choice(strata_1, 50), np.random.choice(strata_2, 50)])

# 3. Cluster Sampling
# Dividing into 10 clusters
clusters = np.array_split(population, 10)
selected_clusters = np.random.choice(np.arange(10), 3, replace=False)  # Select 3 clusters
cluster_sample = np.concatenate([clusters[i] for i in selected_clusters])

# 4. Systematic Sampling
start = np.random.randint(0, 10)
systematic_sample = population[start::10]

# Plotting
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes[0, 0].hist(simple_random_sample, bins=20, color='skyblue')
axes[0, 0].set_title('Simple Random Sampling')
axes[0, 1].hist(stratified_sample, bins=20, color='lightgreen')
axes[0, 1].set_title('Stratified Sampling')
axes[1, 0].hist(cluster_sample, bins=20, color='salmon')
axes[1, 0].set_title('Cluster Sampling')
axes[1, 1].hist(systematic_sample, bins=20, color='gold')
axes[1, 1].set_title('Systematic Sampling')

plt.tight_layout()
plt.show()</code></pre>
</section>
</section>
<section id="sampling-from-distributions" class="level2" data-number="57.13">
<h2 data-number="57.13" class="anchored" data-anchor-id="sampling-from-distributions"><span class="header-section-number">57.13</span> Sampling from Distributions</h2>
<p>Sampling from distributions is a crucial technique in statistics and machine learning, particularly in simulations, probabilistic modeling, and Bayesian inference. Various techniques can be used to sample from different kinds of distributions, each with its own advantages and applicable scenarios.</p>
<section id="inverse-transform-sampling" class="level3" data-number="57.13.1">
<h3 data-number="57.13.1" class="anchored" data-anchor-id="inverse-transform-sampling"><span class="header-section-number">57.13.1</span> Inverse Transform Sampling</h3>
<p>Inverse Transform Sampling, also known as the Inverse Probability Integral Transform, is a method used to generate random samples from any probability distribution, given its cumulative distribution function (CDF). This technique is particularly useful when direct sampling from the distribution is challenging.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Cumulative Distribution Function (CDF)</strong>: Start with the CDF of the desired probability distribution. The CDF, <span class="math inline">\(F(x)\)</span>, of a random variable <span class="math inline">\(X\)</span> is defined as the probability that <span class="math inline">\(X\)</span> will take a value less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Uniform Random Variable</strong>: Generate a random sample <span class="math inline">\(U\)</span> from a standard uniform distribution, i.e., <span class="math inline">\(U \sim \text{Uniform}(0,1)\)</span>.</p></li>
<li><p><strong>Inverse of CDF</strong>: Use the inverse of the CDF, <span class="math inline">\(F^{-1}(u)\)</span>, to transform the uniformly distributed sample <span class="math inline">\(U\)</span> into a sample that follows the desired distribution. The inverse CDF method hinges on the principle that if <span class="math inline">\(U\)</span> is a uniform random variable on the interval <span class="math inline">\([0,1]\)</span>, then the variable <span class="math inline">\(X = F^{-1}(U)\)</span> has the desired distribution.</p></li>
</ol>
<p>This method effectively transforms uniformly distributed data into data that follows any given distribution, using the inverse of the CDF of that distribution. It can be applied to any distribution, provided its CDF is known and is invertible. The method is computationally efficient and widely used, especially when other sampling methods are not feasible. Inverse transform sampling is extensively used in simulations, Monte Carlo methods, and various areas where generating random samples from specific distributions is required.</p>
<p>Suppose we want to generate random samples from an exponential distribution using inverse transform sampling. Here’s how we might implement it in Python:</p>
<pre class="{python}"><code>#| fig-cap: Samples from exponential distribution using inverse CDF technique.
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import expon

# Number of samples to generate
n_samples = 1000

# Generate uniform samples
uniform_samples = np.random.uniform(0, 1, n_samples)

# Inverse CDF (percent point function) of the exponential distribution
exponential_samples = expon.ppf(uniform_samples)

# Plotting the generated samples
plt.hist(exponential_samples, bins=30, density=True, alpha=0.7, label='Sampled Data')
x = np.linspace(0, 4, 100)
plt.plot(x, expon.pdf(x), label='Target Distribution', color='red')
plt.title('Random Samples from an Exponential Distribution')
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.legend()
plt.show()</code></pre>
<p>This example demonstrates the simplicity and power of inverse transform sampling for generating random samples from a specified distribution.</p>
</section>
<section id="rejection-sampling" class="level3" data-number="57.13.2">
<h3 data-number="57.13.2" class="anchored" data-anchor-id="rejection-sampling"><span class="header-section-number">57.13.2</span> Rejection Sampling</h3>
<p>Rejection sampling, also known as the acceptance-rejection method, is a technique used to generate samples from a target probability distribution <span class="math inline">\(f(x)\)</span> when direct sampling is difficult. The method involves using a simpler proposal distribution <span class="math inline">\(g(x)\)</span> from which we can easily sample.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Choose a Proposal Distribution <span class="math inline">\(g(x)\)</span></strong>: Select a distribution <span class="math inline">\(g(x)\)</span> that is easy to sample from and for which there exists a constant <span class="math inline">\(M\)</span> such that <span class="math inline">\(M \cdot g(x) \geq f(x)\)</span> for all <span class="math inline">\(x\)</span>.</p></li>
<li><p><strong>Generate Samples</strong>: Sample a point <span class="math inline">\(x\)</span> from <span class="math inline">\(g(x)\)</span> and a uniform random number <span class="math inline">\(u\)</span> from the interval <span class="math inline">\([0, M \cdot g(x)]\)</span>.</p></li>
<li><p><strong>Accept or Reject</strong>: Accept the sample <span class="math inline">\(x\)</span> if <span class="math inline">\(u \leq f(x)\)</span>; otherwise, reject it and repeat the process.</p></li>
<li><p><strong>Repeat</strong>: Repeat steps 2 and 3 until the desired number of samples is obtained.</p></li>
</ol>
<p>The efficiency of rejection sampling depends on how closely <span class="math inline">\(g(x)\)</span> approximates <span class="math inline">\(f(x)\)</span> and on the value of <span class="math inline">\(M\)</span>. If <span class="math inline">\(M\)</span> is too large or if <span class="math inline">\(g(x)\)</span> is a poor approximation of <span class="math inline">\(f(x)\)</span>, the rejection rate will be high, making the method inefficient.</p>
<p>Let’s demonstrate rejection sampling in Python, where we sample from an exponential distribution using a uniform proposal distribution.</p>
<pre class="{python}"><code>#| layout-ncol: 1
#| fig-cap: "Samples from the exponential distribution using rejection sampling and uniform proposal distribution."
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import expon

# Target distribution: Exponential distribution
def target_distribution(x):
    return expon.pdf(x)

# Proposal distribution: Uniform distribution
def proposal_distribution(x):
    return np.ones_like(x)

# Rejection Sampling
def rejection_sampling(target_dist, proposal_dist, M, size=1000):
    samples = []
    while len(samples) &lt; size:
        x_proposal = np.random.uniform(0, 4, size=size)
        u = np.random.uniform(0, M * proposal_dist(x_proposal), size=size)
        accepted = x_proposal[u &lt;= target_dist(x_proposal)]
        samples.extend(accepted.tolist())
    return np.array(samples[:size])

# Constants
M = 1.5  # Choose M such that M * g(x) &gt;= f(x)

# Generate samples
samples = rejection_sampling(target_distribution, proposal_distribution, M, size=1000)

# Plotting the sampled data
plt.hist(samples, bins=30, density=True, alpha=0.7, label='Sampled Data')
x = np.linspace(0, 4, 100)
plt.plot(x, expon.pdf(x), label='Target Distribution', color='red')
plt.title('Rejection Sampling from an Exponential Distribution')
plt.legend()
plt.show()</code></pre>
<p>Rejection sampling is particularly useful in scenarios where direct sampling from the target distribution is not feasible or too complex. The key is to choose an appropriate proposal distribution and constant <span class="math inline">\(M\)</span> to ensure a reasonable acceptance rate.</p>
</section>
<section id="importance-sampling" class="level3" data-number="57.13.3">
<h3 data-number="57.13.3" class="anchored" data-anchor-id="importance-sampling"><span class="header-section-number">57.13.3</span> Importance Sampling</h3>
<p>Importance Sampling is a statistical technique used in probability and statistics, particularly in the fields of Monte Carlo simulation and data science. It is a method for estimating properties of a particular distribution, while only having samples from a different distribution. This technique is especially useful in scenarios where direct sampling from the desired distribution is challenging or inefficient.</p>
<p>It works in the following way:</p>
<ol type="1">
<li><p><strong>Select a Proposal Distribution</strong>: Choose a distribution <span class="math inline">\(g(x)\)</span> from which it is easier to sample (known as the proposal distribution). This distribution should ideally be similar to the target distribution <span class="math inline">\(f(x)\)</span> but also ensure that where <span class="math inline">\(f(x)\)</span> is significantly non-zero, <span class="math inline">\(g(x)\)</span> is also non-zero.</p></li>
<li><p><strong>Generate Samples</strong>: Draw samples from the proposal distribution <span class="math inline">\(g(x)\)</span>.</p></li>
<li><p><strong>Weighting Samples</strong>: Calculate weights for each sampled point. The weight for a sample <span class="math inline">\(x\)</span> is given by the ratio of the target probability density to the proposal probability density at <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(w(x) = \frac{f(x)}{g(x)}\)</span>.</p></li>
<li><p><strong>Estimate Target Expectation</strong>: The expected value of a function under the target distribution is estimated by the weighted average of that function over the samples drawn from the proposal distribution.</p></li>
</ol>
<p>Importance Sampling can be more efficient than simple random sampling, especially when the area of interest under the target distribution occupies a small part of the space. It allows for the estimation of properties of one distribution using samples from another, providing flexibility in situations where sampling from the target distribution is hard.</p>
<p>However, it has some disadvantages. The efficiency of importance sampling heavily depends on the choice of the proposal distribution. A poor choice can lead to high variance in estimates and inefficient sampling. In some cases, a few samples may end up with very high weights, dominating the estimate and leading to high variance.</p>
<p>The following Python code demonstrates a simple example of importance sampling where we estimate the mean of a target exponential distribution using samples from a uniform distribution:</p>
<p>```{python, results=‘show’} import numpy as np from scipy.stats import expon, uniform</p>
</section>
</section>
</section>
<section id="target-distribution-exponential" class="level1" data-number="58">
<h1 data-number="58"><span class="header-section-number">58</span> Target distribution: Exponential</h1>
<p>target_dist = expon()</p>
</section>
<section id="proposal-distribution-uniform" class="level1" data-number="59">
<h1 data-number="59"><span class="header-section-number">59</span> Proposal distribution: Uniform</h1>
<p>proposal_dist = uniform()</p>
</section>
<section id="number-of-samples" class="level1" data-number="60">
<h1 data-number="60"><span class="header-section-number">60</span> Number of samples</h1>
<p>n_samples = 10000</p>
</section>
<section id="draw-samples-from-the-proposal-distribution" class="level1" data-number="61">
<h1 data-number="61"><span class="header-section-number">61</span> Draw samples from the proposal distribution</h1>
<p>samples = proposal_dist.rvs(size=n_samples)</p>
</section>
<section id="compute-weights" class="level1" data-number="62">
<h1 data-number="62"><span class="header-section-number">62</span> Compute weights</h1>
<p>weights = target_dist.pdf(samples) / proposal_dist.pdf(samples)</p>
</section>
<section id="estimate-the-mean-of-the-target-distribution" class="level1" data-number="63">
<h1 data-number="63"><span class="header-section-number">63</span> Estimate the mean of the target distribution</h1>
<p>estimated_mean = np.sum(weights * samples) / np.sum(weights)</p>
<p>print(“Estimated Mean of the Target Distribution:”, estimated_mean)</p>
<pre><code>
In this code, we use importance sampling to estimate the mean of an exponential distribution, using samples drawn from a uniform distribution. The weights are calculated based on the ratio of the probability densities of the target and proposal distributions at each sampled point. This method provides an estimation of the desired expectation under the target distribution.

### Markov Chain Monte Carlo (MCMC)
Markov Chain Monte Carlo (MCMC) is a set of algorithms used for sampling from probability distributions where direct sampling is difficult or impossible. MCMC enables the estimation of the distribution by constructing a Markov chain that has the desired distribution as its equilibrium distribution.

The Key concepts in MCMC are:

1. **Markov Chain**: A Markov chain is a stochastic model describing a sequence of possible events, where the probability of each event depends only on the state attained in the previous event. MCMC utilizes this property to generate samples.

2. **Monte Carlo Integration**: MCMC methods use Monte Carlo integration, where random samples are generated and used to compute estimates of desired quantities, such as means, variances, or probabilities.

3. **Convergence to Target Distribution**: The Markov chain is constructed so that it converges to the target distribution as its stationary distribution. After a 'burn-in' period, samples drawn from the Markov chain are used as samples from the target distribution.

The following are two popular MCMC algorithms:

- **Metropolis-Hastings Algorithm**: This algorithm generates a Markov chain using a proposal distribution and an acceptance criterion based on the ratio of the target densities.

- **Gibbs Sampling**: A special case of the Metropolis-Hastings algorithm that is particularly useful when sampling from high-dimensional distributions. It samples successively from the conditional distribution of each variable.

MCMC methods are widely used in various fields for estimating complex probability distributions, especially in Bayesian statistics for computing posterior distributions, in statistical physics, and in financial modeling.

Their strentgth is in versatility and ability to handle high dimensional spaces. MCMC methods can sample from virtually any probability distribution. They are particularly powerful in high-dimensional spaces where other sampling methods fail.

However, determining whether the Markov chain has converged to the target distribution can be challenging. Also, these methods can be slow, especially for complex or high-dimensional distributions.

In Bayesian statistics, MCMC is used to estimate the posterior distribution of parameters. For instance, if the likelihood function is complex or the prior distribution is not conjugate to the likelihood, traditional analytical approaches may not work, and MCMC methods like Metropolis-Hastings or Gibbs Sampling can be employed to approximate the posterior distribution. These samples then allow for statistical inference about the parameters, such as estimating means, variances, or constructing credible intervals.

MCMC methods, due to their flexibility and power, have become a cornerstone technique in modern statistical inference, particularly in scenarios where other methods are impractical or infeasible.

Implementing a simple Markov Chain Monte Carlo (MCMC) algorithm in Python can be educational for understanding how MCMC works, especially in Bayesian inference contexts. We'll demonstrate the Metropolis-Hastings algorithm, a widely used MCMC method, to sample from a probability distribution.

For simplicity, let's assume we want to sample from a unimodal Gaussian distribution, but the same approach can be extended to more complex distributions.

```{python}
#| fig-cap: Samples from a unimodal Gaussian using the Metropolis-Hastings algorithm.
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Target distribution (Gaussian in this example)
def target_distribution(x):
    return norm.pdf(x, loc=0, scale=1)

# Metropolis-Hastings Algorithm
def metropolis_hastings(target_pdf, initial_value, iterations, proposal_width):
    x = initial_value
    samples = [x]
    for i in range(iterations):
        # Proposal distribution: Gaussian centered at the current sample
        x_proposal = np.random.normal(x, proposal_width)

        # Calculate acceptance probability
        acceptance_probability = min(target_pdf(x_proposal) / target_pdf(x), 1)

        # Accept or reject the proposal
        if np.random.random() &lt; acceptance_probability:
            x = x_proposal

        samples.append(x)

    return np.array(samples)

# Run the algorithm
initial_value = 0
iterations = 10000
proposal_width = 0.5
samples = metropolis_hastings(target_distribution, initial_value, iterations, proposal_width)

# Plotting the results
plt.hist(samples, bins=30, density=True, alpha=0.6, label='Sampled Distribution')
x = np.linspace(-4, 4, 1000)
plt.plot(x, norm.pdf(x), label='Target Gaussian Distribution', color='red')
plt.title('Sampling from a Gaussian Distribution using MCMC')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.show()</code></pre>
<p>In this script:</p>
<ul>
<li>We define a target distribution as a standard Gaussian.</li>
<li>The <code>metropolis_hastings</code> function implements the MCMC algorithm. It generates a proposal for the next sample based on the current sample (Gaussian proposal distribution) and then decides whether to accept or reject this proposal based on the acceptance probability.</li>
<li>We run the algorithm for a number of iterations and collect the samples.</li>
<li>The resulting samples are plotted against the true Gaussian distribution for comparison.</li>
</ul>
<p>This example demonstrates how MCMC can be used to sample from a given distribution. The Metropolis-Hastings algorithm, in particular, is effective for complex distributions where direct sampling is challenging.</p>
<section id="distance-between-distributions." class="level2" data-number="63.1">
<h2 data-number="63.1" class="anchored" data-anchor-id="distance-between-distributions."><span class="header-section-number">63.1</span> Distance between Distributions.</h2>
<p>In machine learning, measuring the distance between probability distributions is crucial for various tasks, including statistical inference, clustering, and classification. These distances provide a way to quantify how similar or different two distributions are. Here are some commonly used distance metrics:</p>
<section id="kullback-leibler-divergence-kl-divergence" class="level3" data-number="63.1.1">
<h3 data-number="63.1.1" class="anchored" data-anchor-id="kullback-leibler-divergence-kl-divergence"><span class="header-section-number">63.1.1</span> Kullback-Leibler Divergence (KL Divergence)</h3>
<p>KL Divergence, named after Solomon Kullback and Richard Leibler, is a measure of how one probability distribution diverges from a second, reference probability distribution. It’s a concept from information theory, often used in statistics and machine learning.</p>
<p>For discrete distributions, the KL divergence of a distribution <span class="math inline">\(P\)</span> from a distribution <span class="math inline">\(Q\)</span> over the same probability space is defined as: <span class="math display">\[ D_{KL}(P \parallel Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right).\]</span></p>
<p>For continuous distributions, it is given by: <span class="math display">\[ D_{KL}(P \parallel Q) = \int_{-\infty}^{\infty} p(x) \log\left(\frac{p(x)}{q(x)}\right) dx.\]</span></p>
<p>It quantifies the amount of information lost when <span class="math inline">\(Q\)</span> is used to approximate <span class="math inline">\(P\)</span>. A KL divergence of 0 indicates that the two distributions are identical (in the continuous case, almost everywhere).</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Non-Negativity</strong>: <span class="math inline">\(D_{KL}(P \parallel Q) \geq 0\)</span>, with equality if and only if <span class="math inline">\(P = Q\)</span> (almost everywhere).</li>
<li><strong>Not Symmetric</strong>: <span class="math inline">\(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\)</span>, meaning it’s not a distance in the traditional sense.</li>
<li><strong>Not a Distance Metric</strong>: Since it’s not symmetric and doesn’t satisfy the triangle inequality, it’s not a true metric.</li>
</ul>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Model Evaluation and Selection</strong>: In Bayesian statistics, KL divergence measures how much a model’s predicted distribution deviates from the true distribution, helping in model comparison and selection.</p></li>
<li><p><strong>Variational Inference</strong>: It’s used in variational inference as a part of the Evidence Lower Bound (ELBO), to approximate complex posterior distributions in Bayesian models.</p></li>
<li><p><strong>Feature Selection and Dimensionality Reduction</strong>: Methods like t-Distributed Stochastic Neighbor Embedding (t-SNE) use KL divergence to preserve small pairwise distances or similarities between high-dimensional data points when mapping them to a lower-dimensional space.</p></li>
<li><p><strong>Training Generative Models</strong>: In generative models, like Variational Autoencoders (VAEs), KL divergence is used to regularize the encoder by penalizing deviations of its output distribution from a prior distribution, typically a Gaussian.</p></li>
<li><p><strong>Information Theory</strong>: In information retrieval and natural language processing, it helps in quantifying the information gain between different stages of the model or different models.</p></li>
</ol>
<p>KL Divergence is particularly useful in scenarios where understanding the difference or the information gain between distributions is crucial. Its ability to quantify the ‘distance’ between probability distributions makes it a valuable tool for probabilistic modeling, particularly in Bayesian frameworks and in scenarios where approximation of complex distributions is necessary.</p>
</section>
<section id="jensen-shannon-divergence-jsd" class="level3" data-number="63.1.2">
<h3 data-number="63.1.2" class="anchored" data-anchor-id="jensen-shannon-divergence-jsd"><span class="header-section-number">63.1.2</span> Jensen-Shannon Divergence (JSD)</h3>
<p>Jensen-Shannon Divergence is a method to measure the similarity between two probability distributions. It is a symmetrized and smoothed version of the Kullback-Leibler Divergence (KL Divergence). JSD overcomes some of the limitations of KL Divergence, particularly its asymmetry and the fact that it can be infinite if the two distributions being compared do not overlap.</p>
<p>The Jensen-Shannon Divergence between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as the average of the KL Divergences of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> from the mean distribution <span class="math inline">\(M = \frac{1}{2}(P + Q)\)</span>: <span class="math display">\[ \text{JSD}(P \parallel Q) = \frac{1}{2} D_{KL}(P \parallel M) + \frac{1}{2} D_{KL}(Q \parallel M).\]</span></p>
<p>JSD quantifies the similarity between two probability distributions, with a value of 0 indicating identical distributions. It is always a finite value, bounded between 0 and 1, making it a more stable measure compared to KL Divergence.</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Symmetric</strong>: Unlike KL Divergence, JSD is symmetric, i.e., <span class="math display">\[\text{JSD}(P \parallel Q) = \text{JSD}(Q \parallel P).\]</span></li>
<li><strong>Bounded</strong>: The values of JSD range from 0 to 1, where 0 indicates identical distributions and 1 indicates maximal divergence.</li>
</ul>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Model Evaluation</strong>: In machine learning models, especially in natural language processing and information retrieval, JSD can be used to compare word distribution, topic distribution, or any other probability distributions that arise.</p></li>
<li><p><strong>Generative Models</strong>: In training generative models, such as Generative Adversarial Networks (GANs), JSD can be used as an objective function to measure the difference between the generated data distribution and the real data distribution.</p></li>
<li><p><strong>Clustering and Similarity Measurement</strong>: JSD is used in clustering algorithms to measure the similarity between different data points or clusters when the data is represented as probability distributions.</p></li>
<li><p><strong>Feature Selection</strong>: It can be applied in feature selection to measure the amount of information gained by including a particular feature, especially when features can be represented probabilistically.</p></li>
</ol>
<p>JSD’s symmetric and bounded nature makes it a versatile tool for comparing probability distributions in various machine learning tasks. Its ability to provide a smooth and finite measure of divergence is particularly useful in scenarios where KL Divergence might be too sensitive or undefined.</p>
</section>
<section id="earth-movers-distance-emd-or-wasserstein-distance" class="level3" data-number="63.1.3">
<h3 data-number="63.1.3" class="anchored" data-anchor-id="earth-movers-distance-emd-or-wasserstein-distance"><span class="header-section-number">63.1.3</span> Earth Mover’s Distance (EMD) or Wasserstein Distance</h3>
<p>Earth Mover’s Distance (EMD), also known as the Wasserstein Distance, is a measure of the distance between two probability distributions over a given space. The name “Earth Mover’s” stems from a practical analogy: it represents the minimum amount of “work” required to transform one distribution into the other, where “work” is quantified as the product of the amount of “mass” moved and the distance it’s moved.</p>
<p>In a discrete setting, if we have two distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> with the same total mass, EMD is the minimum cost of turning one distribution into the other, given a “ground distance” between individual points.</p>
<p>In continuous spaces, EMD is defined via the solution to the transportation problem from the field of optimization, where the goal is to find the most efficient way to move a distribution of mass to match another distribution.</p>
<p>The Wasserstein distance of order 1 (often used in practice) between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as: <span class="math display">\[ W(P, Q) = \inf_{\gamma \in \Pi(P, Q)} \int_{X \times Y} d(x, y) \, d\gamma(x, y),\]</span> where <span class="math inline">\(\Pi(P, Q)\)</span> is the set of all joint distributions <span class="math inline">\(\gamma(x, y)\)</span> whose marginals are <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span>, and <span class="math inline">\(d(x, y)\)</span> is a ground distance between points <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
<p>Unlike other distances like KL divergence, EMD provides a more intuitive geometric interpretation. EMD is effective even when the compared distributions do not overlap, a scenario where other distances can fail or give misleading results.</p>
<p>EMD’s ability to provide a meaningful and geometrically interpretable measure of distance between distributions has made it a valuable tool in various machine learning applications, particularly in areas where understanding the “transport” of mass or information between distributions is important.</p>
<p>It has the following applications in machine learning:</p>
<ol type="1">
<li><p><strong>Optimal Transport</strong>: EMD provides a natural way to compare distributions, making it useful in optimal transport problems, where the goal is to find the most efficient way to redistribute resources.</p></li>
<li><p><strong>Generative Models</strong>: In training Generative Adversarial Networks (GANs), EMD can be used as a loss function (Wasserstein loss) to measure the distance between the distribution of generated data and the distribution of real data. It has been shown to improve the stability and performance of GAN training.</p></li>
<li><p><strong>Domain Adaptation</strong>: EMD is used in domain adaptation to measure the discrepancy between source and target domains, guiding the learning process to minimize this discrepancy.</p></li>
<li><p><strong>Image Retrieval and Computer Vision</strong>: EMD is applied in image retrieval systems to compare images represented as distributions of features. It’s also used in other computer vision tasks for comparing histograms and texture matching.</p></li>
</ol>
</section>
<section id="hellinger-distance" class="level3" data-number="63.1.4">
<h3 data-number="63.1.4" class="anchored" data-anchor-id="hellinger-distance"><span class="header-section-number">63.1.4</span> Hellinger Distance</h3>
<p>The Hellinger Distance is a metric used to quantify the similarity between two probability distributions. It’s derived from the Bhattacharyya coefficient and is used in various applications in statistics and machine learning.</p>
<p>The Hellinger Distance between two discrete or continuous probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> is defined as follows:</p>
<ul>
<li><p><strong>For Discrete Distributions</strong>: If <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> are discrete distributions with the same support, <span class="math display">\[ H(P, Q) = \frac{1}{\sqrt{2}} \sqrt{\sum_{i} (\sqrt{P(i)} - \sqrt{Q(i)})^2}.\]</span></p></li>
<li><p><strong>For Continuous Distributions</strong>: For continuous distributions with density functions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>, <span class="math display">\[ H(p, q) = \frac{1}{\sqrt{2}} \sqrt{\int (\sqrt{p(x)} - \sqrt{q(x)})^2 dx}.\]</span></p></li>
</ul>
<p>The Hellinger Distance is a measure of the “overlap” between two probability distributions. A value of 0 indicates identical distributions, while a value of 1 indicates no overlap. It is symmetric and bounded between 0 and 1, making it a true metric.</p>
<p>As a metric, it is non-negative, symmetric, and obeys the triangle inequality, making it a reliable measure for comparing distributions. The bounded range (from 0 to 1) offers an intuitive interpretation of results. Suitable for both discrete and continuous distributions and is particularly effective in scenarios where the distributions have non-overlapping support.</p>
<p>In summary, the Hellinger Distance provides a robust and interpretable way to measure the similarity or difference between two probability distributions, making it a valuable tool in various machine learning tasks, particularly those involving probabilistic modeling and analysis.</p>
<p>It has the following application in machine learning:</p>
<ol type="1">
<li><p><strong>Clustering and Classification</strong>: In clustering algorithms and classification models, the Hellinger Distance can be used as a similarity measure between distributions. This is particularly useful when dealing with probability histograms or distributions as features.</p></li>
<li><p><strong>Model Evaluation</strong>: In probabilistic models, it can be used to compare the estimated probability distribution with the true distribution, providing a measure of model performance.</p></li>
<li><p><strong>Feature Selection</strong>: In scenarios where features are represented as distributions (such as word distributions in text data), the Hellinger Distance can aid in assessing the importance of features.</p></li>
<li><p><strong>Kernel Methods</strong>: The Hellinger Distance can be used to construct kernels for SVMs and other kernel-based methods, especially in applications dealing with probability distributions.</p></li>
</ol>
</section>
<section id="total-variation-distance-tvd" class="level3" data-number="63.1.5">
<h3 data-number="63.1.5" class="anchored" data-anchor-id="total-variation-distance-tvd"><span class="header-section-number">63.1.5</span> Total Variation Distance (TVD)</h3>
<p>Total Variation Distance is a measure of the difference between two probability distributions. It is a metric that quantifies how much two distributions differ from each other.</p>
<p>For discrete distributions, the total variation distance between two probability distributions <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> over a finite or countably infinite set is defined as: <span class="math display">\[ TVD(P, Q) = \frac{1}{2} \sum_{i} |P(i) - Q(i)|.\]</span></p>
<p>For continuous distributions with probability density functions <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(q(x)\)</span>, TVD is given by: <span class="math display">\[ TVD(p, q) = \frac{1}{2} \int |p(x) - q(x)| dx.\]</span></p>
<p>TVD ranges from 0 to 1. A value of 0 indicates that the two distributions are identical, while a value of 1 indicates that the distributions are completely disjoint.</p>
<p>It has the following properties:</p>
<ul>
<li><strong>Symmetric and Bounded</strong>: TVD is symmetric (i.e., <span class="math inline">\(TVD(P, Q) = TVD(Q, P)\)</span>) and bounded between 0 and 1.</li>
<li><strong>Interpretable</strong>: It provides an easily interpretable measure of the distance between distributions.</li>
</ul>
<p>TVD is a robust metric for comparing distributions, providing a clear and bounded measure of their difference. It can be applied to both discrete and continuous distributions, making it widely applicable in various machine learning contexts.</p>
<p>Total Variation Distance is particularly valuable in scenarios where a straightforward and robust measure of distributional difference is required. Its bounded and symmetric nature makes it an intuitive and reliable tool for comparing probability distributions in various machine learning applications.</p>
<p>It has the following applications in machine learing:</p>
<ol type="1">
<li><p><strong>Model Comparison and Selection</strong>: In machine learning, especially in probabilistic modeling, TVD can be used to compare different models by measuring how close their output distributions are to the true distribution.</p></li>
<li><p><strong>Generative Models</strong>: In the training of generative models, such as Generative Adversarial Networks (GANs), TVD can be used to measure the difference between the distribution of generated data and the real data distribution.</p></li>
<li><p><strong>Reinforcement Learning</strong>: In reinforcement learning, TVD can help compare the policy distributions over different iterations, aiding in the analysis of policy convergence.</p></li>
<li><p><strong>Statistical Learning Theory</strong>: TVD plays a role in theoretical aspects of machine learning, such as understanding the behavior of learning algorithms and their convergence properties.</p></li>
<li><p><strong>Feature Engineering</strong>: In tasks involving feature engineering, TVD can be useful for comparing feature distributions across different classes or groups, helping to identify features that provide the most discriminative power.</p></li>
</ol>
</section>
</section>
<section id="functions-of-random-variables" class="level2" data-number="63.2">
<h2 data-number="63.2" class="anchored" data-anchor-id="functions-of-random-variables"><span class="header-section-number">63.2</span> Functions of Random Variables</h2>
<p>In probability and statistics, a function of a random variable is a new random variable formed by applying a function to an existing random variable. Functions of random variables are used extensively in statistics for hypothesis testing, in generating derived distributions, and in modeling relationships between variables. In machine learning, transformations of random variables are common in feature engineering and in the development of probabilistic models. Understanding functions of random variables allows one to comprehend how transformations affect the behavior and properties of the data, which is crucial in data analysis, inference, and predictive modeling.</p>
<p>Given a random variable <span class="math inline">\(X\)</span> and a real-valued function <span class="math inline">\(g\)</span>, a new random variable <span class="math inline">\(Y\)</span> can be defined as <span class="math inline">\(Y = g(X)\)</span>. The probability distribution of <span class="math inline">\(Y\)</span> is determined by the function <span class="math inline">\(g\)</span> and the distribution of <span class="math inline">\(X\)</span>. In general, we can have linear or nonlinear transformation of random variables,</p>
<ul>
<li><strong>Linear Transformations</strong>: For example, <span class="math inline">\(Y = aX + b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. Such transformations linearly scale and shift the distribution of <span class="math inline">\(X\)</span>.</li>
<li><strong>Non-linear Transformations</strong>: For example, <span class="math inline">\(Y = X^2\)</span> or <span class="math inline">\(Y = e^X\)</span>. These can significantly alter the shape of the original distribution.</li>
</ul>
<section id="calculating-the-distribution-of-y-gx" class="level3" data-number="63.2.1">
<h3 data-number="63.2.1" class="anchored" data-anchor-id="calculating-the-distribution-of-y-gx"><span class="header-section-number">63.2.1</span> Calculating the Distribution of <span class="math inline">\(Y = g(X)\)</span></h3>
<section id="functions-of-discrete-random-variables" class="level4" data-number="63.2.1.1">
<h4 data-number="63.2.1.1" class="anchored" data-anchor-id="functions-of-discrete-random-variables"><span class="header-section-number">63.2.1.1</span> Functions of Discrete Random Variables</h4>
<p>When dealing with discrete random variables, a function of a random variable results in another discrete random variable. The distribution of this transformed variable can be computed both analytically and numerically.</p>
<p><strong>Example: Analytical Approach</strong></p>
<p>If <span class="math inline">\(X\)</span> is a discrete random variable and <span class="math inline">\(g\)</span> is a function, then <span class="math inline">\(Y = g(X)\)</span> is also a discrete random variable. The PMF of <span class="math inline">\(Y\)</span>, denoted as <span class="math inline">\(p_Y(y)\)</span>, is computed by summing the probabilities of all values <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> that map to the same value <span class="math inline">\(y\)</span> under the function <span class="math inline">\(g\)</span>. Mathematically, <span class="math display">\[ p_Y(y) = \sum_{x: g(x) = y} p_X(x), \]</span> where <span class="math inline">\(p_X(x)\)</span> is the PMF of <span class="math inline">\(X\)</span>.</p>
<p>Consider a fair six-sided die. Let <span class="math inline">\(X\)</span> be the outcome of a roll (1 to 6), and let <span class="math inline">\(Y = X^2\)</span>. We find the PMF of <span class="math inline">\(Y\)</span> as follows:</p>
<ul>
<li>The possible values of <span class="math inline">\(Y\)</span> are <span class="math inline">\(\{1, 4, 9, 16, 25, 36\}\)</span>.</li>
<li>For each <span class="math inline">\(y\)</span> in <span class="math inline">\(\{1, 4, 9, 16, 25, 36\}\)</span>, compute <span class="math inline">\(p_Y(y)\)</span> using the formula. For example, <span class="math inline">\(p_Y(1) = P(X=1) = 1/6\)</span>, <span class="math inline">\(p_Y(4) = P(X=2) = 1/6\)</span>, and so on.</li>
</ul>
<p><strong>Example: Numerical Approach</strong></p>
<p>We can also apply a numerical approach to compute the distribution of a transformed variable. We first generate a large number of values of <span class="math inline">\(X\)</span> and apply <span class="math inline">\(g\)</span> to each value to get corresponding values of <span class="math inline">\(Y\)</span>. We then <em>count</em> the frequency of each value of <span class="math inline">\(Y\)</span> and divide by the total number of simulations to approximate the PMF. For die roll problem, we first simulate rolling a die a large number of times, say 10,000 times. For each roll, we compute compute the square of the outcome. Then we count the frequency of each squared value and divide by 10,000 to estimate the PMF of <span class="math inline">\(Y\)</span>.</p>
<p>This is shown in the following Python code.</p>
<pre class="{python}"><code>#| fig-cap: "Emprical determination of the distributions of a transformed variable."
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt

# Simulation of die roll
np.random.seed(0)  # for reproducibility
rolls = np.random.randint(1, 7, 10000)  # simulate 10,000 die rolls
squared_rolls = rolls ** 2  # square each roll

# Compute empirical PMF
values, counts = np.unique(squared_rolls, return_counts=True)
pmf = counts / counts.sum()

# Plotting the PMF
plt.bar(values, pmf)
plt.xlabel('Y values')
plt.ylabel('Probability')
plt.title('Empirical PMF of Y = X^2')
plt.xticks(values)
plt.show()</code></pre>
<p>This code simulates the square of a die roll and estimates its PMF numerically. It also visualizes the PMF, offering a clear understanding of the distribution of <span class="math inline">\(Y = X^2\)</span> when <span class="math inline">\(X\)</span> is the outcome of a die roll. This approach is particularly useful when the analytical computation is complex or infeasible.</p>
</section>
<section id="functions-of-continuous-random-variables" class="level4" data-number="63.2.1.2">
<h4 data-number="63.2.1.2" class="anchored" data-anchor-id="functions-of-continuous-random-variables"><span class="header-section-number">63.2.1.2</span> Functions of Continuous Random Variables</h4>
<p>When a function is applied to a continuous random variable, it results in another continuous random variable whose distribution can be derived from the original one. This transformation is key in statistical modeling and analysis.</p>
<p>Let <span class="math inline">\(X\)</span> be a continuous random variable with a probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>. Consider a function <span class="math inline">\(g\)</span> that maps <span class="math inline">\(X\)</span> to <span class="math inline">\(Y = g(X)\)</span>. The PDF of <span class="math inline">\(Y\)</span>, denoted as <span class="math inline">\(f_Y(y)\)</span>, can be found using the change-of-variable formula, which involves the derivative of the inverse function of <span class="math inline">\(g\)</span> and the PDF of <span class="math inline">\(X\)</span>. The formula is: <span class="math display">\[ f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|.\]</span></p>
<p><em>Note:</em> This approach requires <span class="math inline">\(g\)</span> to be a monotonic function (i.e., either strictly increasing or decreasing).</p>
<p><strong>Example: Analytical Approach</strong></p>
<p>Let <span class="math inline">\(X\)</span> be uniformly distributed between 0 and 1 (<span class="math inline">\(U(0,1)\)</span>), and consider the transformation <span class="math inline">\(Y = e^X\)</span>. The inverse function of <span class="math inline">\(g(x) = e^x\)</span> is <span class="math inline">\(g^{-1}(y) = \ln(y)\)</span>. The derivative of <span class="math inline">\(g^{-1}(y)\)</span> is <span class="math inline">\(\frac{1}{y}\)</span>. Since <span class="math inline">\(X\)</span> is uniform, <span class="math inline">\(f_X(x) = 1\)</span> for <span class="math inline">\(x\)</span> in [0,1]. The PDF of <span class="math inline">\(Y\)</span> is then <span class="math inline">\(f_Y(y) = 1/y\)</span> for <span class="math inline">\(y\)</span> in [1, <span class="math inline">\(e\)</span>].</p>
<p><strong>Example: Numerical Approach</strong> The numerical approach consists of the following two steps:</p>
<ol type="1">
<li>We first generate a large number of samples from the PDF of <span class="math inline">\(X\)</span> and apply <span class="math inline">\(g\)</span> to obtain samples of <span class="math inline">\(Y\)</span>. This step involves two main actions: simulation and transformation.</li>
</ol>
<ul>
<li><p><strong>Simulation</strong>: First, we simulate a large number of values from the probability density function (PDF) of the random variable <span class="math inline">\(X\)</span>. This simulation process requires selecting a distribution that <span class="math inline">\(X\)</span> follows and generating random samples from this distribution. This can be done using statistical software or programming languages like Python, which have built-in functions to generate random samples from various distributions.</p></li>
<li><p><strong>Transformation with Function <span class="math inline">\(g(\cdot)\)</span></strong>: Once we have these samples, the next step is to apply a function <span class="math inline">\(g\)</span> to each sampled value. This function <span class="math inline">\(g\)</span> transforms the original variable <span class="math inline">\(X\)</span> into a new variable <span class="math inline">\(Y\)</span>. The nature of <span class="math inline">\(g\)</span> can vary greatly depending on the analysis; it could be a simple linear function, a power function, an exponential function, or any other form of mathematical function. The crucial point is that each value of <span class="math inline">\(X\)</span> is independently put through the function <span class="math inline">\(g\)</span> to generate a corresponding value of <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<ol start="2" type="1">
<li>After transforming the simulated values, the next step is to analyze the distribution of the transformed values. There are many ways of achieving this.</li>
</ol>
<ul>
<li><p><strong>Empirical Distribution</strong>: This involves plotting the values of <span class="math inline">\(Y\)</span> to visualize their distribution. One common method is constructing a histogram, where the range of <span class="math inline">\(Y\)</span> values is divided into bins, and the frequency of values in each bin is plotted. This histogram provides a visual approximation of the PDF of <span class="math inline">\(Y\)</span>.</p></li>
<li><p><strong>Density Estimation</strong>: To get a smoother estimate of the PDF of <span class="math inline">\(Y\)</span>, kernel density estimation (KDE) can be used. KDE is a non-parametric way to estimate the probability density function of a random variable and can provide a clearer view of the distribution, especially when the transformation <span class="math inline">\(g\)</span> leads to a complex distribution.</p></li>
</ul>
<p>The following Python code illustrates the steps involved in empirically determing the distribution of the square of a normal distribution. Let <span class="math inline">\(X\)</span> be a standard normal variable. We first simulate a large number of values from <span class="math inline">\(X\)</span> and compute <span class="math inline">\(Y = X^2\)</span>. We then plot a histogram of the <span class="math inline">\(Y\)</span> values to approximate its distribution.</p>
<pre class="{python}"><code>#| fig-cap: Numerical estimation of the distribution of $Y=X^2$, where $X$ is normally distributed.
#| layout-ncol: 1
import numpy as np
import matplotlib.pyplot as plt

# Simulation of a standard normal variable
np.random.seed(0)
x_values = np.random.normal(0, 1, 10000)  # 10,000 samples from N(0,1)
y_values = x_values ** 2  # square each value

# Plotting the empirical distribution of Y
plt.hist(y_values, bins=50, density=True)
plt.title('Empirical Distribution of Y = X^2')
plt.xlabel('Y values')
plt.ylabel('Density')
plt.show()</code></pre>
<p>In summary, numerical approaches are essential when the analytical derivation of <span class="math inline">\(Y\)</span>’s distribution is complex or intractable. It provides a practical means to understand how transformations affect the distribution of a variable, which is a cornerstone in statistical analysis and data science.</p>
</section>
</section>
<section id="expectation-and-variance" class="level3" data-number="63.2.2">
<h3 data-number="63.2.2" class="anchored" data-anchor-id="expectation-and-variance"><span class="header-section-number">63.2.2</span> Expectation and Variance</h3>
<p>Computing the mean and variance of functions of random variables is a fundamental task in probability and statistics. The methods depend on whether the random variable is discrete or continuous, and the nature of the function applied to it.</p>
<section id="mean-of-a-function-of-a-random-variable" class="level4" data-number="63.2.2.1">
<h4 data-number="63.2.2.1" class="anchored" data-anchor-id="mean-of-a-function-of-a-random-variable"><span class="header-section-number">63.2.2.1</span> Mean of a Function of a Random Variable</h4>
<ol type="1">
<li><p><strong>Discrete Random Variables</strong>: Let <span class="math inline">\(X\)</span> be a discrete random variable and <span class="math inline">\(g(X)\)</span> be a function of <span class="math inline">\(X\)</span>. The mean or expected value of <span class="math inline">\(g(X)\)</span> is computed as: <span class="math display">\[ \Exp{g(X)} = \sum_{x} g(x) \cdot P(X = x).\]</span> Here, the sum is taken over all possible values of <span class="math inline">\(X\)</span>, and <span class="math inline">\(P(X = x)\)</span> is the probability mass function (PMF) of <span class="math inline">\(X\)</span>.</p></li>
<li><p><strong>Continuous Random Variables</strong>: For a continuous random variable <span class="math inline">\(X\)</span> with a probability density function (PDF) <span class="math inline">\(f_X(x)\)</span>, the mean of <span class="math inline">\(g(X)\)</span> is: <span class="math display">\[ \Exp{g(X)} = \int_{-\infty}^{\infty} g(x) \cdot f_X(x) \, dx.\]</span> The integral is taken over the entire range of <span class="math inline">\(X\)</span>.</p></li>
</ol>
</section>
<section id="variance-of-a-function-of-a-random-variable" class="level4" data-number="63.2.2.2">
<h4 data-number="63.2.2.2" class="anchored" data-anchor-id="variance-of-a-function-of-a-random-variable"><span class="header-section-number">63.2.2.2</span> Variance of a Function of a Random Variable</h4>
<p>The variance of <span class="math inline">\(g(X)\)</span> measures how much <span class="math inline">\(g(X)\)</span> is expected to deviate from its mean. It is calculated as: <span class="math display">\[ Var[g(X)] = \Exp{g(X)^2} - (\Exp{g(X)})^2.\]</span></p>
<ol type="1">
<li><p><strong>For Discrete Random Variables</strong>: Compute <span class="math inline">\(\Exp{g(X)^2}\)</span> as: <span class="math display">\[ \Exp{g(X)^2} = \sum_{x} g(x)^2 \cdot P(X = x), \]</span> and then, use the formula for variance.</p></li>
<li><p><strong>For Continuous Random Variables</strong>: Compute <span class="math inline">\(\Exp{g(X)^2}\)</span> by integrating: <span class="math display">\[\Exp{g(X)^2} = \int_{-\infty}^{\infty} g(x)^2 \cdot f_X(x) \, dx, \]</span> and then apply the variance formula.</p></li>
</ol>
</section>
<section id="example" class="level4" data-number="63.2.2.3">
<h4 data-number="63.2.2.3" class="anchored" data-anchor-id="example"><span class="header-section-number">63.2.2.3</span> Example</h4>
<p>Consider <span class="math inline">\(X\)</span> is a random variable representing the roll of a fair six-sided die, and <span class="math inline">\(g(X) = X^2\)</span> (the square of the roll).</p>
<ol type="1">
<li><p><strong>Mean of <span class="math inline">\(g(X)\)</span></strong>: <span class="math display">\[\Exp{g(X)} = \sum_{x=1}^{6} x^2 \cdot \frac{1}{6} = \frac{1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2}{6}.\]</span></p></li>
<li><p><strong>Variance of <span class="math inline">\(g(X)\)</span></strong>: First, compute <span class="math inline">\(\Exp{g(X)^2} = \sum_{x=1}^{6} x^4 \cdot \frac{1}{6}\)</span>. Then, use <span class="math inline">\(Var[g(X)] = \Exp{g(X)^2}- (\Exp{g(X)})^2\)</span>.</p></li>
</ol>
</section>
</section>
</section>
<section id="probabilistic-programming-languages" class="level2" data-number="63.3">
<h2 data-number="63.3" class="anchored" data-anchor-id="probabilistic-programming-languages"><span class="header-section-number">63.3</span> Probabilistic Programming Languages</h2>
<p>A probabilistic programming language (PPL) is a high-level programming language designed to describe probabilistic models and perform statistical inference within those models. Essentially, it allows us to define models in terms of probability distributions and then automatically perform complex computations like Bayesian inference.</p>
<p>Probabilistic Programming Languages (PPLs) and are particularly useful in machine learning for several reasons:</p>
<ul>
<li>PPLs allow for defining complex probabilistic models where functions of random variables can represent various stochastic processes or data generation mechanisms. For example, a function might transform a Gaussian random variable to model non-normal data.</li>
<li>Functions of random variables can be used to incorporate domain knowledge or specific hypotheses into a model. For example, a linear combination of random variables might represent a regression model, while more complex functions can represent non-linear relationships.</li>
<li>PPLs provide the flexibility to define arbitrary functions of random variables, making them highly expressive for modeling a wide range of phenomena. This includes transformations, nonlinear relationships, or hierarchical structures.</li>
</ul>
<p>Key features of PPLs include:</p>
<ol type="1">
<li><strong>Model Specification</strong>: Simplifies the specification of complex probabilistic models, often using syntax similar to standard programming languages.</li>
<li><strong>Inbuilt Inference Engines</strong>: PPLs typically come with in-built algorithms for performing inference, such as Markov Chain Monte Carlo (MCMC), variational inference, or other sampling methods.</li>
<li><strong>Flexibility and Extensibility</strong>: They allow for easy modification and extension of models, making them suitable for a wide range of applications from simple statistical tasks to complex machine learning models.</li>
</ol>
<p>Current state of the art packages for PPL are:</p>
<ol type="1">
<li><strong>Stan</strong>: A state-of-the-art platform for statistical modeling and high-performance statistical computation.</li>
<li><strong>PyMC3</strong>: A Python package for Bayesian statistical modeling and probabilistic machine learning which focuses on advanced Markov chain Monte Carlo and variational fitting algorithms.</li>
<li><strong>TensorFlow Probability (TFP)</strong>: A Python library built on TensorFlow for probabilistic reasoning and statistical analysis.</li>
<li><strong>Edward</strong>: A Python library for probabilistic modeling, inference, and criticism, integrated with TensorFlow.</li>
<li><strong>JAGS/BUGS</strong>: Software for analysis of Bayesian hierarchical models using Markov Chain Monte Carlo (MCMC) simulation.</li>
</ol>
<p>Here’s a simple example using TensorFlow Probability (TFP) to infer the distribution of <span class="math inline">\(Y = e^X\)</span>, where <span class="math inline">\(X\)</span> is normal.</p>
<pre class="{python}"><code>#| echo: false
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'} -- To stop log messages from Tensorflow appear in the document.</code></pre>
<p>```{python, results=‘hide’} #| fig-cap: “Use of TensorFlow Probability to estimate distribution of <span class="math inline">\(Y=e^X\)</span>, where <span class="math inline">\(X\)</span> is normal.” #| layout-ncol: 1 import tensorflow as tf import tensorflow_probability as tfp import matplotlib.pyplot as plt import numpy as np</p>
<p>tfd = tfp.distributions</p>
</section>
</section>
<section id="define-a-standard-normal-distribution-for-x" class="level1" data-number="64">
<h1 data-number="64"><span class="header-section-number">64</span> Define a standard normal distribution for X</h1>
<p>dist_X = tfd.Normal(loc=0., scale=1.)</p>
</section>
<section id="define-a-sample-size" class="level1" data-number="65">
<h1 data-number="65"><span class="header-section-number">65</span> Define a sample size</h1>
<p>sample_size = 10000</p>
</section>
<section id="sample-from-x" class="level1" data-number="66">
<h1 data-number="66"><span class="header-section-number">66</span> Sample from X</h1>
<p>samples_X = dist_X.sample(sample_size)</p>
</section>
<section id="apply-the-transformation-gx-expx-to-get-y" class="level1" data-number="67">
<h1 data-number="67"><span class="header-section-number">67</span> Apply the transformation g(X) = exp(X) to get Y</h1>
<p>samples_Y = tf.exp(0.5*samples_X)</p>
</section>
<section id="convert-to-numpy-for-plotting" class="level1" data-number="68">
<h1 data-number="68"><span class="header-section-number">68</span> # Convert to numpy for plotting</h1>
</section>
<section id="samples_y_np-samples_y.numpy" class="level1" data-number="69">
<h1 data-number="69"><span class="header-section-number">69</span> samples_Y_np = samples_Y.numpy()</h1>
</section>
<section id="plot-the-histogram-of-x-and-y" class="level1" data-number="70">
<h1 data-number="70"><span class="header-section-number">70</span> Plot the histogram of X and Y</h1>
<p>plt.subplot(1,2,1); plt.hist(samples_X.numpy(), bins=50, density=True) plt.title(‘Histogram of X (normal distribution)’) plt.xlabel(‘X’) plt.ylabel(‘Density’)</p>
<p>plt.subplot(1,2,2); plt.hist(samples_Y.numpy(), bins=50, density=True) plt.title(‘Histogram of Y = exp(X)’) plt.xlabel(‘Y’) plt.ylabel(‘Density’) plt.tight_layout() plt.show()</p>
<pre><code>
The above script uses TensorFlow and TensorFlow Probability to create a standard normal distribution, samples from it, applies a nonlinear transformation $Y=e^X$, and then plots the histogram of the transformed samples. This is a practical demonstration of how functions of random variables can be explored within the TensorFlow framework, which is especially useful for complex probabilistic models in machine learning. We can use this approach to determine transformation of distributions by more complex functions, such as neural networks.



# References {.unnumbered}

::: {#refs}
:::

# Summary


::: content-hidden
{{&lt; include macros.tex &gt;}}
:::

# Supervised Learning
Supervised learning is one of the fundamental paradigms in machine learning, where the goal is to learn a mapping from input data to output labels based on a set of labeled training data. The "supervised" aspect refers to the presence of known output labels or targets that guide the learning process. *Classification* and *regression* are two primary types of tasks in supervised machine learning, distinguished mainly by the nature of their output variables. 

**Classification**: In classification, the goal is to assign discrete labels or *categories* to input data. The output is categorical, such as 'spam' or 'not spam' in email filtering, or a set of predefined classes in image recognition. Common algorithms used in classification include logistic regression, decision trees, and neural networks, and the performance of classification models is typically evaluated using metrics like accuracy, precision, recall, and F1 score. 

**Regression**: On the other hand, regression deals with predicting continuous, numerical values based on input data. It aims to establish a relationship between variables and predict quantities like prices or temperatures. Algorithms frequently used in regression tasks include linear regression, polynomial regression, and support vector regression. Regression models are evaluated using different metrics, such as Mean Squared Error (MSE) or Mean Absolute Error (MAE). 

The key distinction lies in the output: classification predicts discrete categories, while regression forecasts continuous numerical values. This fundamental difference guides the choice of algorithms, evaluation metrics, and overall approach in solving a specific machine learning problem.

We next discuss some important algorithms commonly used in classification and regression.

## $k$-Nearest Neighbors
The k-nearest neighbors (KNN) algorithm is a simple, yet powerful machine learning method used for both classification and regression tasks. It's part of a family of algorithms known as instance-based or lazy learning algorithms, where generalization of the training data is delayed until a query is made to the system. Here's how it works:

1. **Basic Concept**: KNN operates on a simple principle: it predicts the label of a new point based on the labels of the 'k' nearest points in the training set. In other words, it looks at the 'k' closest data points from the dataset to make a prediction.

2. **Choosing 'k'**: The 'k' in KNN represents the number of nearest neighbors to consider. It's a crucial parameter that influences the accuracy of predictions. A small value of 'k' means that noise will have a higher influence on the result, whereas a large 'k' makes the algorithm slower and may lead to underfitting.

3. **Distance Metric**: To determine which points are closest, KNN uses a distance metric - typically Euclidean distance, although other metrics like Manhattan or Hamming distance can also be used depending on the type of data.

4. **Classification vs. Regression**: In classification tasks, KNN assigns the most common class among the k-nearest neighbors as the class for the new point. In regression tasks, it assigns the average of the values of the k-nearest neighbors.

5. **Lazy Learning**: KNN is considered a lazy learner because it doesn't learn a discriminative function from the training data but “memorizes” the dataset instead. Therefore, there is no explicit training phase or it is very minimal.

6. **Advantages**: The algorithm is straightforward and easy to implement, works well with a small number of input variables (features), and is effective if the training data is large.

7. **Disadvantages**: KNN gets significantly slower as the volume of data increases, making it impractical for large datasets. It also suffers from the curse of dimensionality. High-dimensional data can make distance metrics less effective, leading to poor performance of the algorithm. Furthermore, KNN can be sensitive to the scale of the data and irrelevant features, so feature scaling and selection can be crucial steps in using it effectively.

8. **Applications**: KNN is used in a variety of applications such as finance (for credit scoring and market research), healthcare (for classifying patient health risk), and recommendation systems (like suggesting similar products or services).

In essence, KNN is a versatile algorithm suitable for tackling problems with a smaller dataset and fewer dimensions, where the intuition of "likeness" *based on proximity* in a feature space is a good indicator of similarity or relatedness.

```{python}
#| fig-cap: "Five nearest neighbors of a given point in a scattered data set."
#| layout-ncol: 1
#| echo: false
#| label: fig-knn-demo
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

from sklearn.neighbors import KDTree
from sklearn.datasets import make_blobs

centers = [(-.5,0), (.5,0), (0,.5)]
cluster_std = [0.2,.2, .1]

s = 20
alp = .5

X, y = make_blobs(n_samples=500, cluster_std=cluster_std, centers=centers, n_features=3, random_state=1)
plt.scatter(X[y == 0, 0], X[y == 0, 1], color="white", edgecolors="blue",alpha=alp, s=s, label="Cluster1")
plt.scatter(X[y == 1, 0], X[y == 1, 1], color="white", s=s, edgecolors="red", alpha=alp, label="Cluster2")
plt.scatter(X[y == 2, 0], X[y == 2, 1], color="white", s=s, edgecolors="green", alpha=alp, label="Cluster3")

points = X

k = 6 # Number of neighbors+1
qp = 50 # Index of query point

# Find k nearest neighbors
kdt = KDTree(points, leaf_size=30, metric='euclidean')
NN = kdt.query(points, k=k, return_distance=False)

# Plotting
plt.scatter(points[qp,0], points[qp,1], c='black', label='Query Point',s=s)
plt.scatter(points[NN[qp][1:], 0], points[NN[qp][1:], 1], c='red', label=f'{k-1} Nearest Points',s=s)
# plt.axis('equal')
ax = plt.gca()
ax.set_aspect('equal', adjustable='box')

plt.legend(loc=1)
plt.title(f'{k-1} Nearest Neighbors')

plt.tight_layout()
plt.show()</code></pre>
<p><a href="supervised_learning.html#fig-knn-demo">Figure&nbsp;<span>11.1</span></a> illustrates 5 nearest neighbors of a point in a data set.</p>
<section id="confusion-matrix" class="level3" data-number="70.0.1">
<h3 data-number="70.0.1" class="anchored" data-anchor-id="confusion-matrix"><span class="header-section-number">70.0.1</span> Confusion Matrix</h3>
<p>A confusion matrix is a tool often used in machine learning to visualize the performance of a classification algorithm. It is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows us to see how well our classification model is performing, and to understand the types of errors it is making.</p>
<p>Here’s a breakdown of the terms:</p>
<ol type="1">
<li><p><strong>True Positives (TP)</strong>: These are cases in which the model correctly predicts the positive class.</p></li>
<li><p><strong>True Negatives (TN)</strong>: These are cases in which the model correctly predicts the negative class.</p></li>
<li><p><strong>False Positives (FP)</strong>: These are cases in which the model incorrectly predicts the positive class (also known as a “Type I error”).</p></li>
<li><p><strong>False Negatives (FN)</strong>: These are cases in which the model incorrectly predicts the negative class (also known as a “Type II error”).</p></li>
</ol>
<p>The confusion matrix looks like this:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Predicted Positive</th>
<th>Predicted Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual Positive</strong></td>
<td>True Positive (TP)</td>
<td>False Negative (FN)</td>
</tr>
<tr class="even">
<td><strong>Actual Negative</strong></td>
<td>False Positive (FP)</td>
<td>True Negative (TN)</td>
</tr>
</tbody>
</table>
<p>From the confusion matrix, several performance metrics can be calculated, such as accuracy, precision, recall, and F1 score. These metrics provide deeper insights into the performance of the model, especially in cases where the class distribution is imbalanced.</p>
<ul>
<li><strong>Accuracy</strong> is the proportion of true results (both true positives and true negatives) among the total number of cases examined.</li>
<li><strong>Precision</strong> is the proportion of true positives among all positive predictions (TP / (TP + FP)).</li>
<li><strong>Recall</strong> (or sensitivity) is the proportion of true positives identified correctly (TP / (TP + FN)).</li>
<li><strong>F1 Score</strong> is the harmonic mean of precision and recall, providing a balance between them.</li>
</ul>
<p>A confusion matrix is a simple yet powerful tool for understanding the performance of a classifier, especially in cases where mere accuracy is not sufficient to evaluate the model.</p>
</section>
<section id="example-classification" class="level3" data-number="70.0.2">
<h3 data-number="70.0.2" class="anchored" data-anchor-id="example-classification"><span class="header-section-number">70.0.2</span> Example: Classification</h3>
<p>We next show how KNNs can be used to classify stable and unstable modes of a linear dynamical system based on its eigen-values. This is a contrived example to demonstrate how proximity can be used to solve a classification problem.</p>
<p>We understand that for a dynamical system defined by the equation <span class="math inline">\(\dot{\x} = \A\x\)</span>, where <span class="math inline">\(\A\)</span> is the matrix governing the system, stability is determined by the eigenvalues of <span class="math inline">\(\A\)</span>. Specifically, the system is stable if the real part of an eigenvalue is negative, and unstable if this real part is positive. In this example, eigenvalues with a zero real part are considered unstable. This categorization is visually represented in <a href="supervised_learning.html#fig-ev_distribution">Figure&nbsp;<span>11.2</span></a>, where stable eigenvalues are marked in green and unstable ones in red. The aim is to develop a KNN classifier trained on this dataset, enabling it to accurately classify any given eigenvalue as either stable or unstable.</p>
<pre class="{python}"><code>#| fig-cap: "Distribution of stable and unstable eigen values."
#| label: fig-ev_distribution
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

nSamp = 500
X = np.random.uniform(-1,1,nSamp)
Y = np.random.uniform(-1,1,nSamp)

# Create labels
y = np.ones(nSamp)
y[X&lt;0] = -1   # These are stable.

s = 10
# plt.rcParams['text.usetex'] = False

plt.scatter(X[X&lt;0],Y[X&lt;0],color="green", alpha=0.75, s=s,label="stable")
plt.scatter(X[X&gt;0],Y[X&gt;0],color="red", alpha=0.75, s=s, label="unstable")
plt.plot([0,0],[-1,1],"k--",label="stability bdd")
plt.xlabel("Real(lambda)")
plt.ylabel("Imag(lambda)")
plt.title("Distribution of eigen-values of continuous-time linear dynamical systems: $\dot{\mathbf{x}} = \mathbf{Ax}$.")
plt.legend();</code></pre>

<!-- 1. Data Set: Eigen Values. Classify an eigen value as stable or unstable. Can't check negative sign of real part. The machine should learn it from the data.
1. Something in higher dimension -- like a fault detection of sensors or actuators.
1. Regression -- could be interpolation of scattered data -- aero dynamics. -->
<pre class="{python}"><code>import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, confusion_matrix

nSamp = 500
Data = np.random.uniform(-1,1,size=(nSamp,2))
label = np.ones(nSamp)
label[Data[:,0]&lt;0] = -1   # These are stable.
# Split the dataset into a training set and a test set
Data_train, Data_test, label_train, label_test = train_test_split(Data, label, test_size=0.2)

# Initialize the KNN classifier with k=5
knn = KNeighborsClassifier(n_neighbors=5)

# Train the model
knn.fit(Data_train, label_train)

# Make predictions
label_pred = knn.predict(Data_test)

# Evaluate the model
print(confusion_matrix(label_test, label_pred))
print(classification_report(label_test, label_pred))</code></pre>
<p>We can also check with any other random eigen-value as shown next.</p>
<pre class="{python}"><code># Create outcome labels for printing.
Outcome = {-1:"a stable mode", 1:"an unstable mode"}

# Test
candidate_lambda = np.random.uniform(-1,1,size=(1,2))
prediction = knn.predict(candidate_lambda)
print(f"Prediction: Eigen value {candidate_lambda[0]} corresponds to {Outcome[prediction[0]]}.")</code></pre>
<p>It is unlikely that we will apply this method for verifying the stability of a linear system, as it is much simpler to check the sign of the real part of eigenvalues. The purpose of this example is merely to demonstrate that using proximity can often be an effective way to classify data. Indeed, KNN and other machine learning techniques are more likely to be utilized in scenarios where a direct mathematical relationship between input and output isn’t readily apparent.</p>
<p><strong>Limitations</strong><br> One of the primary drawbacks of KNN based classification is the algorithm’s sensitivity to the scale of the data and irrelevant features. Since KNN uses distance metrics to identify the nearest neighbors, features not scaled uniformly can lead to biased distance calculations, disproportionately influencing the classification. Furthermore, the presence of irrelevant or redundant features can significantly degrade the model’s performance, as KNN does not inherently discern useful features from less useful ones.</p>
<p>Another significant challenge is the curse of dimensionality; as the number of features grows, the volume of the feature space increases exponentially, and the data becomes sparse. This sparsity makes it difficult for KNN to find meaningful nearest neighbors, as most points are almost equally far from each other.</p>
<p>Additionally, KNN can be computationally expensive for large datasets, as it requires storing the entire dataset and computing distances for each prediction.</p>
<p>Lastly, the choice of the number of neighbors (<span class="math inline">\(k\)</span>) and the distance metric can greatly affect the model’s accuracy, and finding the optimal <span class="math inline">\(k\)</span> can be a non-trivial task, often requiring extensive cross-validation.</p>
<p>These limitations necessitate careful pre-processing and tuning when applying KNN to classification problems, especially in complex, high-dimensional datasets.</p>
</section>
<section id="example-regression" class="level3" data-number="70.0.3">
<h3 data-number="70.0.3" class="anchored" data-anchor-id="example-regression"><span class="header-section-number">70.0.3</span> Example: Regression</h3>
<p>Here we consider application of KNN in a scattered data interpolation application. Such a scenarion can occur in aerospace engineering, for example in aerodynamics. In aerodynamics, engineers often deal with experimental data obtained from wind tunnel tests or flight tests. This data, representing various aerodynamic properties like lift, drag, and pressure distribution over aircraft surfaces, is often scattered. Approximating this data accurately is crucial for predicting the performance of the aircraft under different flight conditions.</p>
<p>In the following Python code we use KNN to approximate <span class="math inline">\(\sin(x)\)</span> from a set noisy scattered data. The interpolation is performed as the average of values from <span class="math inline">\(k\)</span> nearest neighbouring points. The results are shown in <a href="supervised_learning.html#fig-knn_fcn_apprx">Figure&nbsp;<span>11.3</span></a>.</p>
<pre class="{python}"><code>#| fig-cap: "Function approximation with KNN. Here we approximate $\\sin(x)$ from noisy data."
#| layout-ncol: 1
#| label: fig-knn_fcn_apprx
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsRegressor

# Step 1: Generate scattered data
# Creating random data points
np.random.seed(0)
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = np.sin(X).ravel()

# Add noise to targets
y[::5] += 1 * (0.5 - np.random.rand(8))

# Step 2: Apply KNN for interpolation
# Create a KNN model with n neighbors
n_neighbors = 5
knn = KNeighborsRegressor(n_neighbors)
knn.fit(X, y)

# Generating points for prediction
T = np.linspace(0, 5, 500)[:, np.newaxis]

# Predicting values for the generated points
y_ = knn.predict(T)

# Step 3: Plotting the data
plt.scatter(X, y, color='darkorange', label='data')
plt.plot(T, y_, color='navy', label='prediction')
plt.legend()
plt.title(f"KNN approximation with {n_neighbors} neighbors")
plt.show()</code></pre>
<p><strong>Limitations</strong><br> <span class="math inline">\(k\)</span>-Nearest Neighbors (KNN) in regression, while useful in certain contexts, has notable limitations. One significant drawback is its sensitivity to the local structure of the data. Since KNN relies on the proximity of neighboring points for predictions, its performance can be severely impacted in areas where the data is sparse or where the neighbors do not represent the underlying trend accurately. This is particularly problematic in high-dimensional spaces due to the curse of dimensionality, where the concept of <em>nearness</em> becomes less meaningful and the nearest neighbors might not be close in all dimensions, leading to poor estimates.</p>
<p>Like the classification scenario, KNN in regression can also be computationally demanding, particularly with large datasets. This is because it requires the calculation and comparison of distances for each query point. Additionally, the model’s significant reliance on the parameter <span class="math inline">\(k\)</span> complicates tuning for optimal performance, as finding the right <span class="math inline">\(k\)</span> value isn’t always straightforward.</p>
</section>
<section id="logistic-regression" class="level2" data-number="70.1">
<h2 data-number="70.1" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">70.1</span> Logistic Regression</h2>
<p>Logistic Regression is a statistical method used in machine learning for binary classification problems—where the output is discrete and typically takes on two possible values, like “yes” or “no”, “spam” or “not spam”, “sick” or “healthy”. Despite its name suggesting a regression algorithm, it’s actually used for classification tasks.</p>
<p>Key concepts of logistic regression are:</p>
<ol type="1">
<li><p><strong>Sigmoid Function</strong>: The core of logistic regression is the sigmoid function (also called the logistic function). This function takes any real-valued number and maps it into a value between 0 and 1, making it particularly suitable for a model that predicts the probability of belonging to a class.</p></li>
<li><p><strong>Probability Estimation</strong>: Logistic regression estimates the probability that a given input point belongs to a certain class. For instance, in a binary classification problem, if the output of the model is greater than 0.5, we might classify the outcome as 1 (or “yes”), and if it is less than 0.5, we classify it as 0 (or “no”).</p></li>
<li><p><strong>Model Training</strong>: The parameters of the logistic regression model are trained using a method called Maximum Likelihood Estimation (MLE). The goal is to find the parameter values that maximize the likelihood of the observed data.</p></li>
<li><p><strong>Linear Decision Boundary</strong>: Logistic regression produces a linear decision boundary. This means that the boundary between the classes is a straight line (or a plane in higher dimensions).</p></li>
<li><p><strong>Advantages</strong>: One of the key strengths of logistic regression is its ability to provide probabilities for different outcomes. This aspect goes beyond simply offering a final classification; it gives a nuanced view of the likelihood of each potential outcome. Such probabilistic outputs can be particularly informative in decision-making processes where understanding the degree of certainty or risk is as important as the decision itself. Additionally, these models can be instrumental in assessing feature importance. By analyzing how variations in input variables affect the predicted probabilities, one can gauge the relative significance of each feature. This not only aids in model interpretation but also guides feature selection and optimization, leading to more effective and efficient models.</p></li>
<li><p><strong>Disadvantages</strong>: Logistic regression is built on the assumption of a linear relationship between the independent variables and the logarithm of the odds. This linear framework, while effective for certain datasets, limits its suitability for modeling complex, nonlinear relationships inherent in some types of data. Moreover, logistic regression can be susceptible to overfitting, particularly in scenarios where the number of features significantly outweighs the number of observations. In such cases, the model might perform well on training data but fail to generalize to new, unseen data, thereby reducing its predictive power and reliability.</p></li>
</ol>
<p>Logistic regression is a statistical method used for binary classification. It models the probability of a binary response based on one or more predictor (independent) variables. The mathematical working of logistic regression revolves around the logistic function, which transforms linear combinations of predictors into probabilities.</p>
<p>The logistic function, also known as the sigmoid function, is defined as: <span class="math display">\[f(z) = \frac{1}{1 + e^{-z}},\]</span></p>
<p>where <span class="math inline">\(e\)</span> is the base of the natural logarithm and <span class="math inline">\(z\)</span> is a linear combination of the independent variables, expressed as: <span class="math display">\[z = \beta_0 + \beta_1x_1 + \beta_2x_2 + \ldots + \beta_nx_n.\]</span></p>
<p>Here, <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_n\)</span> are the <em>unknown</em> parameters of the model, and <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span> are the independent variables.</p>
<p>The logistic regression model uses the logistic function to model the probability that the dependent variable <span class="math inline">\(Y\)</span> belongs to a particular category. For a binary classification (0 or 1), the probability that <span class="math inline">\(Y = 1\)</span> is given by: <span class="math display">\[P(Y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1x_1 + \ldots + \beta_nx_n)}}.\]</span></p>
<p>The coefficients <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_n\)</span> are estimated using Maximum Likelihood Estimation (MLE). The MLE approach seeks to find the values of the coefficients that maximize the likelihood of observing the sample data.</p>
<section id="the-mle-approach" class="level3" data-number="70.1.1">
<h3 data-number="70.1.1" class="anchored" data-anchor-id="the-mle-approach"><span class="header-section-number">70.1.1</span> The MLE Approach</h3>
<p>Maximum Likelihood Estimation (MLE) in logistic regression is a method that applies mathematical rigor to estimate the model’s parameters in a way that the observed data are most probable. This process starts with the construction of a likelihood function that represents the probability of observing the given data under certain parameter values. In logistic regression, this likelihood for each observation is expressed as the product of the individual probabilities for each data point, following the logistic model.</p>
<p>Mathematically, the likelihood for an individual observation <span class="math inline">\(Y_i\)</span> given features <span class="math inline">\(X_i\)</span> and parameters <span class="math inline">\(\beta\)</span> is modeled as: <span class="math display">\[P(Y_i|X_i;\beta) = \sigma(X_i\beta)^{Y_i} \times [1 - \sigma(X_i\beta)]^{(1 - Y_i)}\]</span> where <span class="math inline">\(\sigma(X_i\beta)\)</span> is the logistic function <span class="math inline">\(\frac{1}{1 + e^{-X_i\beta}}\)</span>, defining the probability of <span class="math inline">\(Y_i\)</span> being 1.</p>
<p>The likelihood for the entire dataset of <span class="math inline">\(n\)</span> observations is the product of these individual probabilities: <span class="math display">\[L(\beta) = \prod_{i=1}^{n} P(Y_i|X_i;\beta)\]</span></p>
<p>Given that direct multiplication can lead to numerically unstable results due to very small probabilities, we use the log-likelihood function: <span class="math display">\[\ell(\beta) = \sum_{i=1}^{n} [ Y_i \log(\sigma(X_i\beta)) + (1 - Y_i) \log(1 - \sigma(X_i\beta)) ]\]</span></p>
<p>The MLE approach involves finding the parameter values <span class="math inline">\(\beta\)</span> that maximize this log-likelihood function. This maximization problem does not have a closed-form solution like linear regression, so iterative optimization algorithms such as gradient descent are employed. These algorithms adjust <span class="math inline">\(\beta\)</span> iteratively to find the maximum of <span class="math inline">\(\ell(\beta)\)</span>.</p>
<p>During each iteration, the gradient of the log-likelihood function with respect to <span class="math inline">\(\beta\)</span> is computed to determine the direction in which <span class="math inline">\(\beta\)</span> should be adjusted. The process repeats until it converges to the parameter values that yield the maximum log-likelihood, indicating the most probable parameters given the observed data, under the logistic regression model.</p>
<p>This mathematical framework ensures that MLE in logistic regression is not just a heuristic but a statistically sound method for parameter estimation, aligning the model as closely as possible with the observed empirical data.</p>
<p>The logistic regression model is essentially modeling the log-odds of the probability of the event. The log-odds are given by the logarithm of the odds ratio:</p>
<p><span class="math display">\[\log\left(\frac{P(Y=1)}{1 - P(Y=1)}\right) = \beta_0 + \beta_1x_1 + \ldots + \beta_nx_n.\]</span></p>
<p>This equation shows that logistic regression is modeling a linear relationship between the independent variables and the log-odds of the dependent variable.</p>
</section>
<section id="making-predictions" class="level3" data-number="70.1.2">
<h3 data-number="70.1.2" class="anchored" data-anchor-id="making-predictions"><span class="header-section-number">70.1.2</span> Making Predictions</h3>
<p>After completing the optimization stage in logistic regression, where the model’s coefficients are determined through Maximum Likelihood Estimation (MLE), the model becomes capable of predicting outcomes for new data. This prediction involves a series of precise mathematical steps. Initially, the logistic regression model utilizes its finely-tuned coefficients, <span class="math inline">\(\beta_0, \beta_1, \beta_2, \ldots, \beta_n\)</span>, to analyze the fresh data. Given a new observation with features denoted as <span class="math inline">\(X_{\text{new}}\)</span>, the model performs a calculation of the linear combination of these features with the coefficients: <span class="math display">\[z = \beta_0 + \beta_1 X_{\text{new}_1} + \beta_2 X_{\text{new}_2} + \cdots + \beta_n X_{\text{new}_n}.\]</span></p>
<p>This computed value, <span class="math inline">\(z\)</span>, is then fed into the logistic (sigmoid) function, represented by <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span>. This function converts <span class="math inline">\(z\)</span> into a probability value ranging from 0 to 1, reflecting the probability that the new data point belongs to the positive class, commonly labeled as “1”.</p>
<p>To translate this probability into a binary classification, a threshold is applied — commonly set at 0.5. If the probability <span class="math inline">\(\sigma(z) \geq 0.5\)</span>, the model classifies the observation as belonging to the positive class. Conversely, if it falls below 0.5, the observation is assigned to the negative class (“0”). This threshold can be adjusted to suit specific needs, such as balancing precision and recall in the model’s predictions.</p>
<p>In summary, logistic regression mathematically models the relationship between independent variables and the probability of a particular outcome. It’s a linear model for the log-odds, but represents a non-linear relationship between the dependent and independent variables.</p>
<p>Logistic regression is widely used because of its simplicity and effectiveness in cases where the relationship between the independent variables and the dependent variable is approximately linear. However, in cases where this linearity assumption doesn’t hold, other more complex algorithms might be more appropriate.</p>
</section>
<section id="example-1" class="level3" data-number="70.1.3">
<h3 data-number="70.1.3" class="anchored" data-anchor-id="example-1"><span class="header-section-number">70.1.3</span> Example</h3>
<pre class="{python}"><code>#| fig-cap: "Data used for training and testing logistic regression."
#| layout-ncol: 1
#| label: fig-logistic_regression
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Generate a synthetic dataset
X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,
                           random_state=42, n_clusters_per_class=1)

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 3: Train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Step 4: Evaluate the model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

plt.subplot(1,2,1);
plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], s=30, label="class 1")
plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], s=30, label="class 1")
plt.title("Training data.")

plt.subplot(1,2,2);
plt.scatter(X_test[y_test==0, 0], X_test[y_test==0, 1], s=30, label="class 1")
plt.scatter(X_test[y_test==1, 0], X_test[y_test==1, 1], s=30, label="class 1")
plt.title("Testing data.")
plt.tight_layout()

print("Model Accuracy:", accuracy)
print("Classification Report:\n", report)</code></pre>
<p>The above Python code implements data classification based on logistic regression. The training and testing data are shown in <a href="supervised_learning.html#fig-logistic_regression">Figure&nbsp;<span>11.4</span></a>.</p>
</section>
</section>
<section id="support-vector-machines" class="level2" data-number="70.2">
<h2 data-number="70.2" class="anchored" data-anchor-id="support-vector-machines"><span class="header-section-number">70.2</span> Support Vector Machines</h2>
<p>Support Vector Machines (SVMs) represent a powerful and versatile class of supervised learning algorithms, widely used for both classification and regression tasks. At their core, SVMs seek to find the best hyperplane that separates different classes in a dataset, particularly excelling in high-dimensional spaces. This is achieved through the identification of support vectors and the maximization of the margin between data points of different classes. One of the key strengths of SVMs is their ability to use <em>kernel functions</em>, which enables them to handle non-linear relationships by transforming data into higher dimensions where it can be linearly separable. Originally developed in the 1960s, SVMs have evolved significantly and are highly regarded in the machine learning community for their robustness and effectiveness, especially in complex domains where the relationship between attributes is not readily apparent.</p>
<section id="linear-classification" class="level3" data-number="70.2.1">
<h3 data-number="70.2.1" class="anchored" data-anchor-id="linear-classification"><span class="header-section-number">70.2.1</span> Linear Classification</h3>
<p>SVMs operate by finding a hyperplane in an <span class="math inline">\(N\)</span>-dimensional space (where <span class="math inline">\(N\)</span> is the number of features) that distinctly classifies data points. A hyperplane is essentially a decision boundary that separates data points of different classes.</p>
<p>In two dimensions, this hyperplane is a line, and in three dimensions, it’s a plane. For higher dimensions, we still refer to it as a hyperplane. Mathematically, a hyperplane can be described by the equation:</p>
<p><span class="math display">\[\vo{w}^T\x + b = 0.\]</span></p>
<p>Here, <span class="math inline">\(\vo{w}\in\real^n\)</span> is the weight vector, <span class="math inline">\(\x\in\real^n\)</span> represents the input features, and <span class="math inline">\(b\)</span> is the bias.</p>
<p><strong>Support vectors</strong> are the data points that are closest to the hyperplane and influence its position and orientation. They essentially “support” the hyperplane in the SVM model.</p>
<p><strong>Margin</strong> is the distance between the hyperplane and the nearest data point from either class. Maximizing this margin is the key objective in SVMs. The margin is calculated as the perpendicular distance from the line to the support vectors.</p>
<p>In a classification task, we deal with two hyperplanes that pass through the support vectors. These are defined as: <span class="math display">\[\begin{align*}
\vo{w}^T\x + b = 1 \quad \text{(for one class)},\\
\vo{w}^T\x + b = -1 \quad \text{(for the other class)}.
\end{align*}\]</span></p>
<p>For a linearly separable set of 2D-points which belong to one of two classes, the goal is to find the maximum-margin hyperplane that divides the classes. The optimization problem is formulated as: <span class="math display">\[
\min_{\vo{w}\in\real^n} \; \frac{1}{2} \|\vo{w} \|^2,\;\; \text{subject to }  y_i (\vo{w}^T\x_i + b) \geq 1 \text{ for each data point } i.
\]</span> Here, <span class="math inline">\(y_i\)</span> are the labels (e.g., <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span> for a binary classification).</p>
<p>Here is a Python code example using Scikit-learn that demonstrates the application of a Support Vector Machine (SVM) for a linearly separable dataset. The code first generates a simple, linearly separable dataset using <code>datasets.make_blobs</code>. This function creates two clusters of data points, ideal for binary classification. Next, a linear SVM classifier (<code>SVC</code> with <code>kernel='linear'</code>) is created and fitted to the data. The parameter <code>C=1000</code> is chosen to emphasize the decision boundary. The data points are plotted using different colors for each class, along with the decision function of the SVM – which includes the decision bounday (where the decision function is zero) and the margins (where the decision function is <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>). The support vectors, which are critical in defining the decision boundary, are circled.</p>
<pre class="{python}"><code>from sklearn import datasets
from sklearn.svm import SVC
import matplotlib.pyplot as plt
import numpy as np

# Load a simple, linearly separable dataset
X, y = datasets.make_blobs(n_samples=100, centers=2, random_state=6)

# Create a linear SVM classifier
clf = SVC(kernel='linear', C=1000)
clf.fit(X, y)

# Plotting the data points
plt.scatter(X[y==0, 0], X[y==0, 1], s=30, label="class 1")
plt.scatter(X[y==1, 0], X[y==1, 1], s=30, label="class 2")

# Plotting the decision function
ax = plt.gca()
xlim = ax.get_xlim()
ylim = ax.get_ylim()

# Creating a grid to evaluate the model
xx = np.linspace(xlim[0], xlim[1], 30)
yy = np.linspace(ylim[0], ylim[1], 30)
YY, XX = np.meshgrid(yy, xx)
xy = np.vstack([XX.ravel(), YY.ravel()]).T
Z = clf.decision_function(xy).reshape(XX.shape)

# Plotting decision boundary and margins
ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])

# Highlighting support vectors
ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k',label="support vectors")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('SVM Linear Classifier on Linearly Separable Data')
plt.legend()
plt.show()</code></pre>
<p>The resulting plot visually demonstrates how the linear SVM successfully separates the two classes with a clear margin. The support vectors are the points that lie on the margins of the classifier.</p>
</section>
<section id="non-linear-classification-with-kernel-trick" class="level3" data-number="70.2.2">
<h3 data-number="70.2.2" class="anchored" data-anchor-id="non-linear-classification-with-kernel-trick"><span class="header-section-number">70.2.2</span> Non-linear Classification with Kernel Trick</h3>
<p>In numerous practical situations, data doesn’t naturally separate into distinct linear categories. This challenge is effectively addressed by the <em>kernel trick</em>. The essence of the kernel trick is to project the data into a space of higher dimensions, where linear separation is feasible. This technique enables the operation within an implicitly high-dimensional feature space without the need to explicitly calculate the data coordinates in that space. This approach intelligently sidesteps the usually extensive computational demands associated with high-dimensional data processing.</p>
<p>Let us consider a mapping <span class="math inline">\(\phi: \real^n \rightarrow \real^m\)</span>, which transforms the original feature space <span class="math inline">\(\real^n\)</span> to a higher-dimensional feature space <span class="math inline">\(\real^m\)</span>. In this higher-dimensional space, the inner product of two vectors <span class="math inline">\(\x_i\)</span> and <span class="math inline">\(\x_j\)</span> is given by <span class="math inline">\(\langle \phi(\x_i), \phi(\x_j) \rangle\)</span>.</p>
<p>Calculating this inner product directly in the higher-dimensional space can be highly computationally intensive. The kernel trick involves using a kernel function <span class="math inline">\(K(\cdot,\cdot)\)</span> that corresponds to this inner product, i.e.,</p>
<p><span class="math display">\[K(\x_i, \x_j) = \langle \phi(\x_i), \phi(\x_j) \rangle.\]</span></p>
<p>This kernel function computes the inner product in the transformed space without explicitly performing the transformation <span class="math inline">\(\phi(\cdot)\)</span>. Essentially, <span class="math inline">\(K(\cdot,\cdot)\)</span> is a measure of similarity between <span class="math inline">\(\x_i\)</span> and <span class="math inline">\(\x_j\)</span> in the transformed space.</p>
<p>Some of the common kernels are:</p>
<ol type="1">
<li><p><strong>Linear Kernels</strong>: This is the simplest kernel function, defined as <span class="math inline">\(K(\x_i, \x_j) = \x_i^T \x_j\)</span>. It does not actually map the data into a higher-dimensional space, and is equivalent to no mapping.</p></li>
<li><p><strong>Polynomial Kernels</strong>: Defined as <span class="math inline">\(K(\x_i, \x_j) = (1 + \x_i^T \x_j)^d\)</span>, where <span class="math inline">\(d\)</span> is the degree of the polynomial. This kernel maps the data into a polynomial feature space.</p></li>
<li><p><strong>Radial Basis Function (RBF) or Gaussian Kernels</strong>: It’s given by <span class="math inline">\(K(\x_i, \x_j) = \exp(-\gamma \| \x_i - \x_j \|^2)\)</span>, where <span class="math inline">\(\gamma\)</span> is a parameter. It maps data into an infinite-dimensional space and is widely used for its properties in non-linear separation.</p></li>
<li><p><strong>Perceptron Kernels</strong>: Defined as <span class="math inline">\(K(\x_i, \x_j) = \tanh(\alpha \x_i^T \x_j + c)\)</span>, where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(c\)</span> are constants. This kernel transforms the data similarly to a neural network.</p></li>
<li><p><strong>Additive Kernels</strong>: More complicated kernels can be formed by adding several kernels, possibly of different kinds, since sum of positive definite functions is also positive definite. Such kernels are defined as <span class="math inline">\(K(\x_i,\x_j) = \sum_k K_k(\x_i,\x_j)\)</span>.</p></li>
<li><p><strong>Tensor Product Kernels</strong>:: Multi-dimensional kernels can be formed from tensor product of different kernels, i.e., <span class="math inline">\(K(\x_i,\x_j) = \Pi_k K_k(\x_i,\x_j)\)</span>.</p></li>
</ol>
<p>The kernel trick is primarily used in SVMs but is also applicable in other areas like principal component analysis (kernel PCA), ridge regression, and more. It allows these algorithms to solve non-linear problems by implicitly using higher-dimensional feature spaces, thereby greatly expanding their applicability without a significant increase in computational cost. However, the choice of the kernel and its parameters can significantly affect the performance of the algorithm and requires careful tuning based on the specific data and problem.</p>
<p><strong>Example</strong><br> Here is a Python example demonstrating non-linear classification using Support Vector Machines (SVMs) with a Radial Basis Function (RBF) kernel.</p>
<pre class="{python}"><code>#| fig-cap: "Nonlinear classification using SVM."
#| label: fig-nonlinear_svm
#| layout-ncol: 1
from sklearn.svm import SVC
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# Generate a non-linearly separable dataset (e.g., two interleaving half circles)
X, y = make_moons(n_samples=100, noise=0.15, random_state=42)

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the dataset
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Create a non-linear SVM classifier with RBF kernel
clf = SVC(kernel='rbf', C=1, gamma='auto')
clf.fit(X_train_scaled, y_train)

# Plotting the decision boundary
def plot_decision_boundary(clf, X, y, plot_support=True):
    # Create a grid to plot decision boundaries
    x0, x1 = np.meshgrid(
        np.linspace(X[:, 0].min() - 1, X[:, 0].max() + 1, num=100),
        np.linspace(X[:, 1].min() - 1, X[:, 1].max() + 1, num=100)
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]
    
    # Prediction for each point in the grid
    y_pred = clf.predict(X_new).reshape(x0.shape)
    
    # Plotting the contour and training points
    plt.contourf(x0, x1, y_pred, alpha=0.3, cmap=plt.cm.brg)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.brg)
    if plot_support:
        # Highlight support vectors
        sv = clf.support_vectors_
        plt.scatter(sv[:, 0], sv[:, 1], s=100, facecolors='none', edgecolors='k')

# Plotting the decision boundary for the classifier
plot_decision_boundary(clf, X_train_scaled, y_train)
plt.title("Non-linear SVM Classifier with RBF Kernel")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()</code></pre>
<p>In this Python example, a non-linear classification is demonstrated using Support Vector Machines (SVMs) with a Radial Basis Function (RBF) kernel. The process starts with generating a non-linearly separable dataset using Scikit-learn’s <code>make_moons</code> function, which creates two interleaving half-circles. The dataset is then split into training and testing sets and standardized using <code>StandardScaler</code> to ensure equal contribution of each feature in the SVM’s distance calculations.</p>
<p>The SVM model is configured with an RBF kernel (<code>SVC(kernel='rbf')</code>), chosen for its effectiveness in handling non-linear data. Parameters <code>C</code> and <code>gamma</code> are set to balance between low training error and generalization. A custom function, <code>plot_decision_boundary</code>, visualizes the decision boundary of the SVM. It produces a contour plot (see <a href="supervised_learning.html#fig-nonlinear_svm">Figure&nbsp;<span>11.5</span></a>) showing how the SVM with an RBF kernel successfully classifies the complex dataset, with the support vectors prominently highlighted. This example showcases the SVM’s capability to handle non-linear data and the adaptability of the RBF kernel to complex data patterns.</p>
<p><a href="supervised_learning.html#fig-nonlinear_svm">Figure&nbsp;<span>11.5</span></a> shows that the classification is not clean. Non-linear Support Vector Machines (SVMs) may struggle to cleanly separate data classes due to several inherent challenges related to the data and the model itself. The primary issue often lies in the inherent overlap within the data: real-world datasets frequently exhibit classes with overlapping distributions, meaning some data points naturally share characteristics of more than one class. This overlap makes perfect separation, even with sophisticated models, inherently challenging.</p>
<p>The effectiveness of a non-linear SVM greatly depends on the choice of the kernel function, such as RBF, polynomial, or sigmoid, and the tuning of its parameters. An inappropriate kernel or poorly chosen parameters can lead to inadequate separation of the classes. Furthermore, non-linear SVMs can suffer from overfitting, especially if the decision boundary becomes excessively complex in an attempt to capture subtle patterns in the training data. This complexity might make the model too sensitive to the noise and outliers in the data, impairing its ability to generalize to new, unseen data.</p>
<p>The transformation of data into a higher-dimensional space, a common strategy in non-linear SVMs to achieve separability, can paradoxically complicate the relationships within the data, making clean separation more difficult. Additionally, the presence of noise and outliers can significantly skew the decision boundary. Non-linear SVMs, in their effort to accommodate these anomalies, might fail to establish a clear division between classes.</p>
<p>Lastly, the representation and preprocessing of features play a crucial role. If the features do not adequately capture the distinct characteristics of each class, the SVM may not be able to effectively differentiate between them. Thus, while non-linear SVMs are powerful tools for complex, non-linear datasets, their success in cleanly separating classes hinges on the nature of the data, the selection and tuning of the kernel, and the presence of noise and outliers. Effective separation often requires meticulous data preprocessing, feature engineering, and model parameter tuning.</p>
</section>
<section id="data-normalization" class="level3" data-number="70.2.3">
<h3 data-number="70.2.3" class="anchored" data-anchor-id="data-normalization"><span class="header-section-number">70.2.3</span> Data Normalization</h3>
<p>Data normalization is a crucial preprocessing step in machine learning, especially when working with Support Vector Machines (SVMs) and their associated kernel functions. Certain kernels, due to their intrinsic mathematical properties, have a restricted domain where they operate effectively. For these kernels, normalization of the input data becomes not just beneficial but necessary. However, even for kernels without such restrictions, normalization can still be advantageous.</p>
<ol type="1">
<li><p><strong>Kernels with Restricted Domain</strong>: Some kernel functions, like the Radial Basis Function (RBF) or Gaussian kernel, are sensitive to the scale of the input features. These kernels compute distances between data points; thus, features on larger scales can dominate the distance metric, leading to biased results. Normalizing data ensures that each feature contributes proportionately to the distance calculations.</p></li>
<li><p><strong>Advantages for Unrestricted Kernels</strong>: Even for kernels that do not have a restricted domain, such as the linear kernel, normalization can be beneficial. It helps in avoiding numerical instability and ensures that the optimization algorithm used for training the SVM converges more efficiently.</p></li>
<li><p><strong>Isotropic vs.&nbsp;Non-Isotropic Normalization</strong>:</p>
<ul>
<li><strong>Isotropic Normalization</strong>: This involves scaling the data so that the variance is the same for each feature. It treats all dimensions equally and is commonly achieved through methods like standardization, where each feature is centered around zero with unit variance.</li>
<li><strong>Non-Isotropic Normalization</strong>: Here, different scaling is applied to different features. This might be necessary when features have different units or scales of measurement, and you want to preserve these differences to some extent.</li>
</ul></li>
<li><p><strong>Consideration of Input Features</strong>: Deciding whether to normalize the data (and what type of normalization to use) requires careful consideration of the input features. Features with different scales, units, and distributions might influence the SVM’s performance, and choosing the right normalization technique can significantly impact the effectiveness of the model.</p></li>
<li><p><strong>Improving the Condition Number of the Hessian</strong>: In the optimization problem solved during SVM training, the Hessian matrix plays a critical role. Normalization can improve the condition number of this matrix, which is a measure of its sensitivity to numerical errors. A well-conditioned Hessian ensures that the optimization algorithm is stable and converges efficiently, leading to a more robust and accurate SVM model.</p></li>
</ol>
<p>In summary, data normalization is a key step in preparing data for SVMs with different kernels. It not only accommodates the mathematical requirements of certain kernels but also enhances the overall stability and performance of the SVM training process. The choice of normalization technique should be made in the context of the specific dataset and the characteristics of the input features.</p>
</section>
<section id="vapnik-chervonenkis-vc-dimension" class="level3" data-number="70.2.4">
<h3 data-number="70.2.4" class="anchored" data-anchor-id="vapnik-chervonenkis-vc-dimension"><span class="header-section-number">70.2.4</span> Vapnik-Chervonenkis (VC) Dimension</h3>
<p>The Vapnik-Chervonenkis (VC) dimension is a fundamental concept in statistical learning theory, named after Vladimir Vapnik and Alexey Chervonenkis. It measures the capacity of a set of functions to classify sets of points in all possible ways, essentially quantifying the model’s complexity or expressive power.</p>
<p><strong>Definition</strong>: For a given set of functions (hypotheses), the VC dimension is the largest number of points that can be shattered by these functions. “Shattering” means that for every possible way of labeling these points (into two classes), there is a function in the set that can separate the points into the two classes exactly as per the labeling.</p>
<p><strong>Implication in Machine Learning</strong>: In the context of machine learning models, like SVMs, the VC dimension provides a theoretical upper bound on the model’s capacity to learn from data. A higher VC dimension indicates a more complex model, which can lead to a better fit to the training data. However, it also increases the risk of overfitting, where the model captures noise rather than the underlying pattern.</p>
<p>Calculating the exact VC dimension for a given machine learning model can be complex and is often not straightforward. There are no generic algorithms or formulae that can directly compute the VC dimension for all types of models, especially for non-linear models or those involving kernel methods like SVMs.</p>
<p>However, we can estimate or get insights into the complexity of a model (akin to understanding its VC dimension) using certain practical approaches:</p>
<ol type="1">
<li><p><strong>Model Complexity Parameters</strong>: For some models, the complexity parameters give an indication of the VC dimension. For instance, in SVMs, the choice of the kernel and its parameters can influence the VC dimension.</p></li>
<li><p><strong>Empirical Estimation</strong>: We can empirically estimate the model’s capacity by observing its performance on training and validation datasets. If a model can perfectly classify a training set of a certain size but fails to generalize to new data, it may indicate a high VC dimension relative to the size of the training data.</p></li>
<li><p><strong>Theoretical Calculation</strong>: For simpler models (like linear classifiers in low-dimensional spaces), the VC dimension can sometimes be calculated directly. For example, the VC dimension of a linear classifier in an <span class="math inline">\(n\)</span>-dimensional space is <span class="math inline">\(n+1\)</span>.</p></li>
</ol>
<p>In Python, directly calculating the VC dimension is not commonly done for complex models. Instead, techniques like cross-validation and observing training versus validation performance are used to gauge a model’s complexity and generalization capability.</p>
<p>We can use libraries like Scikit-learn in Python to experiment with different model complexities and observe overfitting versus underfitting. This empirical approach doesn’t calculate the VC dimension explicitly but helps understand the model’s capacity, which is what the VC dimension conceptually represents.</p>
<pre class="{python}"><code>from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
import numpy as np

# Example using a dataset (X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

# Experiment with different complexities (like different kernels or C values for SVM)
model = SVC(kernel='linear', C=1)
model.fit(X_train, y_train)

# Evaluate on training and test sets
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

# Perform cross-validation
cv_scores = cross_val_score(model, X, y, cv=5)

# Observing the differences in scores can give insights into the model's complexity and generalization ability
print(f"Training Score: {train_score}, \nTest Score: {test_score}, \nCV Scores: {np.mean(cv_scores)}")</code></pre>
<p>In summary, while calculating the VC dimension for complex models in Python is not straightforward, understanding the underlying concept and using empirical methods to evaluate model complexity can serve a similar purpose in practical machine learning applications.</p>
</section>
<section id="kernel-selection" class="level3" data-number="70.2.5">
<h3 data-number="70.2.5" class="anchored" data-anchor-id="kernel-selection"><span class="header-section-number">70.2.5</span> Kernel Selection</h3>
<p>The selection of an appropriate kernel function in Support Vector Machines (SVMs) is a critical step that directly impacts the performance of the model. Given the plethora of kernel mappings available, the decision on which kernel to use for a specific problem is not straightforward. This challenge has been a longstanding topic in the field of machine learning, and the integration of various kernels within a unified framework facilitates a more systematic comparison of their performance.</p>
<ol type="1">
<li><p><strong>Comparing Kernels Using Theoretical Measures</strong>: One potential method for comparing different kernels is to use theoretical measures like the upper bound on the Vapnik-Chervonenkis (VC) dimension. The VC dimension provides a measure of the capacity or complexity of a set of functions (in this case, the kernel functions), with a lower VC dimension suggesting a potentially more generalizable model. However, applying this measure practically is challenging. It involves estimating parameters like the radius of the hypersphere that encloses the data in the transformed, non-linear feature space. This estimation can be complex and may not always yield practical insights for kernel selection.</p></li>
<li><p><strong>Practical Challenges in Theoretical Selection</strong>: While theoretical frameworks can guide kernel selection, they often require complex calculations and assumptions that may not hold in real-world scenarios. For instance, estimating the radius of the hypersphere in the feature space is not straightforward and could lead to inaccuracies in the theoretical comparison of kernels.</p></li>
<li><p><strong>Empirical Methods Remain Predominant</strong>: Due to these challenges, empirical methods like bootstrapping and cross-validation continue to be the preferred approaches for kernel selection. These methods involve training the SVM with different kernels on a subset of the data and validating their performance on an independent test set. By comparing the model’s performance across various kernels using these techniques, practitioners can select the kernel that offers the best trade-off between complexity and accuracy for their specific problem.</p></li>
<li><p><strong>Importance of Independent Test Sets</strong>: Independent test sets are crucial in this process as they provide an unbiased evaluation of the model’s performance. A kernel that performs well on the training data might not necessarily generalize well to unseen data. Therefore, validation on independent test sets is essential to ensure that the selected kernel is not only theoretically sound but also practically effective.</p></li>
<li><p><strong>Final Caution</strong>: It’s important to remember that even if a robust theoretical method for kernel selection is developed, it must be validated empirically across a wide range of problems to ensure its reliability. Machine learning, especially in complex real-world applications, often requires a balance between theoretical soundness and empirical validation.</p></li>
</ol>
<p>In conclusion, kernel selection in SVMs is a nuanced process that involves considering both theoretical measures and empirical validation. While theoretical tools like the upper bound on the VC dimension can offer insights, practical methods like cross-validation and bootstrapping, validated against independent test sets, remain essential for making informed decisions about kernel selection.</p>
</section>
<section id="svm-for-regression-svr" class="level3" data-number="70.2.6">
<h3 data-number="70.2.6" class="anchored" data-anchor-id="svm-for-regression-svr"><span class="header-section-number">70.2.6</span> SVM for Regression (SVR)</h3>
<p>Support Vector Machine (SVM) regression, also known as Support Vector Regression (SVR), is an extension of the SVM algorithm from classification to regression problems. While the classification version of SVM focuses on finding a hyperplane that best separates two classes, SVR aims to find a function that approximates the relationship between input features and continuous target values. Here’s a mathematical explanation of how SVM can be used for regression:</p>
<p>In SVM for regression, the goal is to find a function <span class="math inline">\(f(\x): \real^n\mapsto \real\)</span> that has at most an ε (epsilon) deviation from the actually obtained targets <span class="math inline">\(y_i\in\real\)</span> for all the training data, and at the same time is as flat as possible.</p>
<p>Let’s consider a dataset with inputs <span class="math inline">\(\x_i\in\real^n\)</span> and outputs <span class="math inline">\(y_i\)</span>, where <span class="math inline">\(i = 1, ..., n\)</span>. SVM regression tries to fit the function: <span class="math display">\[ f(\x) = \vo{w}^T\x + b.\]</span></p>
<p>Here, <span class="math inline">\(\vo{w}\in\real^n\)</span> is the weight vector and <span class="math inline">\(b\in\real\)</span> is the bias. The objective is to minimize the norm of <span class="math inline">\(\vo{w}\)</span> (i.e., <span class="math inline">\(\vo{w}^T\vo{w}\)</span> to keep the model as simple or as flat as possible, which helps in generalization.</p>
<p><strong>Epsilon-Insensitive Tube</strong><br> SVR allows some errors in the approximation of the target values while keeping the model simple. This is achieved by introducing an ε-insensitive loss function, which does not penalize errors that are within a margin of ε from the true value. Mathematically, this can be represented as: <span class="math display">\[\text{Loss} = \max(0, |y_i - f(\x_i)| - \epsilon).\]</span></p>
<p>This creates an ε-insensitive tube or band around the regression function. Points that fall within this tube do not contribute to the loss in the model.</p>
<p><strong>Optimization Problem</strong><br> The optimization problem for Support Vector Regression (SVR) involves finding a set of parameters that best fit the regression function to the data while maintaining a balance between the model’s complexity and the allowance for deviations beyond a certain threshold. When formulated in terms of vectors, the optimization problem becomes a convex quadratic programming problem.</p>
<p>The primary objective in SVR is to find a function <span class="math inline">\(f(x) = \vo{w}^T\x + b\)</span> that approximates the relationship between the input vectors <span class="math inline">\(\x\)</span> and the target values <span class="math inline">\(y\)</span>, with a certain tolerance for errors. The objective function aims to minimize the norm of the weight vector <span class="math inline">\(\vo{w}\)</span> (which corresponds to the flatness of the function) along with the penalty for errors exceeding a margin <span class="math inline">\(\epsilon\)</span>.</p>
<p>The optimization problem is given by <span class="math display">\[\begin{align*}
&amp; \min_{\vo{w},b} \vo{w}^T\vo{w}, \\
\text{subject to } &amp; \\
&amp; y_i - (\vo{w}^T\x_i + b) \leq \epsilon,\\
&amp; (\vo{w}^T\x_i + b) - y_i \leq \epsilon.\\
\end{align*}\]</span></p>
<p>To allow for some flexibility in this model (tolerating deviations larger than ε for some points), slack variables <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\xi_i^*\)</span> are introduced. The new optimization problem becomes:</p>
<p><span class="math display">\[\begin{align*}
&amp; \min_{\vo{w},b,\vo{\xi},\vo{\xi}^\ast} \vo{w}^T\vo{w} + C \sum_{i=1}^{n} (\xi_i + \xi_i^\ast), \\
\text{subject to } &amp; \\
&amp; y_i - (\vo{w}^T\x_i + b) \leq \epsilon + \xi_i,\\
&amp; (\vo{w}^T\x_i + b) - y_i \leq \epsilon + \xi_i^\ast,
\end{align*}\]</span> where <span class="math inline">\(\vo{\xi}\)</span> and <span class="math inline">\(\vo{\xi}^\ast\)</span> are defined by components <span class="math inline">\(\xi_i\)</span> and <span class="math inline">\(\xi_i^\ast\)</span> respectively.</p>
<p><strong>Kernel Extension</strong><br> For non-linear regression, the optimization problem can incorporate the kernel trick, where the dot product <span class="math inline">\(\vo{w}^T\x_i\)</span> is replaced by a kernel function <span class="math inline">\(K(\x_i, \x_j)\)</span>. This allows the SVR to capture non-linear relationships without explicitly transforming the data into a higher-dimensional space.</p>
<p>In summary, the SVR optimization problem in vector form involves minimizing a function that balances the flatness of the regression model and the penalty for errors exceeding an ε-margin, subject to constraints that allow some flexibility for deviations. This formulation is solved using quadratic programming techniques, and the incorporation of the kernel trick extends its applicability to non-linear regression problems.</p>
<p><strong>Example</strong><br></p>
<pre class="{python}"><code>#| fig-cap: Approximation of $\sin(x)$ from scattered data using SVR with an RBF kernel. Parameters $\gamma$, $C$ and $\epsilon$ control tradeoff between overfit and accuracy. 
#| layout-ncol: 1
#| label: fig-svr_sine
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# Generate a non-linear regression dataset
X, y = make_regression(n_samples=300, n_features=1, noise=10, random_state=42)
y = np.sin(X).ravel()  # Making the relationship non-linear

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

def svr(X_train,y_train,kernel,C,gamma,epsilon):
    svr_rbf = SVR(kernel='rbf', C=C, gamma=gamma, epsilon=epsilon)
    svr_rbf.fit(X_train, y_train)
    y_pred = svr_rbf.predict(X_test_scaled)
    mse = mean_squared_error(y_test, y_pred)
    return y_pred, mse

C = 100
gamma = 0.1
kernel = 'rbf'

# Nominal accuracy
epsilon1 = 0.1
y_pred1, mse1 =  svr(X_train_scaled,y_train,kernel,C,gamma,epsilon1)
print(f'Mean square error (nominal):{mse1}')

# High accuracy
epsilon2 = 0.01
y_pred2, mse2 =  svr(X_train_scaled,y_train,kernel,C,gamma,epsilon2)
print(f'Mean square error (high accuracy):{mse2}')

def plot_results(X_train, X_test, y_train, y_test, y_pred,epsilon):
    s = 10
    plt.scatter(X_train, y_train, color='gray', label='Training data',s=s)
    plt.scatter(X_test, y_test, color='red', label='Test data',s=s)
    plt.scatter(X_test, y_pred,  label='SVR prediction',s=s)
    plt.title(f'SVR for Non-linear Regression, $\epsilon$={epsilon}.')
    plt.xlabel('Feature: x')
    plt.ylabel('Target: f(x)')
    plt.legend()

plt.figure(figsize=(10,6))
plt.subplot(1,2,1); plot_results(X_train_scaled, X_test_scaled, y_train, y_test, y_pred1, epsilon1)
plt.subplot(1,2,2); plot_results(X_train_scaled, X_test_scaled, y_train, y_test, y_pred2, epsilon2)
plt.tight_layout()
plt.show()
</code></pre>
<p>In this Python example, we demonstrate the use of Support Vector Regression (SVR) for a non-linear regression task, employing the Radial Basis Function (RBF) kernel. The process begins with the generation of a regression dataset using Scikit-learn’s <code>make_regression</code> function. To introduce non-linearity into the dataset, the target variable <code>y</code> is transformed using a sine function, creating a more complex relationship between the input features and the target.</p>
<p>The dataset is then divided into training and testing sets to enable model evaluation. Before training the model, the data undergoes standardization using <code>StandardScaler</code>. This step ensures that each feature contributes equally to the distance calculations in the SVR model, which is particularly important for kernels like RBF that are sensitive to the scale of input features.</p>
<p>The SVR model is instantiated with an RBF kernel by setting <code>kernel='rbf'</code>. Parameters such as <code>C</code>, <code>gamma</code>, and <code>epsilon</code> are configured to control the complexity of the model and its sensitivity to deviations from the training data. The model is trained on the scaled training data and subsequently used to make predictions on the scaled test data.</p>
<p>To evaluate the performance of the model, the mean squared error (MSE) is calculated, comparing the predicted values with the actual values in the test set. A lower MSE value indicates a better fit of the model to the data.</p>
<p>Finally, the results are visualized through a scatter plot that displays the training data, test data, and the predictions made by the SVR model. <a href="supervised_learning.html#fig-svr_sine">Figure&nbsp;<span>11.6</span></a> clearly shows how the SVR with an RBF kernel is able to capture the underlying non-linear pattern in the data. The calculated MSE of approximately 0.0054, with <span class="math inline">\(\epsilon = 0.1\)</span>, further confirms the model’s effectiveness in accurately modeling the complex, non-linear relationships present in the dataset. Reducing <span class="math inline">\(\epsilon\)</span> to <span class="math inline">\(0.01\)</span> further reduces the MSE.</p>
</section>
<section id="example-fault-diagnostics-using-classification" class="level3" data-number="70.2.7">
<h3 data-number="70.2.7" class="anchored" data-anchor-id="example-fault-diagnostics-using-classification"><span class="header-section-number">70.2.7</span> Example: Fault Diagnostics Using Classification</h3>
<p>Need to think about this one.</p>
</section>
</section>
</section>
<section id="unsupervised-learning" class="level1" data-number="71">
<h1 data-number="71"><span class="header-section-number">71</span> Unsupervised Learning</h1>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./supervised_learning.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Supervised Learning</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./dimensionality_reduction.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Dimensionality Reduction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>